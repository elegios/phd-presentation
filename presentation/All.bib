@article{aasaPrecedencesSpecificationsImplementations1995,
  title = {Precedences in Specifications and Implementations of Programming Languages},
  author = {Aasa, Annika},
  year = {1995},
  month = may,
  journal = {Theoretical Computer Science},
  series = {Selected {{Papers}} of the {{Symposium}} on {{Programming Language Implementation}} and {{Logic Programming}}},
  volume = {142},
  number = {1},
  pages = {3--26},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(95)90680-J},
  urldate = {2019-12-13},
  abstract = {Although precedences are often used to resolve ambiguities in programming language descriptions, there has been no parser-independent definition of languages which are generated by grammars with precedence rules. This paper gives such a definition for a subclass of context-free grammars. The definition is shown to be equivalent to the implicit definition an operator precedence parser gives. A problem with a language containing infix, prefix and postfix operators of different precedences is that the well-known algorithm, which transforms a grammar with infix operator precedences to an ordinary unambiguous context-free grammar, does not work. This paper gives an algorithm that works also for prefix and postfix operators, and the correctness of it is proved. An application of the algorithm is also presented.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/HCKKL8W2/Aasa - 1995 - Precedences in specifications and implementations .pdf;/home/vipa/Zotero/storage/FK42AYI6/030439759590680J.html}
}

@inproceedings{adamsComplexityPerformanceParsing2016,
  title = {On the {{Complexity}} and {{Performance}} of {{Parsing}} with {{Derivatives}}},
  booktitle = {Proceedings of the 37th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Adams, Michael D. and Hollenbeck, Celeste and Might, Matthew},
  year = {2016},
  series = {{{PLDI}} '16},
  pages = {224--236},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2908080.2908128},
  urldate = {2019-11-05},
  abstract = {Current algorithms for context-free parsing inflict a trade-off between ease of understanding, ease of implementation, theoretical complexity, and practical performance. No algorithm achieves all of these properties simultaneously. Might et al. introduced parsing with derivatives, which handles arbitrary context-free grammars while being both easy to understand and simple to implement. Despite much initial enthusiasm and a multitude of independent implementations, its worst-case complexity has never been proven to be better than exponential. In fact, high-level arguments claiming it is fundamentally exponential have been advanced and even accepted as part of the folklore. Performance ended up being sluggish in practice, and this sluggishness was taken as informal evidence of exponentiality. In this paper, we reexamine the performance of parsing with derivatives. We have discovered that it is not exponential but, in fact, cubic. Moreover, simple (though perhaps not obvious) modifications to the implementation by Might et al. lead to an implementation that is not only easy to understand but also highly performant in practice.},
  isbn = {978-1-4503-4261-2},
  keywords = {Parsing,Parsing with derivatives,Performance},
  file = {/home/vipa/Zotero/storage/YP6XPYMG/Adams et al. - 2016 - On the Complexity and Performance of Parsing with .pdf}
}

@article{adamsPrincipledParsingIndentationsensitive2013,
  title = {Principled Parsing for Indentation-Sensitive Languages: Revisiting Landin's Offside Rule},
  shorttitle = {Principled Parsing for Indentation-Sensitive Languages},
  author = {Adams, Michael D.},
  year = {2013},
  month = jan,
  journal = {ACM SIGPLAN Notices},
  volume = {48},
  number = {1},
  pages = {511--522},
  issn = {0362-1340},
  doi = {10.1145/2480359.2429129},
  urldate = {2021-08-03},
  abstract = {Several popular languages, such as Haskell, Python, and F\#, use the indentation and layout of code as part of their syntax. Because context-free grammars cannot express the rules of indentation, parsers for these languages currently use ad hoc techniques to handle layout. These techniques tend to be low-level and operational in nature and forgo the advantages of more declarative specifications like context-free grammars. For example, they are often coded by hand instead of being generated by a parser generator. This paper presents a simple extension to context-free grammars that can express these layout rules, and derives GLR and LR(k) algorithms for parsing these grammars. These grammars are easy to write and can be parsed efficiently. Examples for several languages are presented, as are benchmarks showing the practical efficiency of these algorithms.},
  keywords = {indentation,offside rule,parsing},
  file = {/home/vipa/Zotero/storage/WERMJAKP/Adams - 2013 - Principled parsing for indentation-sensitive langu.pdf}
}

@inproceedings{adelson-velskiiAlgorithmOrganizationInformation1962,
  title = {An Algorithm for Organization of Information},
  booktitle = {Doklady Akademii Nauk},
  author = {{Adel'son-Velskii}, Georgii Maksimovich and Landis, Evgenii Mikhailovich},
  year = {1962},
  volume = {146},
  pages = {263--266},
  publisher = {Russian Academy of Sciences}
}

@inproceedings{afroozehFasterPracticalGLL2015,
  title = {Faster, {{Practical GLL Parsing}}},
  booktitle = {Compiler {{Construction}}},
  author = {Afroozeh, Ali and Izmaylova, Anastasia},
  editor = {Franke, Bj{\"o}rn},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {89--108},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Generalized LL (GLL) parsing is an extension of recursivedescent (RD) parsing that supports all context-free grammars in cubic time and space. GLL parsers have the direct relationship with the grammar that RD parsers have, and therefore, compared to GLR, are easier to understand, debug, and extend. This makes GLL parsing attractive for parsing programming languages.In this paper we propose a more efficient Graph-Structured Stack (GSS) for GLL parsing that leads to significant performance improvement. We also discuss a number of optimizations that further improve the performance of GLL. Finally, for practical scannerless parsing of programming languages, we show how common lexical disambiguation filters can be integrated in GLL parsing.Our new formulation of GLL parsing is implemented as part of the Iguana parsing framework. We evaluate the effectiveness of our approach using a highly-ambiguous grammar and grammars of real programming languages. Our results, compared to the original GLL, show a speedup factor of 10 on the highly-ambiguous grammar, and a speedup factor of 1.5, 1.7, and 5.2 on the grammars of Java, C\#, and OCaml, respectively.},
  isbn = {978-3-662-46663-6},
  langid = {english},
  file = {/home/vipa/Zotero/storage/D6R349WT/Afroozeh and Izmaylova - 2015 - Faster, Practical GLL Parsing.pdf}
}

@inproceedings{afroozehOneParserRule2015,
  title = {One Parser to Rule Them All},
  booktitle = {2015 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}} ({{Onward}}!)},
  author = {Afroozeh, Ali and Izmaylova, Anastasia},
  year = {2015},
  month = oct,
  series = {Onward! 2015},
  pages = {151--170},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2814228.2814242},
  urldate = {2021-08-03},
  abstract = {Despite the long history of research in parsing, constructing parsers for real programming languages remains a difficult and painful task. In the last decades, different parser generators emerged to allow the construction of parsers from a BNF-like specification. However, still today, many parsers are handwritten, or are only partly generated, and include various hacks to deal with different peculiarities in programming languages. The main problem is that current declarative syntax definition techniques are based on pure context-free grammars, while many constructs found in programming languages require context information. In this paper we propose a parsing framework that embraces context information in its core. Our framework is based on data-dependent grammars, which extend context-free grammars with arbitrary computation, variable binding and constraints. We present an implementation of our framework on top of the Generalized LL (GLL) parsing algorithm, and show how common idioms in syntax of programming languages such as (1) lexical disambiguation filters, (2) operator precedence, (3) indentation-sensitive rules, and (4) conditional preprocessor directives can be mapped to data-dependent grammars. We demonstrate the initial experience with our framework, by parsing more than 20000 Java, C\#, Haskell, and OCaml source files.},
  isbn = {978-1-4503-3688-8},
  keywords = {context-aware scanning,data-dependent grammars,disambiguation,GLL,offside rule,operator precedence,Parsing,preprocessor directives,scannerless parsing},
  file = {/home/vipa/Zotero/storage/K5QDN8AD/Afroozeh and Izmaylova - 2015 - One parser to rule them all.pdf}
}

@inproceedings{afroozehSafeSpecificationOperator2013,
  title = {Safe {{Specification}} of {{Operator Precedence Rules}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Afroozeh, Ali and {van den Brand}, Mark and Johnstone, Adrian and Scott, Elizabeth and Vinju, Jurgen},
  editor = {Erwig, Martin and Paige, Richard F. and Van Wyk, Eric},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {137--156},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-02654-1_8},
  abstract = {In this paper we present an approach to specifying operator precedence based on declarative disambiguation constructs and an implementation mechanism based on grammar rewriting. We identify a problem with existing generalized context-free parsing and disambiguation technology: generating a correct parser for a language such as OCaml using declarative precedence specification is not possible without resorting to some manual grammar transformation. Our approach provides a fully declarative solution to operator precedence specification for context-free grammars, is independent of any parsing technology, and is safe in that it guarantees that the language of the resulting grammar will be the same as the language of the specification grammar. We evaluate our new approach by specifying the precedence rules from the OCaml reference manual against the highly ambiguous reference grammar and validate the output of our generated parser.},
  isbn = {978-3-319-02654-1},
  langid = {english},
  keywords = {Derivation Tree,Operator Precedence,Parse Tree,Precedence Rule,Production Rule},
  file = {/home/vipa/Zotero/storage/D82XFQ84/Afroozeh et al. - 2013 - Safe Specification of Operator Precedence Rules.pdf}
}

@book{ahoCompilersPrinciplesTechniques2006,
  title = {Compilers: {{Principles}}, {{Techniques}}, and {{Tools}}},
  shorttitle = {Compilers},
  author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman, Jeffrey D.},
  year = {2006},
  month = sep,
  edition = {2nd},
  publisher = {Addison Wesley},
  address = {Boston},
  abstract = {Compilers: Principles, Techniques and Tools, known to professors, students, and developers worldwide as the "Dragon Book," is available in a new edition.~ Every chapter has been completely revised to reflect developments in software engineering, programming languages, and computer architecture that have occurred since 1986, when the last edition published.~ The authors, recognizing that few readers will ever go on to construct a compiler, retain their focus on the broader set of problems faced in software design and software development.},
  isbn = {978-0-321-48681-3},
  langid = {english}
}

@inproceedings{ahoDeterministicParsingAmbiguous1973,
  title = {Deterministic Parsing of Ambiguous Grammars},
  booktitle = {Proceedings of the 1st Annual {{ACM SIGACT-SIGPLAN}} Symposium on {{Principles}} of Programming Languages},
  author = {Aho, A. V. and Johnson, S. C. and Ullman, J. D.},
  year = {1973},
  month = oct,
  series = {{{POPL}} '73},
  pages = {1--21},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/512927.512928},
  urldate = {2020-11-06},
  abstract = {We consider methods of describing the syntax of programming languages in ways that are more flexible and natural than conventional BNF descriptions. These methods involve the use of ambiguous context-free grammars together with rules to resolve syntactic ambiguities. We show how efficient LL and LR parsers can be constructed directly from certain classes of these specifications.},
  isbn = {978-1-4503-7349-4},
  file = {/home/vipa/Zotero/storage/TBNTWPCC/Aho et al. - 1973 - Deterministic parsing of ambiguous grammars.pdf}
}

@inproceedings{alurVisiblyPushdownLanguages2004,
  title = {Visibly {{Pushdown Languages}}},
  booktitle = {Proceedings of the {{Thirty-sixth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Alur, Rajeev and Madhusudan, P.},
  year = {2004},
  series = {{{STOC}} '04},
  pages = {202--211},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1007352.1007390},
  urldate = {2019-02-28},
  abstract = {We propose the class of visibly pushdown languages as embeddings of context-free languages that is rich enough to model program analysis questions and yet is tractable and robust like the class of regular languages. In our definition, the input symbol determines when the pushdown automaton can push or pop, and thus the stack depth at every position. We show that the resulting class Vpl of languages is closed under union, intersection, complementation, renaming, concatenation, and Kleene-*, and problems such as inclusion that are undecidable for context-free languages are Exptime-complete for visibly pushdown automata. Our framework explains, unifies, and generalizes many of the decision procedures in the program analysis literature, and allows algorithmic verification of recursive programs with respect to many context-free properties including access control properties via stack inspection and correctness of procedures with respect to pre and post conditions. We demonstrate that the class Vpl is robust by giving two alternative characterizations: a logical characterization using the monadic second order (MSO) theory over words augmented with a binary matching predicate, and a correspondence to regular tree languages. We also consider visibly pushdown languages of infinite words and show that the closure properties, MSO-characterization and the characterization in terms of regular trees carry over. The main difference with respect to the case of finite words turns out to be determinizability: nondeterministic B{\"u}chi visibly pushdown automata are strictly more expressive than deterministic Muller visibly pushdown automata.},
  isbn = {978-1-58113-852-8},
  keywords = {$omega$-languages,context-free languages,logic,pushdown automata,regular tree languages,verification},
  file = {/home/vipa/Zotero/storage/8DLIPGMN/Alur and Madhusudan - 2004 - Visibly Pushdown Languages.pdf}
}

@article{alvesMLstyleRecordCalculus2021,
  title = {An {{ML-style Record Calculus}} with {{Extensible Records}}},
  author = {Alves, Sandra and Ramos, Miguel},
  year = {2021},
  month = dec,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {351},
  eprint = {2108.06296},
  primaryclass = {cs},
  pages = {1--17},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.351.1},
  urldate = {2024-07-03},
  abstract = {In this work, we develop a polymorphic record calculus with extensible records. Extensible records are records that can have new fields added to them, or preexisting fields removed from them. We also develop a static type system for this calculus and a sound and complete type inference algorithm. Most ML-style polymorphic record calculi that support extensible records are based on row variables. We present an alternative construction based on the polymorphic record calculus developed by Ohori. Ohori based his polymorphic record calculus on the idea of kind restrictions. This allowed him to express polymorphic operations on records such as field selection and modification. With the addition of extensible types, we were able to extend Ohori's original calculus with other powerful operations on records such as field addition and removal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science},
  file = {/home/vipa/Zotero/storage/2UAW9QDW/Alves and Ramos - 2021 - An ML-style Record Calculus with Extensible Record.pdf;/home/vipa/Zotero/storage/Y6DYL5LD/2108.html}
}

@article{aminamanzoorHajpenAvElizabeth2016,
  title = {Hajpen Av {{Elizabeth Holmes}} V{\"a}ckte Ont Blod i {{Silicon Valley}}},
  author = {{Amina Manzoor}},
  year = {2016},
  month = dec,
  journal = {Dagens Nyheter},
  urldate = {2023-03-17}
}

@inproceedings{amorimDeepPriorityConflicts2017,
  title = {Deep Priority Conflicts in the Wild: A Pilot Study},
  shorttitle = {Deep Priority Conflicts in the Wild},
  booktitle = {Proceedings of the 10th {{ACM SIGPLAN International Conference}} on {{Software Language Engineering}}},
  author = {Amorim, Lu{\'i}s Eduardo de Souza and Steindorfer, Michael J. and Visser, Eelco},
  year = {2017},
  month = oct,
  series = {{{SLE}} 2017},
  pages = {55--66},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3136014.3136020},
  urldate = {2021-01-19},
  abstract = {Context-free grammars are suitable for formalizing the syntax of programming languages concisely and declaratively. Thus, such grammars are often found in reference manuals of programming languages, and used in language workbenches for language prototyping. However, the natural and concise way of writing a context-free grammar is often ambiguous. Safe and complete declarative disambiguation of operator precedence and associativity conflicts guarantees that all ambiguities arising from combining the operators of the language are resolved. Ambiguities can occur due to shallow conflicts, which can be captured by one-level tree patterns, and deep conflicts, which require more elaborate techniques. Approaches to solve deep priority conflicts include grammar transformations, which may result in large unambiguous grammars, or may require adapted parser technologies to include data-dependency tracking at parse time. In this paper we study deep priority conflicts "in the wild". We investigate the efficiency of grammar transformations to solve deep priority conflicts by using a lazy parse table generation technique. On top of lazily-generated parse tables, we define metrics, aiming to answer how often deep priority conflicts occur in real-world programs and to what extent programmers explicitly disambiguate programs themselves. By applying our metrics to a small corpus of popular open-source repositories we found that in OCaml, up to 17\% of the source files contain deep priority conflicts.},
  isbn = {978-1-4503-5525-4},
  keywords = {declarative syntax definition,Disambiguation,empirical study,grammars,operator precedence},
  file = {/home/vipa/Zotero/storage/ADYE7IW2/Amorim et al. - 2017 - Deep priority conflicts in the wild a pilot study.pdf}
}

@inproceedings{anderssonProfileGuidedComposition2008,
  title = {Profile-{{Guided Composition}}},
  booktitle = {Software {{Composition}}},
  author = {Andersson, Jesper and Ericsson, Morgan and Kessler, Christoph and L{\"o}we, Welf},
  editor = {Pautasso, Cesare and Tanter, {\'E}ric},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {157--164},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-78789-1_12},
  abstract = {We present an approach that generates context-aware, optimized libraries of algorithms and data structures. The search space contains all combinations of implementation variants of algorithms and data structures including dynamically switching and converting between them. Based on profiling, the best implementation for a certain context is precomputed at deployment time and selected at runtime. In our experiments, the profile-guided composition outperforms the individual variants in almost all cases.},
  isbn = {978-3-540-78789-1},
  langid = {english},
  keywords = {Algorithm Implementation,Data Representation,Execution Time,Library Designer,Matrix Multiplication},
  file = {/home/vipa/Zotero/storage/TZYC488F/Andersson et al. - 2008 - Profile-Guided Composition.pdf}
}

@phdthesis{anselAutotuningProgramsAlgorithmic2014,
  type = {Thesis},
  title = {Autotuning Programs with Algorithmic Choice},
  author = {Ansel, Jason (Jason Andrew)},
  year = {2014},
  urldate = {2024-01-06},
  abstract = {The process of optimizing programs and libraries, both for performance and quality of service, can be viewed as a search problem over the space of implementation choices. This search is traditionally manually conducted by the programmer and often must be repeated when systems, tools, or requirements change. The overriding goal of this work is to automate this search so that programs can change themselves and adapt to achieve performance portability across different environments and requirements. To achieve this, first, this work presents the PetaBricks programming language which focuses on ways for expressing program implementation search spaces at the language level. Second, this work presents OpenTuner which provides sophisticated techniques for searching these search spaces in a way that can easily be adopted by other projects. PetaBricks is a implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. PetaBricks also introduces novel techniques to autotune algorithms for different convergence criteria or quality of service requirements. We show that the PetaBricks autotuner is often able to find non-intuitive poly-algorithms that outperform more traditional hand written solutions. OpenTuner is a open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. OpenTuner has been shown to perform well on complex search spaces up to 10{$^{3000}$} possible configurations in size.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2014-06-13T22:31:32Z},
  file = {/home/vipa/Zotero/storage/M7Q7VY89/Ansel - 2014 - Autotuning programs with algorithmic choice.pdf}
}

@inproceedings{anselEfficientEvolutionaryAlgorithm2011,
  title = {An Efficient Evolutionary Algorithm for Solving Incrementally Structured Problems},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Ansel, Jason and Pacula, Maciej and Amarasinghe, Saman and O'Reilly, Una-May},
  year = {2011},
  month = jul,
  series = {{{GECCO}} '11},
  pages = {1699--1706},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2001576.2001805},
  urldate = {2024-01-07},
  abstract = {Many real world problems have a structure where small problem instances are embedded within large problem instances, or where solution quality for large problem instances is loosely correlated to that of small problem instances. This structure can be exploited because smaller problem instances typically have smaller search spaces and are cheaper to evaluate. We present an evolutionary algorithm, INCREA, which is designed to incrementally solve a large, noisy, computationally expensive problem by deriving its initial population through recursively running itself on problem instances of smaller sizes. The INCREA algorithm also expands and shrinks its population each generation and cuts off work that doesn't appear to promise a fruitful result. For further efficiency, it addresses noisy solution quality efficiently by focusing on resolving it for small, potentially reusable solutions which have a much lower cost of evaluation. We compare INCREA to a general purpose evolutionary algorithm and find that in most cases INCREA arrives at the same solution in significantly less time.},
  isbn = {978-1-4503-0557-0},
  keywords = {autotuning,compiler,genetic algorithm},
  file = {/home/vipa/Zotero/storage/KVB34QK5/Ansel et al. - 2011 - An efficient evolutionary algorithm for solving in.pdf}
}

@article{anselPetaBricksLanguageCompiler2009,
  title = {{{PetaBricks}}: A Language and Compiler for Algorithmic Choice},
  shorttitle = {{{PetaBricks}}},
  author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
  year = {2009},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {44},
  number = {6},
  pages = {38--49},
  issn = {0362-1340},
  doi = {10.1145/1543135.1542481},
  urldate = {2020-01-17},
  abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
  keywords = {adaptive,algorithmic choice,autotuning,compiler,implicitly parallel,language},
  file = {/home/vipa/Zotero/storage/ZT7AS3BU/Ansel et al. - 2009 - PetaBricks a language and compiler for algorithmi.pdf}
}

@inproceedings{antwerpenConstraintLanguageStatic2016,
  title = {A {{Constraint Language}} for {{Static Semantic Analysis Based}} on {{Scope Graphs}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {van Antwerpen, Hendrik and N{\'e}ron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  year = {2016},
  series = {{{PEPM}} '16},
  pages = {49--60},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2847538.2847543},
  urldate = {2018-10-18},
  abstract = {In previous work, we introduced scope graphs as a formalism for describing program binding structure and performing name resolution in an AST-independent way. In this paper, we show how to use scope graphs to build static semantic analyzers. We use constraints extracted from the AST to specify facts about binding, typing, and initialization. We treat name and type resolution as separate building blocks, but our approach can handle language constructs---such as record field access---for which binding and typing are mutually dependent. We also refine and extend our previous scope graph theory to address practical concerns including ambiguity checking and support for a wider range of scope relationships. We describe the details of constraint generation for a model language that illustrates many of the interesting static analysis issues associated with modules and records.},
  isbn = {978-1-4503-4097-7},
  keywords = {Domain Specific Languages,Language Specification,Meta-Theory,Name Binding,Types},
  file = {/home/vipa/Zotero/storage/7LK3QKJ2/Antwerpen et al. - 2016 - A Constraint Language for Static Semantic Analysis.pdf}
}

@inproceedings{aspertiHintsUnification2009,
  title = {Hints in {{Unification}}},
  booktitle = {Theorem {{Proving}} in {{Higher Order Logics}}},
  author = {Asperti, Andrea and Ricciotti, Wilmer and Sacerdoti Coen, Claudio and Tassi, Enrico},
  editor = {Berghofer, Stefan and Nipkow, Tobias and Urban, Christian and Wenzel, Makarius},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {84--98},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-03359-9_8},
  abstract = {Several mechanisms such as Canonical Structures [14], Type Classes [13,16], or Pullbacks [10] have been recently introduced with the aim to improve the power and flexibility of the type inference algorithm for interactive theorem provers. We claim that all these mechanisms are particular instances of a simpler and more general technique, just consisting in providing suitable hints to the unification procedure underlying type inference. This allows a simple, modular and not intrusive implementation of all the above mentioned techniques, opening at the same time innovative and unexpected perspectives on its possible applications.},
  isbn = {978-3-642-03359-9},
  langid = {english},
  keywords = {Abstract Syntax,Canonical Structure,Proof Assistant,Record Type,Type Class},
  file = {/home/vipa/Zotero/storage/IHQTU84K/Asperti et al. - 2009 - Hints in Unification.pdf}
}

@inproceedings{augustssonParadiseTwostageDSL2008,
  title = {Paradise: {{A Two-stage DSL Embedded}} in {{Haskell}}},
  shorttitle = {Paradise},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  year = {2008},
  series = {{{ICFP}} '08},
  pages = {225--228},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1411204.1411236},
  urldate = {2018-10-18},
  abstract = {We have implemented a two-stage language, Paradise, for building reusable components which are used to price financial products. Paradise is embedded in Haskell and makes heavy use of type-class based overloading, allowing the second stage to be compiled into a variety of backend platforms. Paradise has enabled us to begin moving away from implementation directly in monolithic Excel spreadsheets and towards a more modular and retargetable approach.},
  isbn = {978-1-59593-919-7},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  file = {/home/vipa/Zotero/storage/EPSGJHGT/Augustsson et al. - 2008 - Paradise A Two-stage DSL Embedded in Haskell.pdf}
}

@article{augustssonParadiseTwostageDSL2008a,
  title = {Paradise: {{A Two-stage DSL Embedded}} in {{Haskell}}},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  year = {2008},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {43},
  number = {9},
  pages = {225--228},
  issn = {0362-1340},
  doi = {10.1145/1411203.1411236},
  keywords = {dsels,Haskell,metaprogramming,paradise}
}

@inproceedings{axelssonAnalyzingContextFreeGrammars2008,
  title = {Analyzing {{Context-Free Grammars Using}} an {{Incremental SAT Solver}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Axelsson, Roland and Heljanko, Keijo and Lange, Martin},
  editor = {Aceto, Luca and Damg{\aa}rd, Ivan and Goldberg, Leslie Ann and Halld{\'o}rsson, Magn{\'u}s M. and Ing{\'o}lfsd{\'o}ttir, Anna and Walukiewicz, Igor},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {410--422},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-70583-3_34},
  abstract = {We consider bounded versions of undecidable problems about context-free languages which restrict the domain of words to some finite length: inclusion, intersection, universality, equivalence, and ambiguity. These are in (co)-NP and thus solvable by a reduction to the (un-)satisfiability problem for propositional logic. We present such encodings -- fully utilizing the power of incrementat SAT solvers -- prove correctness and validate this approach with benchmarks.},
  isbn = {978-3-540-70583-3},
  langid = {english},
  keywords = {Ambiguous Word,Conjunctive Normal Form,Parse Tree,Propositional Formula,Word Problem},
  file = {/home/vipa/Zotero/storage/V9SK6FKI/Axelsson et al. - 2008 - Analyzing Context-Free Grammars Using an Increment.pdf}
}

@article{aycockEarlyActionEarley2009,
  title = {Early Action in an {{Earley}} Parser},
  author = {Aycock, John and Borsotti, Angelo},
  year = {2009},
  month = oct,
  journal = {Acta Informatica},
  volume = {46},
  number = {8},
  pages = {549},
  issn = {1432-0525},
  doi = {10.1007/s00236-009-0107-6},
  urldate = {2019-08-27},
  abstract = {Traditional Earley parsers operate in two phases: first recognizing the input, then constructing the forest of parse trees. Practically speaking, this quirk makes it awkward to use in a compiler-compiler, because semantic actions attached to rules are only executed after the fact. We address this problem by identifying safe Earley sets, points during the recognition phase at which partial parse trees can be constructed; this means that semantic actions may be executed on the fly. A secondary benefit is that Earley sets can be deleted during recognition, resulting in a substantial savings of both space and time.},
  langid = {english},
  keywords = {Causal Link,Early Action,Grammar Rule,Parse Tree,Semantic Action},
  file = {/home/vipa/Zotero/storage/XBYNWTKU/Aycock and Borsotti - 2009 - Early action in an Earley parser.pdf}
}

@article{aycockPracticalEarleyParsing2002,
  title = {Practical {{Earley Parsing}}},
  author = {Aycock, John and Horspool, R. Nigel},
  year = {2002},
  journal = {The Computer Journal},
  volume = {45},
  number = {6},
  pages = {620--630},
  issn = {0010-4620},
  doi = {10.1093/comjnl/45.6.620},
  abstract = {Earley's parsing algorithm is a general algorithm, able to handle any context-free grammar. As with most parsing algorithms, however, the presence of grammar rules having empty right-hand sides complicates matters. By analyzing why Earley's algorithm struggles with these grammar rules, we have devised a simple solution to the problem. Our empty-rule solution leads to a new type of finite automaton expressly suited for use in Earley parsers and to a new statement of Earley's algorithm. We show that this new form of Earley parser is much more time efficient in practice than the original.},
  keywords = {Algorithms,Article,Automation,Computing Milieux (General) (Ci),Copyrights,Grammars,Handles,Mathematical Analysis,Mathematical Models,Parsers,Parsing Algorithms},
  file = {/home/vipa/Zotero/storage/SNR548ZN/Aycock and Horspool - 2002 - Practical Earley Parsing.pdf}
}

@inproceedings{backusSyntaxSemanticsProposed1959,
  title = {The Syntax and the Semantics of the Proposed International Algebraic Language of the {{Zurich ACM-GAMM Conference}}},
  booktitle = {{{ICIP}} Proceedings},
  author = {Backus, John W},
  year = {1959},
  pages = {125--132},
  file = {/home/vipa/Zotero/storage/G9Y7JSNF/Backus - 1959 - The syntax and the semantics of the proposed inter.pdf}
}

@article{baggeDomainSpecificOptimisationUserDefined2003,
  title = {Domain-{{Specific Optimisation}} with {{User-Defined Rules}} in {{CodeBoost}}},
  author = {Bagge, Otto Skrove and Haveraaen, Magne},
  year = {2003},
  month = sep,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {4th {{International Workshop}} on {{Rule-Based Programming}} (in Connection with {{RDP}}'03, {{Federated Conference}} on {{Rewriting}}, {{Deduction}} and {{Programming}})},
  volume = {86},
  number = {2},
  pages = {119--133},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)80679-0},
  urldate = {2020-11-27},
  abstract = {The use of domain-specific optimisations can significantly enhance the performance of high-level programs. However, current programming languages have poor support for specifying such optimisations. In this paper, we introduce user-defined rules in CodeBoost. CodeBoost is a tool for source-to-source transformation of C++ programs. With CodeBoost, domain-specific optimisations can be specified as rewrite rules in C++-like syntax, within the C++ program, together with the classes where they apply. We also illustrate the effectiveness of user-defined rules and domain-specific optimisation on a real-world example application.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/2TNWW3YN/Bagge and Haveraaen - 2003 - Domain-Specific Optimisation with User-Defined Rul.pdf;/home/vipa/Zotero/storage/NF9JT7X5/S1571066104806790.html}
}

@article{baggeInterfacingConceptsWhy2010,
  title = {Interfacing {{Concepts}}: {{Why Declaration Style Shouldn}}'t {{Matter}}},
  shorttitle = {Interfacing {{Concepts}}},
  author = {Bagge, Anya Helene and Haveraaen, Magne},
  year = {2010},
  month = sep,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)},
  volume = {253},
  number = {7},
  pages = {37--50},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2010.08.030},
  urldate = {2020-02-17},
  abstract = {A concept (or signature) describes the interface of a set of abstract types by listing the operations that should be supported for those types. When implementing a generic operation, such as sorting, we may then specify requirements such as ``elements must be comparable'' by requiring that the element type models the Comparable concept. We may also use axioms to describe behaviour that should be common to all models of a concept. However, the operations specified by the concept are not always the ones that are best suited for the implementation. For example, numbers and matrices may both be addable, but adding two numbers is conveniently done by using a return value, whereas adding a sparse and a dense matrix is probably best achieved by modifying the dense matrix. In both cases, though, we may want to pretend we're using a simple function with a return value, as this most closely matches the notation we know from mathematics. This paper presents two simple concepts to break the notational tie between implementation and use of an operation: functionalisation, which derives a set of canonical pure functions from a procedure; and mutification, which translates calls using the functionalised declarations into calls to the implemented procedure.},
  langid = {english},
  keywords = {axioms,concept-based programming,concepts,functions,imperative vs functional,mutification,procedures},
  file = {/home/vipa/Zotero/storage/TGLZG3AL/Bagge and Haveraaen - 2010 - Interfacing Concepts Why Declaration Style Should.pdf;/home/vipa/Zotero/storage/QC6HILGH/S157106611000109X.html}
}

@inproceedings{basiosDarwinianDataStructure2018,
  title = {Darwinian Data Structure Selection},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Barr, Earl T.},
  year = {2018},
  month = oct,
  series = {{{ESEC}}/{{FSE}} 2018},
  pages = {118--128},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3236024.3236043},
  urldate = {2024-01-06},
  abstract = {Data structure selection and tuning is laborious but can vastly improve an application's performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86\% (37/43) of the projects. The median improvement across the best solutions is 4.8\%, 10.1\%, 5.1\% for runtime, memory and CPU usage. These aggregate results understate ARTEMIS's potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache's XML transformation tool. ARTEMIS improves gson by 16.5\%, 1\% and 2.2\% for memory, runtime, and CPU; ARTEMIS improves xalan's memory consumption by 23.5\%. Every client of these projects will benefit from these performance improvements.},
  isbn = {978-1-4503-5573-5},
  keywords = {Data Structure Optimisation,Genetic Improvement,Search-based Software Engineering,Software Analysis and Optimisation},
  file = {/home/vipa/Zotero/storage/V36RWPH4/Basios et al. - 2018 - Darwinian data structure selection.pdf}
}

@inproceedings{basiosOptimisingDarwinianData2017,
  title = {Optimising {{Darwinian Data Structures}} on {{Google Guava}}},
  booktitle = {Search {{Based Software Engineering}}},
  author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Barr, Earl T.},
  editor = {Menzies, Tim and Petke, Justyna},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {161--167},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-66299-2_14},
  abstract = {Data structure selection and tuning is laborious but can vastly improve application performance and memory footprint. In this paper, we demonstrate how artemis, a multiobjective, cloud-based optimisation framework can automatically find optimal, tuned data structures and how it is used for optimising the Guava library. From the proposed solutions that artemis found, \$\$27.45{\textbackslash}\%\$\$27.45\%of them improve all measures (execution time, CPU usage, and memory consumption). More specifically, artemis managed to improve the memory consumption of Guava by up 13\%, execution time by up~to 9\%, and 4\% CPU usage.},
  isbn = {978-3-319-66299-2},
  langid = {english},
  keywords = {Genetic improvement,Multi-objective optimisation,Search-based software engineering,Software analysis and optimisation},
  file = {/home/vipa/Zotero/storage/BH74SV4C/Basios et al. - 2017 - Optimising Darwinian Data Structures on Google Gua.pdf}
}

@phdthesis{bastenAmbiguityDetectionProgramming2011,
  title = {Ambiguity {{Detection}} for {{Programming Language Grammars}}},
  author = {Basten, Bas},
  year = {2011},
  month = dec,
  abstract = {Context-free grammars are the most suitable and most widely used method for describing the syntax of programming languages. They can be used to generate parsers, which transform a piece of source code into a tree-shaped representation of the code's syntactic structure. These parse trees can then be used for further processing or analysis of the source text. In this sense, grammars form the basis of many engineering and reverse engineering applications, like compilers, interpreters and tools for software analysis and transformation. Unfortunately, context-free grammars have the undesirable property that they can be ambiguous, which can seriously hamper their applicability. A grammar is ambiguous if at least one sentence in its language has more than one valid parse tree. Since the parse tree of a sentence is often used to infer its semantics, an ambiguous sentence can have multiple meanings. For programming languages this is almost always unintended. Ambiguity can therefore be seen as a grammar bug. A specific category of context-free grammars that is particularly sensitive to ambiguity are character-level grammars, which are used to generate scannerless parsers. Unlike traditional token-based grammars, character-level grammars include the full lexical definition of their language. This has the advantage that a language can be specified in a single formalism, and that no separate lexer or scanner phase is necessary in the parser. However, the absence of a scanner does require some additional lexical disambiguation. Character-level grammars can therefore be annotated with special disambiguation declarations to specify which parse trees to discard in case of ambiguity. Unfortunately, it is very hard to determine whether all ambiguities have been covered. The task of searching for ambiguities in a grammar is very complex and time consuming, and is therefore best automated. Since the invention of context-free grammars, several ambiguity detection methods have been developed to this end. However, the ambiguity problem for context-free grammars is undecidable in general, so the perfect detection method cannot exist. This implies a trade-off between accuracy and termination. Methods that apply exhaustive searching are able to correctly find all ambiguities, but they might never terminate. On the other hand, approximative search techniques do always produce an ambiguity report, but these might contain false positives or false negatives. Nevertheless, the fact that every method has flaws does not mean that ambiguity detection cannot be useful in practice. This thesis investigates ambiguity detection with the aim of checking grammars for programming languages. The challenge is to improve upon the state-of-the-art, by finding accurate enough methods that scale to realistic grammars. First we evaluate existing methods with a set of criteria for practical usability. Then we present various improvements to ambiguity detection in the areas of accuracy, performance and report quality. The main contributions of this thesis are two novel techniques. The first is an ambi- guity detection method that applies both exhaustive and approximative searching, called AMBIDEXTER. The key ingredient of AMBIDEXTER is a grammar filtering technique that can remove harmless production rules from a grammar. A production rule is harmless if it does not contribute to any ambiguity in the grammar. Any found harmless rules can therefore safely be removed. This results in a smaller grammar that still contains the same ambiguities as the original one. However, it can now be searched with exhaustive techniques in less time. The grammar filtering technique is formally proven correct, and experimentally validated. A prototype implementation is applied to a series of programming language grammars, and the performance of exhaustive detection methods are measured before and after filtering. The results show that a small investment in filtering time can substantially reduce the run-time of exhaustive searching, sometimes with several orders of magnitude. After this evaluation on token-based grammars, the grammar filtering technique is extended for use with character-level grammars. The extensions deal with the increased complexity of these grammars, as well as their disambiguation declarations. This enables the detection of productions that are harmless due to disambiguation. The extentions are experimentally validated on another set of programming language grammars from practice, with similar results as before. Measurements show that, even though character-level grammars are more expensive to filter, the investment is still very worthwhile. Exhaustive search times were again reduced substantially. The second main contribution of this thesis is DR. AMBIGUITY, an expert system to help grammar developers to understand and solve found ambiguities. If applied to an ambiguous sentence, DR. AMBIGUITY analyzes the causes of the ambiguity and proposes a number of applicable solutions. A prototype implementation is presented and evaluated with a mature Java grammar. After removing disambiguation declarations from the grammar we analyze sentences that have become ambiguous by this removal. The results show that in all cases the removed filter is proposed by DR. AMBIGUITY as a possible cure for the ambiguity. Concluding, this thesis improves ambiguity detection with two novel methods. The first is the ambiguity detection method AMBIDEXTER that applies grammar filtering to substantially speed up exhaustive searching. The second is the expert system DR. AMBIGUITY that automatically analyzes found ambiguities and proposes applicable cures. The results obtained with both methods show that automatic ambiguity detection is now ready for realistic programming language grammars.},
  hal_id = {tel-00644079},
  hal_version = {v1},
  school = {Universiteit van Amsterdam},
  keywords = {ambiguity detection,context-free grammars,programming languages,scannerless,static analysis},
  file = {/home/vipa/Zotero/storage/SZG9TBEY/Basten - 2011 - Ambiguity Detection for Programming Language Gramm.pdf;/home/vipa/Zotero/storage/IK7K99UC/tel-00644079.html}
}

@inproceedings{bastenTrackingOriginsAmbiguity2010,
  title = {Tracking {{Down}} the {{Origins}} of {{Ambiguity}} in {{Context-Free Grammars}}},
  booktitle = {Theoretical {{Aspects}} of {{Computing}} -- {{ICTAC}} 2010},
  author = {Basten, H. J. S.},
  editor = {Cavalcanti, Ana and Deharbe, David and Gaudel, Marie-Claude and Woodcock, Jim},
  year = {2010},
  pages = {76--90},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14808-8_6},
  abstract = {Context-free grammars are widely used but still hindered by ambiguity. This stresses the need for detailed detection methods that point out the sources of ambiguity in a grammar. In this paper we show how the approximative Noncanonical Unambiguity Test by Schmitz can be extended to conservatively identify production rules that do not contribute to the ambiguity of a grammar. We prove the correctness of our approach and consider its practical applicability.},
  isbn = {978-3-642-14808-8},
  langid = {english},
  file = {/home/vipa/Zotero/storage/VQMU27SR/Basten - 2010 - Tracking Down the Origins of Ambiguity in Context-.pdf}
}

@misc{bergerACMSIGPLANEmpirical2018,
  title = {{{ACM SIGPLAN Empirical Evaluation Guidelines}}},
  author = {Berger, Emery and Blackburn, Steve and Hauswirth, Matthias and Hicks, Michael},
  year = {2018}
}

@inproceedings{binCodeGenerationAbstract2015,
  title = {Code Generation for Abstract Data Types Based on Program Analysis},
  booktitle = {2015 {{IEEE}} International Conference on Software Quality, Reliability and Security - Companion},
  author = {Bin, Li and Jun, Liu and Jianhua, Zhao},
  year = {2015},
  pages = {153--160},
  doi = {10.1109/QRS-C.2015.42},
  file = {/home/vipa/Zotero/storage/TESKMMCS/Code Generation for Abstract Data Types Based on P.pdf}
}

@phdthesis{birmanTmgRecognitionSchema1970,
  title = {The Tmg Recognition Schema},
  author = {Birman, Alexander},
  year = {1970},
  school = {Princeton University}
}

@inproceedings{bjesseLavaHardwareDesign1998,
  title = {Lava: {{Hardware Design}} in {{Haskell}}},
  shorttitle = {Lava},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Bjesse, Per and Claessen, Koen and Sheeran, Mary and Singh, Satnam},
  year = {1998},
  series = {{{ICFP}} '98},
  pages = {174--184},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/289423.289440},
  urldate = {2018-10-18},
  abstract = {Lava is a tool to assist circuit designers in specifying, designing, verifying and implementing hardware. It is a collection of Haskell modules. The system design exploits functional programming language features, such as monads and type classes, to provide multiple interpretations of circuit descriptions. These interpretations implement standard circuit analyses such as simulation, formal verification and the generation of code for the production of real circuits.Lava also uses polymorphism and higher order functions to provide more abstract and general descriptions than are possible in traditional hardware description languages. Two Fast Fourier Transform circuit examples illustrate this.},
  isbn = {978-1-58113-024-9},
  file = {/home/vipa/Zotero/storage/SPFXCFX6/Bjesse et al. - 1998 - Lava Hardware Design in Haskell.pdf}
}

@article{boehmRopesAlternativeStrings1995,
  title = {Ropes: {{An}} Alternative to Strings},
  shorttitle = {Ropes},
  author = {Boehm, Hans-J. and Atkinson, Russ and Plass, Michael},
  year = {1995},
  journal = {Software: Practice and Experience},
  volume = {25},
  number = {12},
  pages = {1315--1330},
  issn = {1097-024X},
  doi = {10.1002/spe.4380251203},
  urldate = {2023-11-02},
  abstract = {Programming languages generally provide a `string' or `text' type to allow manipulation of sequences of characters. This type is usually of crucial importance, since it is normally mentioned in most interfaces between system components. We claim that the traditional implementations of strings, and often the supported functionality, are not well suited to such general-purpose use. They should be confined to applications with specific, and unusual, performance requirements. We present `ropes' or `heavyweight' strings as an alternative that, in our experience leads to systems that are more robust, both in functionality and in performance. Ropes have been in use in the Cedar environment almost since its inception, but this appears to be neither well-known, nor discussed in the literature. The algorithms have been gradually refined. We have also recently built a second similar, but somewhat lighter weight, C-language implementation, which is included in our publically released garbage collector distribution. We describe the algorithms used in both, and give some performance measurements for the C version.},
  langid = {english},
  keywords = {balanced trees,C,Cedar,character strings,concatenation,immutable},
  file = {/home/vipa/Zotero/storage/C7HMNQSN/spe.html}
}

@inproceedings{brabrandAnalyzingAmbiguityContextFree2007,
  title = {Analyzing {{Ambiguity}} of {{Context-Free Grammars}}},
  booktitle = {Implementation and {{Application}} of {{Automata}}},
  author = {Brabrand, Claus and Giegerich, Robert and M{\o}ller, Anders},
  editor = {Holub, Jan and {\v Z}{\v d}{\'a}rek, Jan},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {214--225},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-76336-9_21},
  abstract = {It has been known since 1962 that the ambiguity problem for context-free grammars is undecidable. Ambiguity in context-free grammars is a recurring problem in language design and parser generation, as well as in applications where grammars are used as models of real-world physical structures.We observe that there is a simple linguistic characterization of the grammar ambiguity problem, and we show how to exploit this to conservatively approximate the problem based on local regular approximations and grammar unfoldings. As an application, we consider grammars that occur in RNA analysis in bioinformatics, and we demonstrate that our static analysis of context-free grammars is sufficiently precise and efficient to be practically useful.},
  isbn = {978-3-540-76336-9},
  langid = {english},
  keywords = {CFG ambiguity,regular approximation,RNA analysis},
  file = {/home/vipa/Zotero/storage/LTUIAIMJ/Brabrand et al. - 2007 - Analyzing Ambiguity of Context-Free Grammars.pdf}
}

@inproceedings{brabrandTypedUnambiguousPattern2010,
  title = {Typed and {{Unambiguous Pattern Matching}} on {{Strings Using Regular Expressions}}},
  booktitle = {Proceedings of the 12th {{International ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  author = {Brabrand, Claus and Thomsen, Jakob G.},
  year = {2010},
  series = {{{PPDP}} '10},
  pages = {243--254},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1836089.1836120},
  urldate = {2018-11-19},
  abstract = {We show how to achieve typed and unambiguous declarative pattern matching on strings using regular expressions extended with a simple recording operator. We give a characterization of ambiguity of regular expressions that leads to a sound and complete static analysis. The analysis is capable of pinpointing all ambiguities in terms of the structure of the regular expression and report shortest ambiguous strings. We also show how pattern matching can be integrated into statically typed programming languages for deconstructing strings and reproducing typed and structured values. We validate our approach by giving a full implementation of the approach presented in this paper. The resulting tool, reg-exp-rec, adds typed and unambiguous pattern matching to Java in a standalone and non-intrusive manner. We evaluate the approach using several realistic examples.},
  isbn = {978-1-4503-0132-9},
  keywords = {ambiguity,disambiguation,parsing,pattern matching,regular expressions,static analysis,type inference},
  file = {/home/vipa/Zotero/storage/U2CTN8FZ/Brabrand and Thomsen - 2010 - Typed and Unambiguous Pattern Matching on Strings .pdf}
}

@inproceedings{brandDisambiguationFiltersScannerless2002,
  title = {Disambiguation {{Filters}} for {{Scannerless Generalized LR Parsers}}},
  booktitle = {Compiler {{Construction}}},
  author = {van den Brand, Mark G. J. and Scheerder, Jeroen and Vinju, Jurgen J. and Visser, Eelco},
  editor = {Horspool, R. Nigel},
  year = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {143--158},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45937-5_12},
  abstract = {In this paper we present the fusion of generalized LR parsing and scannerless parsing. This combination supports syntax definitions in which all aspects (lexical and context-free) of the syntax of a language are defined explicitly in one formalism. Furthermore, there are no restrictions on the class of grammars, thus allowing a natural syntax tree structure. Ambiguities that arise through the use of unrestricted grammars are handled by explicit disambiguation constructs, instead of implicit defaults that are taken by traditional scanner and parser generators. Hence, a syntax definition becomes a full declarative description of a language. Scannerless generalized LR parsing is a viable technique that has been applied in various industrial and academic projects.},
  isbn = {978-3-540-45937-8},
  langid = {english},
  keywords = {Generalize Parsing,Lexical Ambiguity,Longe Match,Parse Table,Parse Tree},
  file = {/home/vipa/Zotero/storage/26Q5SB32/van den Brand et al. - 2002 - Disambiguation Filters for Scannerless Generalized.pdf}
}

@article{brandonBetterDefunctionalizationLambda2023,
  title = {Better {{Defunctionalization}} through {{Lambda Set Specialization}}},
  author = {Brandon, William and Driscoll, Benjamin and Dai, Frank and Berkow, Wilson and Milano, Mae},
  year = {2023},
  month = jun,
  journal = {Reproduction Package for Article "Better Defunctionalization Through Lambda Set Specialization"},
  volume = {7},
  number = {PLDI},
  pages = {146:977--146:1000},
  doi = {10.1145/3591260},
  urldate = {2024-07-26},
  abstract = {Higher-order functions pose a challenge for both static program analyses and optimizing compilers.   To simplify the analysis and compilation of languages with higher-order functions, a rich body of   prior work has proposed a variety of defunctionalization techniques, which can eliminate   higher-order functions from a program by transforming the program to a semantically-equivalent   first-order representation. Several modern languages take this a step further, specializing   higher-order functions with respect to the functions on which they operate, and in turn allowing   compilers to generate more efficient code. However, existing specializing defunctionalization   techniques restrict how function values may be used, forcing implementations to fall back on costly   dynamic alternatives. We propose lambda set specialization (LSS), the first specializing   defunctionalization technique which imposes no restrictions on how function values may be used. We   formulate LSS in terms of a polymorphic type system which tracks the flow of function values through   the program, and use this type system to recast specialization of higher-order functions with   respect to their arguments as a form of type monomorphization. We show that our type system admits a   simple and tractable type inference algorithm, and give a formalization and fully-mechanized proof   in the Isabelle/HOL proof assistant showing soundness and completeness of the type inference   algorithm with respect to the type system. To show the benefits of LSS, we evaluate its impact on   the run time performance of code generated by the MLton compiler for Standard ML, the OCaml   compiler, and the new Morphic functional programming language. We find that pre-processing with LSS   achieves run time speedups of up to 6.85x under MLton, 3.45x for OCaml, and 78.93x for Morphic.},
  file = {/home/vipa/Zotero/storage/CI7UW6Y8/Brandon et al. - 2023 - Better Defunctionalization through Lambda Set Spec.pdf}
}

@inproceedings{bravenboerGeneralizedTypeBasedDisambiguation2005,
  title = {Generalized {{Type-Based Disambiguation}} of {{Meta Programs}} with {{Concrete Object Syntax}}},
  booktitle = {Generative {{Programming}} and {{Component Engineering}}},
  author = {Bravenboer, Martin and Vermaas, Rob and Vinju, Jurgen and Visser, Eelco},
  editor = {Gl{\"u}ck, Robert and Lowry, Michael},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {157--172},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11561347_12},
  abstract = {In meta programming with concrete object syntax, object-level programs are composed from fragments written in concrete syntax. The use of small program fragments in such quotations and the use of meta-level expressions within these fragments (anti-quotation) often leads to ambiguities. This problem is usually solved through explicit disambiguation, resulting in considerable syntactic overhead. A few systems manage to reduce this overhead by using type information during parsing. Since this is hard to achieve with traditional parsing technology, these systems provide specific combinations of meta and object languages, and their implementations are difficult to reuse. In this paper, we generalize these approaches and present a language independent method for introducing concrete object syntax without explicit disambiguation. The method uses scannerless generalized-LR parsing to parse meta programs with embedded object-level fragments, which produces a forest of all possible parses. This forest is reduced to a tree by a disambiguating type checker for the meta language. To validate our method we have developed embeddings of several object languages in Java, including AspectJ and Java itself.},
  isbn = {978-3-540-31977-1},
  langid = {english},
  keywords = {Abstract Syntax,Concrete Syntax,Method Invocation,Object Language,Type Checker},
  file = {/home/vipa/Zotero/storage/YK6JC6YW/Bravenboer et al. - 2005 - Generalized Type-Based Disambiguation of Meta Prog.pdf}
}

@article{bravenboerStrategoXT172008,
  title = {Stratego/{{XT}} 0.17. {{A Language}} and {{Toolset}} for {{Program Transformation}}},
  author = {Bravenboer, Martin and Kalleberg, Karl Trygve and Vermaas, Rob and Visser, Eelco},
  year = {2008},
  month = jun,
  journal = {Sci. Comput. Program.},
  volume = {72},
  number = {1-2},
  pages = {52--70},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.11.003},
  urldate = {2018-10-18},
  abstract = {Stratego/XT is a language and toolset for program transformation. The Stratego language provides rewrite rules for expressing basic transformations, programmable rewriting strategies for controlling the application of rules, concrete syntax for expressing the patterns of rules in the syntax of the object language, and dynamic rewrite rules for expressing context-sensitive transformations, thus supporting the development of transformation components at a high level of abstraction. The XT toolset offers a collection of flexible, reusable transformation components, and tools for generating such components from declarative specifications. Complete program transformation systems are composed from these components. This paper gives an overview of Stratego/XT 0.17, including a description of the Stratego language and XT transformation tools; a discussion of the implementation techniques and software engineering process; and a description of applications built with Stratego/XT.},
  keywords = {Concrete syntax,Dynamic rewrite rules,Program transformation,Rewrite rules,Rewriting strategies,Stratego,Stratego/XT},
  file = {/home/vipa/Zotero/storage/UQGBYURH/Bravenboer et al. - 2008 - StrategoXT 0.17. A Language and Toolset for Progr.pdf}
}

@inproceedings{bromanGraduallyTypedSymbolic2018,
  title = {Gradually {{Typed Symbolic Expressions}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Broman, David and Siek, Jeremy G.},
  year = {2018},
  series = {{{PEPM}} '18},
  pages = {15--29},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3162068},
  urldate = {2018-10-18},
  abstract = {Embedding a domain-specific language (DSL) in a general purpose host language is an efficient way to develop a new DSL. Various kinds of languages and paradigms can be used as host languages, including object-oriented, functional, statically typed, and dynamically typed variants, all having their pros and cons. For deep embedding, statically typed languages enable early checking and potentially good DSL error messages, instead of reporting runtime errors. Dynamically typed languages, on the other hand, enable flexible transformations, thus avoiding extensive boilerplate code. In this paper, we introduce the concept of gradually typed symbolic expressions that mix static and dynamic typing for symbolic data. The key idea is to combine the strengths of dynamic and static typing in the context of deep embedding of DSLs. We define a gradually typed calculus {$\lambda<\star>$}, formalize its type system and dynamic semantics, and prove type safety. We introduce a host language called Modelyze that is based on {$\lambda<\star>$}, and evaluate the approach by embedding a series of equation-based domain-specific modeling languages, all within the domain of physical modeling and simulation.},
  isbn = {978-1-4503-5587-2},
  keywords = {DSL,Symbolic expressions,Type systems},
  file = {/home/vipa/Zotero/storage/5WK3GCIK/Broman and Siek - 2018 - Gradually Typed Symbolic Expressions.pdf}
}

@techreport{bromanModelyzeGraduallyTyped2012,
  title = {Modelyze: A {{Gradually Typed Host Language}} for {{Embedding Equation-Based Modeling Languages}}},
  author = {Broman, David and Siek, Jeremy G.},
  year = {2012},
  month = jun,
  number = {UCB/EECS-2012-173},
  institution = {EECS Department, University of California, Berkeley},
  file = {/home/vipa/Zotero/storage/75BXDBYN/Broman and Siek - Modelyze a Gradually Typed Host Language for Embe.pdf}
}

@inproceedings{bromanVisionMikingInteractive2019,
  title = {A Vision of Miking: Interactive Programmatic Modeling, Sound Language Composition, and Self-Learning Compilation},
  shorttitle = {A Vision of Miking},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Software Language Engineering}}},
  author = {Broman, David},
  year = {2019},
  month = oct,
  series = {{{SLE}} 2019},
  pages = {55--60},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3357766.3359531},
  urldate = {2020-11-13},
  abstract = {This paper introduces a vision of Miking, a language framework for constructing efficient and sound language environments and compilers for domain-specific modeling languages. In particular, this language framework has three key objectives: (i) to automatically generate interactive programmatic modeling environments, (ii) to guarantee sound compositions of language fragments that enable both rapid and safe domain-specific language development, (iii) to include first-class support for self-learning compilation, targeting heterogeneous execution platforms. The initiative is motivated in the domain of mathematical modeling languages. Specifically, two different example domains are discussed: (i) modeling, simulation, and verification of cyber-physical systems, and (ii) domain-specific differentiable probabilistic programming. The paper describes the main objectives of the vision, as well as concrete research challenges and research directions.},
  isbn = {978-1-4503-6981-7},
  keywords = {compilers,composition,domain-specific languages,machine learning,modeling languages,semantics},
  file = {/home/vipa/Zotero/storage/5VCHVKJH/Broman - 2019 - A vision of miking interactive programmatic model.pdf}
}

@inproceedings{burzanskaIntermediateStructureReduction2010,
  title = {Intermediate {{Structure Reduction Algorithms}} for {{Stack Based Query Languages}}},
  booktitle = {Advances in {{Software Engineering}}},
  author = {Burza{\'n}ska, Marta and Stencel, Krzysztof and Wi{\'s}niewski, Piotr},
  editor = {Kim, Tai-hoon and Kim, Haeng-Kon and Khan, Muhammad Khurram and Kiumi, Akingbehin and Fang, Wai-chi and {\'S}l{\k e}zak, Dominik},
  year = {2010},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {317--326},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-17578-7_31},
  abstract = {Data processing often results in generation of a lot of temporary structures. They cause an increase in processing time and resources consumption. This especially concerns databases since their temporary data are huge and often they must be dumped to secondary storage. This situation has a serious impact on the query engine. An interesting technique of program fusion has been proposed for functional programming languages. Its goal is to reduce the size or entirely eliminate intermediate structures. In this paper we show how this technique can be used to generate robust execution plans of aggregate and recursive queries of query languages based on Stack Based Approach. We will use SBQL as an exemplary language.},
  isbn = {978-3-642-17578-7},
  langid = {english},
  keywords = {Intermediate structures,optimization,program fusion,rewriting algorithms,SBQL},
  file = {/home/vipa/Zotero/storage/CH49BTN9/Burzańska et al. - 2010 - Intermediate Structure Reduction Algorithms for St.pdf}
}

@inproceedings{calimeriOptimizingAnswerSet2018,
  title = {Optimizing {{Answer Set Computation}} via {{Heuristic-Based Decomposition}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {Calimeri, Francesco and Fusc{\`a}, Davide and Perri, Simona and Zangari, Jessica},
  editor = {Calimeri, Francesco and Hamlen, Kevin and Leone, Nicola},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {135--151},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-73305-0_9},
  abstract = {Answer Set Programming (ASP) is a purely declarative formalism developed in the field of logic programming and nonmonotonic reasoning. With ASP, computational problems are encoded by logic programs whose answer sets, corresponding to solutions, are computed by an ASP system. In general, several semantically equivalent programs might be defined for the same problem; however, performance of ASP systems while evaluating them might significantly vary. We propose an approach for automatically transforming an input logic program into an equivalent one that can be evaluated more efficiently. To this aim, one can make use of existing tree-decomposition techniques for rewriting selected rules into a set of multiple ones. The idea is to guide and adaptively apply them on the basis of proper new heuristics, in order to obtain a smart rewriting algorithm to be integrated into an ASP system. The method is rather general: it can be adapted to the system at hand and implement different preference policies. Furthermore, we define a set of new heuristics explicitly tailored at optimizing one of the main phases of the typical ASP computation, namely the grounding process; we make use of them in order to actually implement the approach into the ASP system DLV, and in particular into its grounding subsystem II{\textbackslash}mathcal I-DLV, and carry out an extensive experimental activity aimed at assessing the impact of the proposal.},
  isbn = {978-3-319-73305-0},
  langid = {english},
  keywords = {Answer Set Programming,Artificial intelligence,ASP in practice},
  file = {/home/vipa/Zotero/storage/4B2RIXKQ/Calimeri et al. - 2018 - Optimizing Answer Set Computation via Heuristic-Ba.pdf}
}

@article{cantorAmbiguityProblemBackus1962,
  title = {On {{The Ambiguity Problem}} of {{Backus Systems}}},
  author = {Cantor, David G.},
  year = {1962},
  month = oct,
  journal = {Journal of the ACM},
  volume = {9},
  number = {4},
  pages = {477--479},
  issn = {0004-5411},
  doi = {10.1145/321138.321145},
  urldate = {2018-10-18},
  abstract = {Backus [1] has developed an elegant method of defining well-formed formulas for computer languages such as ALGOL. It consists of (our notation is slightly different from that of Backus):    A finite alphabet: a1, a2, {\dots}, at; Predicates: P1, P2, {\dots}, P{$\epsilon$}; Productions, either of the form (a) aj {$\in$} Pi;},
  file = {/home/vipa/Zotero/storage/9CEMZWYX/Cantor - 1962 - On The Ambiguity Problem of Backus Systems.pdf}
}

@article{caralpTrimmingVisiblyPushdown2015,
  title = {Trimming Visibly Pushdown Automata},
  author = {Caralp, Mathieu and Reynier, Pierre-Alain and Talbot, Jean-Marc},
  year = {2015},
  month = may,
  journal = {Theoretical Computer Science},
  series = {Implementation and {{Application}} of {{Automata}}},
  volume = {578},
  pages = {13--29},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2015.01.018},
  urldate = {2019-07-03},
  abstract = {We study the problem of trimming visibly pushdown automata (VPA). We first describe a polynomial time procedure which, given a visibly pushdown automaton that accepts only well-nested words, returns an equivalent visibly pushdown automaton that is trimmed. We then show how this procedure can be lifted to the setting of arbitrary VPA. Furthermore, we present a way of building, given a VPA, an equivalent VPA which is both deterministic and trimmed. Last, our trimming procedures can be applied to weighted VPA.},
  langid = {english},
  keywords = {Polynomial Time,Polynomial Time Complexity,Pushdown Automaton,Return Transition,Tree Automaton,Trimming,Visibly pushdown automata},
  file = {/home/vipa/Zotero/storage/WXNLNVWD/Caralp et al. - 2015 - Trimming visibly pushdown automata.pdf;/home/vipa/Zotero/storage/9DJLR73G/S0304397515000456.html}
}

@inproceedings{castagnaSettheoreticTypesPolymorphic2016,
  title = {Set-Theoretic Types for Polymorphic Variants},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Castagna, Giuseppe and Petrucciani, Tommaso and Nguyễn, Kim},
  year = {2016},
  month = sep,
  series = {{{ICFP}} 2016},
  pages = {378--391},
  publisher = {Association for Computing Machinery},
  address = {Nara, Japan},
  doi = {10.1145/2951913.2951928},
  urldate = {2020-05-15},
  abstract = {Polymorphic variants are a useful feature of the OCaml language whose current definition and implementation rely on kinding constraints to simulate a subtyping relation via unification. This yields an awkward formalization and results in a type system whose behaviour is in some cases unintuitive and/or unduly restrictive. In this work, we present an alternative formalization of polymorphic variants, based on set-theoretic types and subtyping, that yields a cleaner and more streamlined system. Our formalization is more expressive than the current one (it types more programs while preserving type safety), it can internalize some meta-theoretic properties, and it removes some pathological cases of the current implementation resulting in a more intuitive and, thus, predictable type system. More generally, this work shows how to add full-fledged union types to functional languages of the ML family that usually rely on the Hindley-Milner type system. As an aside, our system also improves the theory of semantic subtyping, notably by proving completeness for the type reconstruction algorithm.},
  isbn = {978-1-4503-4219-3},
  keywords = {type constraints,Type reconstruction,union types},
  file = {/home/vipa/Zotero/storage/HSTN5WLM/Castagna et al. - 2016 - Set-theoretic types for polymorphic variants.pdf}
}

@inproceedings{chavanDBridgeProgramRewrite2011,
  title = {{{DBridge}}: {{A}} Program Rewrite Tool for Set-Oriented Query Execution},
  shorttitle = {{{DBridge}}},
  booktitle = {2011 {{IEEE}} 27th {{International Conference}} on {{Data Engineering}}},
  author = {Chavan, M. and Guravannavar, R. and Ramachandra, K. and Sudarshan, S.},
  year = {2011},
  month = apr,
  pages = {1284--1287},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2011.5767949},
  abstract = {We present DBridge, a novel static analysis and program transformation tool to optimize database access. Traditionally, rewrite of queries and programs are done independently, by the database query optimzier and the language compiler respectively, leaving out many optimization opportunities. Our tool aims to bridge this gap by performing holistic transformations, which include both program and query rewrite. Many applications invoke database queries multiple times with different parameter values. Such query invocations made using imperative loops are often the cause of poor performance due to random I/O and round trip delays. In practice, such performance issues are addressed by manually rewriting the application to make it set oriented. Such manual rewriting of programs is often time consuming and error prone. Guravannavar et. al. propose program analysis and transformation techniques for automatically rewriting an application to make it set oriented. DBridge implements these program transformation techniques for Java programs that use JDBC to access database. In this demonstration, we showcase the holistic program/query transformations that DBridge can perform, over a variety of scenarios taken from real-world applications. We then walk through the design of DBridge, which uses the SOOT optimization framework for static analysis. Finally, we demonstrate the performance gains achieved through the transformations.},
  keywords = {Context,database access optimization,database query optimizer,Database systems,DBridge,Decorrelation,imperative loops,Java,Java programs,language compiler,Performance gain,program analysis,program compilers,program diagnostics,program rewrite tool,program transformation tool,query processing,set-oriented query execution,SOOT optimization framework,static analysis,Transforms},
  file = {/home/vipa/Zotero/storage/G2BBLHGI/Chavan et al. - 2011 - DBridge A program rewrite tool for set-oriented q.pdf;/home/vipa/Zotero/storage/UW7V35EV/5767949.html}
}

@incollection{chomskyAlgebraicTheoryContextFree1963,
  title = {The {{Algebraic Theory}} of {{Context-Free Languages}}*},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  author = {Chomsky, N. and Sch{\"u}tzenberger, M. P.},
  editor = {Braffort, P. and Hirschberg, D.},
  year = {1963},
  month = jan,
  series = {Computer {{Programming}} and {{Formal Systems}}},
  volume = {35},
  pages = {118--161},
  publisher = {Elsevier},
  doi = {10.1016/S0049-237X(08)72023-8},
  urldate = {2018-10-18},
  abstract = {This chapter discusses the several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. By a language it simply mean a set of strings in some finite set V of symbols called the vocabulary of the language. By a grammar a set of rules that give a recursive enumeration of the strings belonging to the language. It can be said that the grammar generates these strings. The chapter discusses the aspect of the structural description of a sentence, namely, its subdivision into phrases belonging to various categories. A major concern of the general theory of natural languages is to define the class of possible strings; the class of possible grammars; the class of possible structural descriptions; a procedure for assigning structural descriptions to sentences, given a grammar; and to do all of this in such a way that the structural description assigned to a sentence by the grammar of a natural language will provide the basis for explaining how a speaker of this language would understand this sentence.},
  file = {/home/vipa/Zotero/storage/3TMVYR9W/Chomsky and Schützenberger - 1963 - The Algebraic Theory of Context-Free Languages.pdf;/home/vipa/Zotero/storage/3BXH7TYT/S0049237X08720238.html}
}

@article{chomskyThreeModelsDescription1956,
  title = {Three Models for the Description of Language},
  author = {Chomsky, N.},
  year = {1956},
  month = sep,
  journal = {IRE Transactions on Information Theory},
  volume = {2},
  number = {3},
  pages = {113--124},
  issn = {2168-2712},
  doi = {10.1109/TIT.1956.1056813},
  urldate = {2024-04-25},
  abstract = {We investigate several conceptions of linguistic structure to determine whether or not they can provide simple and "revealing" grammars that generate all of the sentences of English and only these. We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar. Furthermore, the particular subclass of such processes that producen-order statistical approximations to English do not come closer, with increasingn, to matching the output of an English grammar. We formalize-the notions of "phrase structure" and show that this gives us a method for describing language which is essentially more powerful, though still representable as a rather elementary type of finite-state process. Nevertheless, it is successful only when limited to a small subset of simple sentences. We study the formal properties of a set of grammatical transformations that carry sentences with phrase structure into new sentences with derived phrase structure, showing that transformational grammars are processes of the same elementary type as phrase-structure grammars; that the grammar of English is materially simplified if phrase structure description is limited to a kernel of simple sentences from which all other sentences are constructed by repeated transformations; and that this view of linguistic structure gives a certain insight into the use and understanding of language.},
  keywords = {Impedance matching,Kernel,Laboratories,Markov processes,Natural languages,Research and development,Testing},
  file = {/home/vipa/Zotero/storage/88SSAA42/Chomsky - 1956 - Three models for the description of language.pdf;/home/vipa/Zotero/storage/4SIJQE27/1056813.html}
}

@inproceedings{chouAutomaticGenerationEfficient2020,
  title = {Automatic Generation of Efficient Sparse Tensor Format Conversion Routines},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
  year = {2020},
  month = jun,
  series = {{{PLDI}} 2020},
  pages = {823--838},
  publisher = {Association for Computing Machinery},
  address = {London, UK},
  doi = {10.1145/3385412.3385963},
  urldate = {2020-06-08},
  abstract = {This paper shows how to generate code that efficiently converts sparse tensors between disparate storage formats (data layouts) such as CSR, DIA, ELL, and many others. We decompose sparse tensor conversion into three logical phases: coordinate remapping, analysis, and assembly. We then develop a language that precisely describes how different formats group together and order a tensor's nonzeros in memory. This lets a compiler emit code that performs complex remappings of nonzeros when converting between formats. We also develop a query language that can extract statistics about sparse tensors, and we show how to emit efficient analysis code that computes such queries. Finally, we define an abstract interface that captures how data structures for storing a tensor can be efficiently assembled given specific statistics about the tensor. Disparate formats can implement this common interface, thus letting a compiler emit optimized sparse tensor conversion code for arbitrary combinations of many formats without hard-coding for any specific combination. Our evaluation shows that the technique generates sparse tensor conversion routines with performance between 1.00 and 2.01{\texttimes} that of hand-optimized versions in SPARSKIT and Intel MKL, two popular sparse linear algebra libraries. And by emitting code that avoids materializing temporaries, which both libraries need for many combinations of source and target formats, our technique outperforms those libraries by 1.78 to 4.01{\texttimes} for CSC/COO to DIA/ELL conversion.},
  isbn = {978-1-4503-7613-6},
  keywords = {attribute query language,coordinate remapping notation,sparse tensor algebra,sparse tensor assembly,sparse tensor conversion,sparse tensor formats}
}

@article{chuExploitingSubproblemDominance2012,
  title = {Exploiting Subproblem Dominance in Constraint Programming},
  author = {Chu, Geoffrey and {de la Banda}, Maria Garcia and Stuckey, Peter J.},
  year = {2012},
  month = jan,
  journal = {Constraints},
  volume = {17},
  number = {1},
  pages = {1--38},
  issn = {1572-9354},
  doi = {10.1007/s10601-011-9112-9},
  urldate = {2023-10-03},
  abstract = {Many search problems contain large amounts of redundancy in the search. In this paper we examine how to automatically exploit subproblem dominance, which arises when different partial assignments lead to subproblems that dominate (or are dominated by) other subproblems. Subproblem dominance is exploited by caching subproblems that have already been explored by the search, using keys that characterise the subproblems, and failing the search when the current subproblem is dominated by a subproblem already in the cache. In this paper we show how we can automatically and efficiently define keys for arbitrary constraint problems using constraint projection. We show how, for search problems where subproblem dominance arises, a constraint programming solver with this capability can solve these problems orders of magnitude faster than solvers without caching. The system is fully automatic, i.e., subproblem dominance is detected and exploited without any effort from the problem modeller.},
  langid = {english},
  keywords = {Caching,Dominance,Search},
  file = {/home/vipa/Zotero/storage/MF7ACLUJ/Chu et al. - 2012 - Exploiting subproblem dominance in constraint prog.pdf}
}

@inproceedings{claessenQuickCheckLightweightTool2000,
  title = {{{QuickCheck}}: A Lightweight Tool for Random Testing of {{Haskell}} Programs},
  shorttitle = {{{QuickCheck}}},
  booktitle = {Proceedings of the {{Fifth ACM SIGPLAN International Conference}} on {{Functional Programming}}, {{ICFP}}},
  author = {Claessen, Koen and Hughes, John},
  year = {2000},
  pages = {268--279},
  doi = {10.1145/1988042.1988046},
  urldate = {2020-11-08},
  abstract = {QuickCheck is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are discribed as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffuces to obtain good coverage of the definition under test.},
  file = {/home/vipa/Zotero/storage/94HQ4GCC/Claessen and Hughes - 2011 - QuickCheck a lightweight tool for random testing .pdf}
}

@inproceedings{clementSimpleApplicativeLanguage1986,
  title = {A Simple Applicative Language: Mini-{{ML}}},
  shorttitle = {A Simple Applicative Language},
  booktitle = {Proceedings of the 1986 {{ACM}} Conference on {{LISP}} and Functional Programming},
  author = {Cl{\'e}ment, Dominique and Despeyroux, Thierry and Kahn, Gilles and Despeyroux, Jo{\"e}lle},
  year = {1986},
  month = aug,
  series = {{{LFP}} '86},
  pages = {13--27},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/319838.319847},
  urldate = {2023-09-27},
  isbn = {978-0-89791-200-6},
  file = {/home/vipa/Zotero/storage/J5M4HPVB/Clément et al. - 1986 - A simple applicative language mini-ML.pdf}
}

@inproceedings{clingerMacrosThatWork1991,
  title = {Macros {{That Work}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Clinger, William and Rees, Jonathan},
  year = {1991},
  series = {{{POPL}} '91},
  pages = {155--162},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/99583.99607},
  urldate = {2018-10-18},
  isbn = {978-0-89791-419-2},
  file = {/home/vipa/Zotero/storage/TKPZME6Y/Clinger and Rees - 1991 - Macros That Work.pdf}
}

@inproceedings{coenEfficientAmbiguousParsing2004,
  title = {Efficient {{Ambiguous Parsing}} of {{Mathematical Formulae}}},
  booktitle = {Mathematical {{Knowledge Management}}},
  author = {Coen, Claudio Sacerdoti and Zacchiroli, Stefano},
  editor = {Asperti, Andrea and Bancerek, Grzegorz and Trybulec, Andrzej},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {347--362},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-27818-4_25},
  abstract = {Mathematical notation has the characteristic of being ambiguous: operators can be overloaded and information that can be deduced is often omitted. Mathematicians are used to this ambiguity and can easily disambiguate a formula making use of the context and of their ability to find the right interpretation.Software applications that have to deal with formulae usually avoid these issues by fixing an unambiguous input notation. This solution is annoying for mathematicians because of the resulting tricky syntaxes and becomes a show stopper to the simultaneous adoption of tools characterized by different input languages.In this paper we present an efficient algorithm suitable for ambiguous parsing of mathematical formulae. The only requirement of the algorithm is the existence of a ``validity'' predicate over abstract syntax trees of incomplete formulae with placeholders. This requirement can be easily fulfilled in the applicative area of interactive proof assistants, and in several other areas of Mathematical Knowledge Management.},
  isbn = {978-3-540-27818-4},
  langid = {english},
  keywords = {Abstract Data Type,Mathematical Formula,Proof Assistant,Syntax Tree,Type Theory},
  file = {/home/vipa/Zotero/storage/X3IQJ3DN/Coen and Zacchiroli - 2004 - Efficient Ambiguous Parsing of Mathematical Formul.pdf}
}

@article{cohenAutomatingRelationalOperations1993,
  title = {Automating Relational Operations on Data Structures},
  author = {Cohen, D. and Campbell, N.},
  year = {1993},
  month = may,
  journal = {IEEE Software},
  volume = {10},
  number = {3},
  pages = {53--60},
  issn = {1937-4194},
  doi = {10.1109/52.210604},
  urldate = {2024-01-07},
  abstract = {An approach to having compilers do most of the implementation detail work in programming that divides a program into two parts is described. The specification part describes what the program should do, but in a way that avoids commitment to implementation details. The annotation part provides implementation instructions that the compiler will carry out. Annotations affect execution efficiency, but not functional behavior. They are very high level and usually very short and hence encourage experimentation. To try out different implementation choices, the programmer simply changes the annotations and recompiles. The implementation details related to data representations are discussed. The testing of compilers that produce Lisp code for iteration, and for adding and deleting tuples of composite relations, is reviewed.{$<>$}},
  file = {/home/vipa/Zotero/storage/VB7EC745/Cohen and Campbell - 1993 - Automating relational operations on data structure.pdf;/home/vipa/Zotero/storage/L9KABNXP/210604.html}
}

@inproceedings{cohenRefinementsFree2013,
  title = {Refinements for {{Free}}!},
  booktitle = {Certified {{Programs}} and {{Proofs}}},
  author = {Cohen, Cyril and D{\'e}n{\`e}s, Maxime and M{\"o}rtberg, Anders},
  editor = {Gonthier, Georges and Norrish, Michael},
  year = {2013},
  pages = {147--162},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-03545-1_10},
  abstract = {Formal verification of algorithms often requires a choice between definitions that are easy to reason about and definitions that are computationally efficient. One way to reconcile both consists in adopting a high-level view when proving correctness and then refining stepwise down to an efficient low-level implementation. Some refinement steps are interesting, in the sense that they improve the algorithms involved, while others only express a switch from data representations geared towards proofs to more efficient ones geared towards computations. We relieve the user of these tedious refinements by introducing a framework where correctness is established in a proof-oriented context and automatically transported to computation-oriented data structures. Our design is general enough to encompass a variety of mathematical objects, such as rational numbers, polynomials and matrices over refinable structures. Moreover, the rich formalism of the Coq proof assistant enables us to develop this within Coq, without having to maintain an external tool.},
  isbn = {978-3-319-03545-1},
  langid = {english},
  keywords = {COQ,Data refinements,Efficient algorithms and data structures,Formal proofs,Parametricity},
  file = {/home/vipa/Zotero/storage/YD7SE2MY/Cohen et al. - 2013 - Refinements for Free!.pdf}
}

@article{colbyLookaheadLrParsing1997,
  title = {Lookahead Lr Parsing with Regular Right Part Grammars},
  author = {Colby, John E. and Bermudez, Manuel E.},
  year = {1997},
  month = jan,
  journal = {International Journal of Computer Mathematics},
  volume = {64},
  number = {1-2},
  pages = {1--15},
  publisher = {Taylor \& Francis},
  issn = {0020-7160},
  doi = {10.1080/00207169708804571},
  urldate = {2022-08-09},
  abstract = {This paper proposes a simple method for the application of a general model for lookahead parsing to regular right part grammars (RRPG's). Previous approaches have been limited to single LR grammar classes:LR(k) and LALR(k), for example. The present approach, however, makes several simple modifications to a general model of lookahead LR parsing, partitioning RRPG's into a infinite number of LR grammar classes.},
  keywords = {Context-free grammar,D.3.1,D.3.4,F.4.2,lookahead automata,LR(k) parsing,regular right-part grammar},
  file = {/home/vipa/Zotero/storage/CFMIKZ32/Colby and Bermudez - 1997 - Lookahead lr parsing with regular right part gramm.pdf}
}

@article{comerComputingDiscipline1989,
  title = {Computing as a Discipline},
  author = {Comer, D. E. and Gries, David and Mulder, Michael C. and Tucker, Allen and Turner, A. Joe and Young, Paul R. and Denning, Peter J.},
  year = {1989},
  month = jan,
  journal = {Commun. ACM},
  volume = {32},
  number = {1},
  pages = {9--23},
  issn = {0001-0782},
  doi = {10.1145/63238.63239},
  urldate = {2024-06-28},
  abstract = {The final report of the Task Force on the Core of Computer Science presents a new intellectual framework for the discipline of computing and a new basis for computing curricula. This report has been endorsed and approved for release by the ACM Education Board.},
  file = {/home/vipa/Zotero/storage/QYNFXVYP/Comer et al. - 1989 - Computing as a discipline.pdf}
}

@book{comonTreeAutomataTechniques2007,
  title = {Tree {{Automata Techniques}} and {{Applications}}},
  author = {Comon, H. and Dauchet, M. and Gilleron, R. and L{\"o}ding, C. and Jacquemard, F. and Lugiez, D. and Tison, S. and Tommasi, M.},
  year = {2007},
  howpublished = {Available on: http://www.grappa.univ-lille3.fr/tata},
  annotation = {release October, 12th 2007},
  file = {/home/vipa/Zotero/storage/C5C5FSYG/Comon et al. - 2007 - Tree Automata Techniques and Applications.pdf}
}

@book{cooperEngineeringCompiler2011,
  title = {Engineering a {{Compiler}}},
  author = {Cooper, Keith and Torczon, Linda},
  year = {2011},
  month = jan,
  edition = {2nd},
  publisher = {Elsevier},
  abstract = {This entirely revised second edition of Engineering a Compiler is full of technical updates and new material covering the latest developments in compiler technology. In this comprehensive text you will learn important techniques for constructing a modern compiler. Leading educators and researchers Keith Cooper and Linda Torczon combine basic principles with pragmatic insights from their experience building state-of-the-art compilers. They will help you fully understand important techniques such as compilation of imperative and object-oriented languages, construction of static single assignment forms, instruction scheduling, and graph-coloring register allocation.In-depth treatment of algorithms and techniques used in the front end of a modern compilerFocus on code optimization and code generation, the primary areas of recent research and developmentImprovements in presentation including conceptual overviews for each chapter, summaries and review questions for sections, and prominent placement of definitions for new termsExamples drawn from several different programming languages},
  googlebooks = {\_tgh4bgQ6PAC},
  isbn = {978-0-08-091661-3},
  langid = {english}
}

@article{cordyTXLSourceTransformation2006,
  title = {The {{TXL}} Source Transformation Language},
  author = {Cordy, James R.},
  year = {2006},
  month = aug,
  journal = {Science of Computer Programming},
  series = {Special {{Issue}} on {{The Fourth Workshop}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} '04)},
  volume = {61},
  number = {3},
  pages = {190--210},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2006.04.002},
  urldate = {2020-02-17},
  abstract = {TXL is a special-purpose programming language designed for creating, manipulating and rapidly prototyping language descriptions, tools and applications. TXL is designed to allow explicit programmer control over the interpretation, application, order and backtracking of both parsing and rewriting rules. Using first order functional programming at the higher level and term rewriting at the lower level, TXL provides for flexible programming of traversals, guards, scope of application and parameterized context. This flexibility has allowed TXL users to express and experiment with both new ideas in parsing, such as robust, island and agile parsing, and new paradigms in rewriting, such as XML mark-up, rewriting strategies and contextualized rules, without any change to TXL itself. This paper outlines the history, evolution and concepts of TXL with emphasis on its distinctive style and philosophy, and gives examples of its use in expressing and applying recent new paradigms in language processing.},
  langid = {english},
  keywords = {Functional programming,Grammars,Source transformation,Term rewriting},
  file = {/home/vipa/Zotero/storage/UCIHVL54/Cordy - 2006 - The TXL source transformation language.pdf;/home/vipa/Zotero/storage/ZZRCCFBX/S0167642306000669.html}
}

@inproceedings{costaCollectionSwitchFrameworkEfficient2018,
  title = {{{CollectionSwitch}}: A Framework for Efficient and Dynamic Collection Selection},
  shorttitle = {{{CollectionSwitch}}},
  booktitle = {Proceedings of the 2018 {{International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Costa, Diego and Andrzejak, Artur},
  year = {2018},
  month = feb,
  series = {{{CGO}} 2018},
  pages = {16--26},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3168825},
  urldate = {2024-01-06},
  abstract = {Selecting collection data structures for a given application is a crucial aspect of the software development. Inefficient usage of collections has been credited as a major cause of performance bloat in applications written in Java, C++ and C\#. Furthermore, a single implementation might not be optimal throughout the entire program execution. This demands an adaptive solution that adjusts at runtime the collection implementations to varying workloads. We present CollectionSwitch, an application-level framework for efficient collection adaptation. It selects at runtime collection implementations in order to optimize the execution and memory performance of an application. Unlike previous works, we use workload data on the level of collection allocation sites to guide the optimization process. Our framework identifies allocation sites which instantiate suboptimal collection variants, and selects optimized variants for future instantiations. As a further contribution we propose adaptive collection implementations which switch their underlying data structures according to the size of the collection. We implement this framework in Java, and demonstrate the improvements in terms of time and memory behavior across a range of benchmarks. To our knowledge, it is the first approach which is capable of runtime performance optimization of Java collections with very low overhead.},
  isbn = {978-1-4503-5617-6},
  keywords = {adaptive algorithms,data structure,optimization,performance},
  file = {/home/vipa/Zotero/storage/E69S66TR/Costa and Andrzejak - 2018 - CollectionSwitch a framework for efficient and dy.pdf}
}

@inproceedings{costaEmpiricalStudyUsage2017,
  title = {Empirical {{Study}} of {{Usage}} and {{Performance}} of {{Java Collections}}},
  booktitle = {Proceedings of the 8th {{ACM}}/{{SPEC}} on {{International Conference}} on {{Performance Engineering}}},
  author = {Costa, Diego and Andrzejak, Artur and Seboek, Janos and Lo, David},
  year = {2017},
  month = apr,
  series = {{{ICPE}} '17},
  pages = {389--400},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3030207.3030221},
  urldate = {2024-01-07},
  abstract = {Collection data structures have a major impact on the performance of applications, especially in languages such as Java, C\#, or C++. This requires a developer to select an appropriate collection from a large set of possibilities, including different abstractions (e.g. list, map, set, queue), and multiple implementations. In Java, the default implementation of collections is provided by the standard Java Collection Framework (JCF). However, there exist a large variety of less known third-party collection libraries which can provide substantial performance benefits with minimal code changes. In this paper, we first study the popularity and usage patterns of collection implementations by mining a code corpus comprised of 10,986 Java projects. We use the results to evaluate and compare the performance of the six most popular alternative collection libraries in a large variety of scenarios. We found that for almost every scenario and JCF collection type there is an alternative implementation that greatly decreases memory consumption while offering comparable or even better execution time. Memory savings range from 60\% to 88\% thanks to reduced overhead and some operations execute 1.5x to 50x faster. We present our results as a comprehensive guideline to help developers in identifying the scenarios in which an alternative implementation can provide a substantial performance improvement. Finally, we discuss how some coding patterns result in substantial performance differences of collections.},
  isbn = {978-1-4503-4404-3},
  keywords = {collections,empirical study,execution time,java,memory,performance},
  file = {/home/vipa/Zotero/storage/NZAY282Y/Costa et al. - 2017 - Empirical Study of Usage and Performance of Java C.pdf}
}

@inproceedings{coudercClassificationbasedStaticCollection2023,
  title = {Classification-Based {{Static Collection Selection}} for {{Java}}: {{Effectiveness}} and {{Adaptability}}},
  shorttitle = {Classification-Based {{Static Collection Selection}} for {{Java}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  author = {Couderc, Noric and Reichenbach, Christoph and S{\"o}derberg, Emma},
  year = {2023},
  month = jun,
  series = {{{EASE}} '23},
  pages = {111--120},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3593434.3593469},
  urldate = {2024-01-07},
  abstract = {Carefully selecting the right collection datastructure can significantly improve the performance of a Java program. Unfortunately, the performance impact of a certain collection selection can be hard to estimate. To assist developers, there exist tools that recommend collections to use based on static and/or dynamic information about a program. The majority of existing collection selection tools for Java pick their selections dynamically, which means that they must trade off sophistication in their selection algorithm against its run time overhead. For static collection selection, the Brainy tool has demonstrated that complex, machine-dependent models can produce substantial performance improvements, albeit only for C++ so far. In this paper, we port Brainy from C++ to Java, and evaluate its effectiveness for 5 benchmarks from the DaCapo benchmark suite. We compare it against the original program, but also to a ground truth, which we estimate using a variant of a brute-force approach to collection selection. Our results show that in four benchmarks out of five, our ground truth and the original program are similar. In one case, the ground truth shows an optimization yielding 20\% speedup was available, but our port did not find this substantial optimization. We find that the port is more efficient but less effective than the ground truth, can easily adapt to new hardware architectures, and incorporate new datastructures with at most a few hours of human effort. We detail challenges that we encountered porting the Brainy approach to Java, and list a number of insights and directions for future research.},
  isbn = {9798400700446},
  file = {/home/vipa/Zotero/storage/WLU6BKH7/Couderc et al. - 2023 - Classification-based Static Collection Selection f.pdf}
}

@article{culikLRregularGrammarsExtension1973,
  title = {{{LR-regular}} Grammars---an Extension of {{LR}}({\emph{k}}) Grammars},
  author = {{\v C}ulik, Karel and Cohen, Rina},
  year = {1973},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {7},
  number = {1},
  pages = {66--96},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(73)80050-9},
  urldate = {2024-06-24},
  abstract = {LR-regular grammars are defined similarly to Knuth's LR(k) grammars, with the following exception: arbitrarily long look-ahead is allowed before making a parsing decision during the bottom-up syntactical analysis; however, this look-ahead is restricted in that the essential ``look-ahead information'' can be represented by a finite number of regular sets, thus can be computed by a finite state machine. LR-regular grammars can be parsed deterministically in linear time by a rather simple two-scan algorithm. Efficient parsers are constructed for given LR-regular grammars. The family of LR-regular languages is studied; it properly includes the family of deterministic CF languages and has similar properties. Necessary and sufficient conditions for a grammar to be LR-regular are derived and then utilized for developing parser generation techniques for arbitrary grammars.},
  file = {/home/vipa/Zotero/storage/JVF3CCHG/Čulik and Cohen - 1973 - LR-regular grammars—an extension of LR(k) g.pdf;/home/vipa/Zotero/storage/IS5QQ764/S0022000073800509.html}
}

@article{cunhaStronglyTypedRewriting2007,
  title = {Strongly {{Typed Rewriting For Coupled Software Transformation}}},
  author = {Cunha, Alcino and Visser, Joost},
  year = {2007},
  month = apr,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the 7th {{International Workshop}} on {{Rule Based Programming}} ({{RULE}} 2006)},
  volume = {174},
  number = {1},
  pages = {17--34},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2006.10.019},
  urldate = {2020-02-10},
  abstract = {Coupled transformations occur in software evolution when multiple artifacts must be modified in such a way that they remain consistent with each other. An important example involves the coupled transformation of a data type, its instances, and the programs that consume or produce it. Previously, we have provided a formal treatment of transformation of the first two: data types and instances. The treatment involved the construction of type-safe, type-changing strategic rewrite systems. In this paper, we extend our treatment to the transformation of corresponding data processing programs. The key insight underlying the extension is that both data migration functions and data processors can be represented type-safely by a generalized abstract data type (GADT). These representations are then subjected to program calculation rules, harnessed in type-safe, type-preserving strategic rewrite systems. For ease of calculation, we use point-free representations and corresponding calculation rules. Thus, coupled transformations are carried out in two steps. First, a type-changing rewrite system is applied to a source type to obtain a target type together with (representations of) migration functions between source and target. Then, a type-preserving rewrite system is applied to the composition of a migration function and a data processor on the source (or target) type to obtain a data processor on the target (or source) type. All rewrites are type-safe.},
  langid = {english},
  keywords = {data refinement,generalized abstract datatypes,Program transformation,strategic programming,term rewriting},
  file = {/home/vipa/Zotero/storage/TPI4SXEB/Cunha and Visser - 2007 - Strongly Typed Rewriting For Coupled Software Tran.pdf;/home/vipa/Zotero/storage/5DH6FWDA/S157106610700151X.html}
}

@misc{DafnyDocumentation,
  title = {Dafny {{Documentation}}},
  author = {{The dafny-lang community}},
  year = {2022},
  journal = {Dafny Documentation},
  urldate = {2022-10-25},
  abstract = {Dafny is a verification-aware programming language},
  howpublished = {https://dafny-lang.github.io/dafny/DafnyRef/DafnyRef.html},
  langid = {english},
  file = {/home/vipa/Zotero/storage/D7WR4YYA/DafnyRef.html}
}

@inproceedings{damasPrincipalTypeschemesFunctional1982,
  title = {Principal Type-Schemes for Functional Programs},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Damas, Luis and Milner, Robin},
  year = {1982},
  month = jan,
  series = {{{POPL}} '82},
  pages = {207--212},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/582153.582176},
  urldate = {2023-09-21},
  isbn = {978-0-89791-065-1},
  file = {/home/vipa/Zotero/storage/78MK2EPU/Damas and Milner - 1982 - Principal type-schemes for functional programs.pdf}
}

@inproceedings{danielssonParsingMixfixOperators2011,
  title = {Parsing {{Mixfix Operators}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Danielsson, Nils Anders and Norell, Ulf},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {80--99},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-24452-0_5},
  abstract = {A simple grammar scheme for expressions containing mixfix operators is presented. The scheme is parameterised by a precedence relation which is only restricted to be a directed acyclic graph; this makes it possible to build up precedence relations in a modular way. Efficient and simple implementations of parsers for languages with user-defined mixfix operators, based on the grammar scheme, are also discussed. In the future we plan to replace the support for mixfix operators in the language Agda with a grammar scheme and an implementation based on this work.},
  isbn = {978-3-642-24452-0},
  langid = {english},
  file = {/home/vipa/Zotero/storage/BK5SP8KH/Danielsson and Norell - 2011 - Parsing Mixfix Operators.pdf}
}

@inproceedings{danvyDefunctionalizationWork2001,
  title = {Defunctionalization at {{Work}}},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Conference}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  author = {Danvy, Olivier and Nielsen, Lasse R.},
  year = {2001},
  series = {{{PPDP}} '01},
  pages = {162--174},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/773184.773202},
  urldate = {2019-11-21},
  abstract = {Reynolds's defunctionalization technique is a whole-program transformation from higher-order to first-order functional programs. We study practical applications of this transformation and uncover new connections between seemingly unrelated higher-order and first-order specifications and between their correctness proofs. Defunctionalization therefore appearsboth as a springboard for rev ealing new connections and as a bridge for transferring existing results between the first-order world and the higher-order world.},
  isbn = {978-1-58113-388-2},
  keywords = {church encoding,closure conversion,continuation-passing style (CPS),continuations,CPS transformation,defunctionalization,direct-style transformation,first-order programs,higher-order programs,lambda-lifting,ML,regular expressions,Scheme,supercombinator conversion,syntactic theories},
  file = {/home/vipa/Zotero/storage/INVQCBF8/Danvy and Nielsen - 2001 - Defunctionalization at Work.pdf}
}

@article{danvyRefunctionalizationWork2009,
  title = {Refunctionalization at Work},
  author = {Danvy, Olivier and Millikin, Kevin},
  year = {2009},
  month = jun,
  journal = {Science of Computer Programming},
  series = {Special {{Issue}} on {{Mathematics}} of {{Program Construction}} ({{MPC}} 2006)},
  volume = {74},
  number = {8},
  pages = {534--549},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.10.007},
  urldate = {2019-11-21},
  abstract = {We present the left inverse of Reynolds' defunctionalization and we show its relevance to programming and to programming languages. We propose two methods to transform a program that is almost in defunctionalized form into one that is actually in defunctionalized form, and we illustrate them with a recognizer for Dyck words and with Dijkstra's shunting-yard algorithm.},
  langid = {english},
  keywords = {Abstract machines,Continuation-passing style (CPS),Continuations,Defunctionalization,Refunctionalization,Shunting-yard algorithm},
  file = {/home/vipa/Zotero/storage/B6L936Q4/Danvy and Millikin - 2009 - Refunctionalization at work.pdf;/home/vipa/Zotero/storage/FXNEPQEN/S0167642309000227.html}
}

@inproceedings{darulovaSoundMixedPrecisionOptimization2018,
  title = {Sound {{Mixed-Precision Optimization}} with {{Rewriting}}},
  booktitle = {2018 {{ACM}}/{{IEEE}} 9th {{International Conference}} on {{Cyber-Physical Systems}} ({{ICCPS}})},
  author = {Darulova, E. and Horn, E. and Sharma, S.},
  year = {2018},
  month = apr,
  pages = {208--219},
  doi = {10.1109/ICCPS.2018.00028},
  abstract = {Finite-precision arithmetic, widely used in embedded systems for numerical calculations, faces an inherent tradeoff between accuracy and efficiency. The points in this tradeoff space are determined, among other factors, by different data types but also evaluation orders. To put it simply, the shorter a precision's bit-length, the larger the roundoff error will be, but the faster the program will run. Similarly, the fewer arithmetic operations the program performs, the faster it will run; however, the effect on the roundoff error is less clear-cut. Manually optimizing the efficiency of finite-precision programs while ensuring that results remain accurate enough is challenging. The unintuitive and discrete nature of finite-precision makes estimation of roundoff errors difficult; furthermore the space of possible data types and evaluation orders is prohibitively large. We present the first fully automated and sound technique and tool for optimizing the performance of floating-point and fixed-point arithmetic kernels. Our technique combines rewriting and mixed-precision tuning. Rewriting searches through different evaluation orders to find one which minimizes the roundoff error at no additional runtime cost. Mixed-precision tuning assigns different finite precisions to different variables and operations and thus provides finer-grained control than uniform precision. We show that when these two techniques are designed and applied together, they can provide higher performance improvements than each alone.},
  keywords = {Cost function,embedded systems,finite-precision arithmetic,finite-precision programs,fixed point arithmetic,fixed-point arithmetic kernels,floating point,floating point arithmetic,floating-point,fully automated technique,Hardware,Kernel,mixed precision,mixed-precision tuning,numerical calculations,optimisation,optimization,rewriting,roundoff error,roundoff errors,Roundoff errors,sound mixed-precision optimization,sound technique,static analysis,Tools,Tuning},
  file = {/home/vipa/Zotero/storage/AGZWEDG6/Darulova et al. - 2018 - Sound Mixed-Precision Optimization with Rewriting.pdf;/home/vipa/Zotero/storage/P96HDUIY/8443735.html}
}

@article{delawareFiatDeductiveSynthesis2015,
  title = {Fiat: {{Deductive Synthesis}} of {{Abstract Data Types}} in a {{Proof Assistant}}},
  shorttitle = {Fiat},
  author = {Delaware, Benjamin and {Pit-Claudel}, Cl{\'e}ment and Gross, Jason and Chlipala, Adam},
  year = {2015},
  month = jan,
  journal = {ACM SIGPLAN Notices},
  volume = {50},
  number = {1},
  pages = {689--700},
  issn = {0362-1340},
  doi = {10.1145/2775051.2677006},
  urldate = {2024-06-04},
  abstract = {We present Fiat, a library for the Coq proof assistant supporting refinement of declarative specifications into efficient functional programs with a high degree of automation. Each refinement process leaves a proof trail, checkable by the normal Coq kernel, justifying its soundness. We focus on the synthesis of abstract data types that package methods with private data. We demonstrate the utility of our framework by applying it to the synthesis of query structures -- abstract data types with SQL-like query and insert operations. Fiat includes a library for writing specifications of query structures in SQL-inspired notation, expressing operations over relations (tables) in terms of mathematical sets. This library includes a suite of tactics for automating the refinement of specifications into efficient, correct-by-construction OCaml code. Using these tactics, a programmer can generate such an implementation completely automatically by only specifying the equivalent of SQL indexes, data structures capturing useful views of the abstract data. Throughout we speculate on the new programming modularity possibilities enabled by an automated refinement system with proved-correct rules.},
  keywords = {deductive synthesis,mechanized derivation of abstract data types},
  file = {/home/vipa/Zotero/storage/XZG3GSLF/Delaware et al. - 2015 - Fiat Deductive Synthesis of Abstract Data Types i.pdf}
}

@inproceedings{demouraZ3EfficientSMT2008,
  title = {Z3: {{An Efficient SMT Solver}}},
  shorttitle = {Z3},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {{de Moura}, Leonardo and Bj{\o}rner, Nikolaj},
  editor = {Ramakrishnan, C. R. and Rehof, Jakob},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {337--340},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-78800-3_24},
  abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
  isbn = {978-3-540-78800-3},
  langid = {english},
  keywords = {Bound Model Check,Linear Arithmetic,Predicate Abstraction,Symbolic Execution,Theory Solver},
  file = {/home/vipa/Zotero/storage/QIY976FU/de Moura and Bjørner - 2008 - Z3 An Efficient SMT Solver.pdf}
}

@inproceedings{desouzaamorimMultipurposeSyntaxDefinition2020,
  title = {Multi-Purpose {{Syntax Definition}} with {{SDF3}}},
  booktitle = {Software {{Engineering}} and {{Formal Methods}}},
  author = {{de Souza Amorim}, Lu{\'i}s Eduardo and Visser, Eelco},
  editor = {{de Boer}, Frank and Cerone, Antonio},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--23},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58768-0_1},
  abstract = {SDF3 is a syntax definition formalism that extends plain context-free grammars with features such as constructor declarations, declarative disambiguation rules, character-level grammars, permissive syntax, layout constraints, formatting templates, placeholder syntax, and modular composition. These features support the multi-purpose interpretation of syntax definitions, including derivation of type schemas for abstract syntax tree representations, scannerless generalized parsing of the full class of context-free grammars, error recovery, layout-sensitive parsing, parenthesization and formatting, and syntactic completion. This paper gives a high level overview of SDF3 by means of examples and provides a guide to the literature for further details.},
  isbn = {978-3-030-58768-0},
  langid = {english},
  keywords = {Parsing,Programming language,Syntax definition},
  file = {/home/vipa/Zotero/storage/D36GHK7M/de Souza Amorim and Visser - 2020 - Multi-purpose Syntax Definition with SDF3.pdf}
}

@article{desouzaamorimZerooverheadDisambiguationDeep2018,
  ids = {desouzaamorimZeroOverheadDisambiguationDeep2018},
  title = {Towards Zero-Overhead Disambiguation of Deep Priority Conflicts},
  author = {{de Souza Amorim}, Lu{\'i}s Eduardo and Steindorfer, Michael J. and Visser, Eelco},
  year = {2018},
  journal = {Programming Journal},
  volume = {2},
  number = {3},
  pages = {13},
  doi = {10.22152/programming-journal.org/2018/2/13},
  abstract = {Context Context-free grammars are widely used for language prototyping and implementation. They allow formalizing the syntax of domain-specific or general-purpose programming languages concisely and declaratively. However, the natural and concise way of writing a context-free grammar is often ambiguous. Therefore, grammar formalisms support extensions in the form of declarative disambiguation rules to specify operator precedence and associativity, solving ambiguities that are caused by the subset of the grammar that corresponds to expressions. Inquiry Implementing support for declarative disambiguation within a parser typically comes with one or more of the following limitations in practice: a lack of parsing performance, or a lack of modularity (i.e., disallowing the composition of grammar fragments of potentially different languages). The latter subject is generally addressed by scannerless generalized parsers. We aim to equip scannerless generalized parsers with novel disambiguation methods that are inherently performant, without compromising the concerns of modularity and language composition. Approach In this paper, we present a novel low-overhead implementation technique for disambiguating deep associativity and priority conflicts in scannerless generalized parsers with lightweight data-dependency. Knowledge Ambiguities with respect to operator precedence and associativity arise from combining the various operators of a language. While shallow conflicts can be resolved efficiently by one-level tree patterns, deep conflicts require more elaborate techniques, because they can occur arbitrarily nested in a tree. Current state-of-the-art approaches to solving deep priority conflicts come with a severe performance overhead. Grounding We evaluated our new approach against state-of-the-art declarative disambiguation mechanisms. By parsing a corpus of popular open-source repositories written in Java and OCaml, we found that our approach yields speedups of up to 1.73x over a grammar rewriting technique when parsing programs with deep priority conflicts---with a modest overhead of 1--2 \% when parsing programs without deep conflicts. Importance A recent empirical study shows that deep priority conflicts are indeed wide-spread in real-world programs. The study shows that in a corpus of popular OCaml projects on Github, up to 17 \% of the source files contain deep priority conflicts. However, there is no solution in the literature that addresses efficient disambiguation of deep priority conflicts, with support for modular and composable syntax definitions.},
  citedby = {0},
  cites = {0},
  researchr = {https://researchr.org/publication/AmorimSV18},
  file = {/home/vipa/Zotero/storage/G6MSQ4SE/de Souza Amorim et al. - 2018 - Towards zero-overhead disambiguation of deep prior.pdf}
}

@inproceedings{dewaelJustintimeDataStructures2015,
  title = {Just-in-Time Data Structures},
  booktitle = {2015 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}} ({{Onward}}!)},
  author = {De Wael, Mattias and Marr, Stefan and De Koster, Joeri and Sartor, Jennifer B. and De Meuter, Wolfgang},
  year = {2015},
  month = oct,
  series = {Onward! 2015},
  pages = {61--75},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2814228.2814231},
  urldate = {2024-01-06},
  abstract = {Today, software engineering practices focus on finding the single ``right'' data representation for a program. The ``right'' data representation, however, might not exist: changing the representation of an object during program execution can be better in terms of performance. To this end we introduce Just-in-Time Data Structures, which enable representation changes at runtime, based on declarative input from a performance expert programmer. Just-in-Time Data Structures are an attempt to shift the focus from finding the ``right'' data structure to finding the ``right'' sequence of data representations. We present JitDS, a programming language to develop such Just-in-Time Data Structures. Further, we show two example programs that benefit from changing the representation at runtime.},
  isbn = {978-1-4503-3688-8},
  keywords = {algorithms,data structures,dynamic reclassification,performance},
  file = {/home/vipa/Zotero/storage/W662F8C5/De Wael et al. - 2015 - Just-in-time data structures.pdf}
}

@inproceedings{dewaelJustintimeDataStructures2015a,
  title = {Just-in-Time Data Structures: Towards Declarative Swap Rules},
  shorttitle = {Just-in-Time Data Structures},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Dynamic Analysis}}},
  author = {De Wael, Mattias},
  year = {2015},
  month = oct,
  series = {{{WODA}} 2015},
  pages = {33--34},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2823363.2823371},
  urldate = {2024-01-06},
  abstract = {Just-in-Time Data Structures are an attempt to vulgarise the idea that changing the representation (i.e., implementation) of a data structure at runtime can improve the performance of a program compared to its counter part that relies on a single representation. In previous work, we developed a language to develop such Just-in-Time Data Structures. To express ``when'' to change between representations, a dedicated language construct was introduced: the swap rule. A swap rule analyses the state and usage of a just-in-time data structure and reacts as defined by a developer. Opposed to what the name suggest, swap rules are currently implemented as imperative statements woven into the codebase. Their intend, however, is declarative and therefore we think that swap rules should become real declarative rules. This extended abstract presents Just-in-Time Data Structures as a case for applying state-of-the-art in low overhead dynamic analysis. Changing from an imperative to a declarative implementation of swap rules will allow for more efficient execution of our programs by reducing the overhead of continuous analysis.},
  isbn = {978-1-4503-3909-4},
  keywords = {algorithms,Data structures,dynamic reclassification,performance},
  file = {/home/vipa/Zotero/storage/S6VZ8ZC3/De Wael - 2015 - Just-in-time data structures towards declarative .pdf}
}

@inproceedings{doczkalConstructiveTheoryRegular2013,
  title = {A {{Constructive Theory}} of {{Regular Languages}} in {{Coq}}},
  booktitle = {Certified {{Programs}} and {{Proofs}}},
  author = {Doczkal, Christian and Kaiser, Jan-Oliver and Smolka, Gert},
  editor = {Gonthier, Georges and Norrish, Michael},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {82--97},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {We present a formal constructive theory of regular languages consisting of about 1400 lines of Coq/Ssreflect. As representations we consider regular expressions, deterministic and nondeterministic automata, and Myhill and Nerode partitions. We construct computable functions translating between these representations and show that equivalence of representations is decidable. We also establish the usual closure properties, give a minimization algorithm for DFAs, and prove that minimal DFAs are unique up to state renaming. Our development profits much from Ssreflect's support for finite types and graphs.},
  isbn = {978-3-319-03545-1},
  langid = {english},
  keywords = {Coq,finite automata,Myhill-Nerode,regular expressions,regular languages,Ssreflect},
  file = {/home/vipa/Zotero/storage/W7G5H6J5/Doczkal et al. - 2013 - A Constructive Theory of Regular Languages in Coq.pdf}
}

@book{dolanAlgebraicSubtypingDistinguished2017,
  title = {Algebraic Subtyping: {{Distinguished Dissertation}} 2017},
  shorttitle = {Algebraic Subtyping},
  author = {Dolan, Stephen},
  year = {2017},
  publisher = {BCS},
  address = {Swindon, GBR},
  abstract = {Type inference gives programmers the benefit of static, compile-time type checking without the cost of manually specifying types, and has long been a standard feature of functional programming languages. However, it has proven difficult to integrate type inference with subtyping, since the unification engine at the core of classical type inference accepts only equations, not subtyping constraints. This thesis presents a type system combining ML-style parametric polymorphism and subtyping, with type inference, principal types, and decidable type subsumption. Type inference is based on biunification, an analogue of unification that works with subtyping constraints. Making this possible are several contributions, beginning with the notion of an extensible type system, in which an open world of types is assumed, so that no typeable program becomes untypeable by the addition of new types to the language. While previous formulations of subtyping fail to be extensible, this thesis shows that adopting a more algebraic approach can remedy this. Using such an approach, this thesis develops the theory of biunification, shows how it is used to infer types, and shows how it can be efficiently implemented, exploiting deep connections between the algebra of regular languages and polymorphic subtyping.},
  isbn = {978-1-78017-415-0},
  file = {/home/vipa/Zotero/storage/YE63WIER/Dolan - 2017 - Algebraic subtyping Distinguished Dissertation 20.pdf}
}

@article{dolstraBuildingInterpretersRewriting2002,
  title = {Building {{Interpreters}} with {{Rewriting Strategies}}},
  author = {Dolstra, Eelco and Visser, Eelco},
  year = {2002},
  month = jul,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{LDTA}} 2002, {{Second Workshop}} on {{Language Descriptions}}, {{Tools}} and {{Applications}} ({{Satellite Event}} of {{ETAPS}} 2002)},
  volume = {65},
  number = {3},
  pages = {57--76},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)80427-4},
  urldate = {2020-11-27},
  abstract = {Programming language semantics based on pure rewrite rules suffers from the gap between the rewriting strategy implemented in rewriting engines and the intended evaluation strategy. This paper shows how programmable rewriting strategies can be used to implement interpreters for programming languages based on rewrite rules. The advantage of this approach is that reduction rules are first class entities that can be reused in different strategies, even in other kinds of program transformations such as optimizers. The approach is illustrated with several interpreters for the lambda calculus based on implicit and explicit (parallel) substitution, different strategies including normalization, eager evaluation, lazy evaluation, and lazy evaluation with updates. An extension with pattern matching and choice shows that such interpreters can easily be extended.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/FG7X6S82/Dolstra and Visser - 2002 - Building Interpreters with Rewriting Strategies.pdf;/home/vipa/Zotero/storage/TVZRUF5N/S1571066104804274.html}
}

@article{dunfieldCompleteEasyBidirectional2013,
  title = {Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism},
  author = {Dunfield, Jana and Krishnaswami, Neelakantan R.},
  year = {2013},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {48},
  number = {9},
  pages = {429--442},
  issn = {0362-1340},
  doi = {10.1145/2544174.2500582},
  urldate = {2022-05-03},
  abstract = {Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its scalability (unlike Damas-Milner type inference, bidirectional typing remains decidable even for very expressive type systems), its error reporting, and its relative ease of implementation. Following design principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to polymorphism, however, are less obvious. We give a declarative, bidirectional account of higher-rank polymorphism, grounded in proof theory; this calculus enjoys many properties such as eta-reduction and predictability of annotations. We give an algorithm for implementing the declarative system; our algorithm is remarkably simple and well-behaved, despite being both sound and complete.},
  keywords = {bidirectional typechecking,higher-rank polymorphism},
  file = {/home/vipa/Zotero/storage/862QCEVV/Dunfield and Krishnaswami - 2013 - Complete and easy bidirectional typechecking for h.pdf}
}

@article{dunfieldSoundCompleteBidirectional2020,
  title = {Sound and {{Complete Bidirectional Typechecking}} for {{Higher-Rank Polymorphism}} with {{Existentials}} and {{Indexed Types}}},
  author = {Dunfield, Jana and Krishnaswami, Neelakantan R.},
  year = {2020},
  month = sep,
  journal = {arXiv:1601.05106 [cs]},
  eprint = {1601.05106},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its applicability to a variety of type systems, its error reporting, and its ease of implementation. Following principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to indexed types (generalized algebraic datatypes) are less clear. Building on proof-theoretic treatments of equality, we give a declarative specification of typing based on focalization. This approach permits declarative rules for coverage of pattern matching, as well as support for first-class existential types using a focalized subtyping judgment. We use refinement types to avoid explicitly passing equality proofs in our term syntax, making our calculus similar to languages such as Haskell and OCaml. We also extend the declarative specification with an explicit rules for deducing when a type is principal, permitting us to give a complete declarative specification for a rich type system with significant type inference. We also give a set of algorithmic typing rules, and prove that it is sound and complete with respect to the declarative system. The proof requires a number of technical innovations, including proving soundness and completeness in a mutually recursive fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  file = {/home/vipa/Zotero/storage/E27PQQUC/Dunfield and Krishnaswami - 2020 - Sound and Complete Bidirectional Typechecking for .pdf;/home/vipa/Zotero/storage/62MQFTGQ/1601.html}
}

@article{dybvigSyntacticAbstractionScheme1993,
  title = {Syntactic Abstraction in Scheme},
  author = {Dybvig, R. Kent and Hieb, Robert and Bruggeman, Carl},
  year = {1993},
  month = dec,
  journal = {LISP and Symbolic Computation},
  volume = {5},
  number = {4},
  pages = {295--326},
  issn = {1573-0557},
  doi = {10.1007/BF01806308},
  urldate = {2018-10-18},
  abstract = {Naive program transformations can have surprising effects due to the interaction between introduced identifier references and previously existing identifier bindings, or between introduced bindings and previously existing references. These interactions can result in inadvertent binding, or capturing, of identifiers. A further complication is that transformed programs may have little resemblance to original programs, making correlation of source and object code difficult. This article describes an efficient macro system that prevents inadvertent capturing while maintaining the correlation between source and object code. The macro system allows the programmer to define program transformations using an unrestricted, general-purpose language. Previous approaches to the capturing problem have been inadequate, overly restrictive, or inefficient, and the problem of source-object correlation has been largely unaddressed. The macro system is based on a new algorithm for implementing syntactic transformations and a new representation for syntactic expressions.},
  langid = {english},
  keywords = {Hygienic Macros,Macros,Program Transformation,Syntactic Abstraction},
  file = {/home/vipa/Zotero/storage/CRNNP7AX/Dybvig et al. - 1993 - Syntactic abstraction in scheme.pdf}
}

@article{dyvbigMonadicFrameworkDelimited2007,
  title = {A Monadic Framework for Delimited Continuations},
  author = {Dyvbig, R. Kent and Jones, Simon Peyton and Sabry, Amr},
  year = {2007},
  month = nov,
  journal = {Journal of Functional Programming},
  volume = {17},
  number = {6},
  pages = {687--730},
  publisher = {Cambridge University Press},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796807006259},
  urldate = {2020-04-19},
  abstract = {Delimited continuations are more expressive than traditional abortive continuations and they apparently require a framework beyond traditional continuation-passing style (CPS). We show that this is not the case: standard CPS is sufficient to explain the common control operators for delimited continuations. We demonstrate this fact and present an implementation as a Scheme library. We then investigate a typed account of delimited continuations that makes explicit where control effects can occur. This results in a monadic framework for typed and encapsulated delimited continuations, which we design and implement as a Haskell library.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/3X8MDQ9U/Dyvbig et al. - 2007 - A monadic framework for delimited continuations.pdf;/home/vipa/Zotero/storage/3WX3NCPX/D99D1394370DFA8EA8428D552B5D8E7E.html}
}

@article{earleyAmbiguityPrecedenceSyntax1975,
  title = {Ambiguity and Precedence in Syntax Description},
  author = {Earley, Jay},
  year = {1975},
  month = jun,
  journal = {Acta Informatica},
  volume = {4},
  number = {2},
  pages = {183--192},
  issn = {1432-0525},
  doi = {10.1007/BF00288747},
  urldate = {2018-10-18},
  abstract = {SummaryThis paper describes a method of syntax description for programming languages which allows one to factor out that part of the description which deals with the relative precedences of syntactic units. This has been found to produce simpler and more flexible syntax descriptions. It is done by allowing the normal part of the description, which is done in BNF, to be ambiguous; these ambiguities are then resolved by a separate part of the description which gives precedence relations between the conflicting productions from the grammar. The method can be used with any left-to-right parser which is capable of detecting ambiguities and recognizing which productions they come from; We have studied its use with an LR(1) parser, and it requires a small and localized addition to the parser to enable it to deal with the precedence relations.},
  langid = {english},
  keywords = {Communication Network,Data Structure,Information System,Information Theory,Operating System},
  file = {/home/vipa/Zotero/storage/CNU8S7EC/Earley - 1975 - Ambiguity and precedence in syntax description.pdf}
}

@article{earleyEfficientContextfreeParsing1970,
  title = {An {{Efficient Context-free Parsing Algorithm}}},
  author = {Earley, Jay},
  year = {1970},
  month = feb,
  journal = {Communications of the ACM},
  volume = {13},
  number = {2},
  pages = {94--102},
  issn = {0001-0782},
  doi = {10.1145/362007.362035},
  urldate = {2018-10-18},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  file = {/home/vipa/Zotero/storage/JF9URNT6/Earley - 1970 - An Efficient Context-free Parsing Algorithm.pdf}
}

@article{ekmanJastAddSystemModular2007,
  title = {The {{JastAdd}} System --- Modular Extensible Compiler Construction},
  author = {Ekman, Torbj{\"o}rn and Hedin, G{\"o}rel},
  year = {2007},
  month = dec,
  journal = {Science of Computer Programming},
  series = {Special Issue on {{Experimental Software}} and {{Toolkits}}},
  volume = {69},
  number = {1},
  pages = {14--26},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.02.003},
  urldate = {2018-10-18},
  abstract = {The JastAdd system enables modular specifications of extensible compiler tools and languages. Java has been extended with the Rewritable Circular Reference Attributed Grammars formalism that supports modularization and extensibility through several synergistic mechanisms. Object-orientation and static aspect-oriented programming are combined with declarative attributes and context-dependent rewrites to allow highly modular specifications. The techniques have been verified by implementing a full Java 1.4 compiler with modular extensions for non-null types and Java 5 features.},
  keywords = {Compiler construction,Extensible languages,Modular implementation},
  file = {/home/vipa/Zotero/storage/IIL2SA46/Ekman and Hedin - 2007 - The JastAdd system — modular extensible compiler c.pdf;/home/vipa/Zotero/storage/TARP2EM9/S0167642307001591.html}
}

@inproceedings{emrichFreezeMLCompleteEasy2020,
  title = {{{FreezeML}}: Complete and Easy Type Inference for First-Class Polymorphism},
  shorttitle = {{{FreezeML}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Emrich, Frank and Lindley, Sam and Stolarek, Jan and Cheney, James and Coates, Jonathan},
  year = {2020},
  month = jun,
  series = {{{PLDI}} 2020},
  pages = {423--437},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3385412.3386003},
  urldate = {2021-08-16},
  abstract = {ML is remarkable in providing statically typed polymorphism without the programmer ever having to write any type annotations. The cost of this parsimony is that the programmer is limited to a form of polymorphism in which quantifiers can occur only at the outermost level of a type and type variables can be instantiated only with monomorphic types. Type inference for unrestricted System F-style polymorphism is undecidable in general. Nevertheless, the literature abounds with a range of proposals to bridge the gap between ML and System F. We put forth a new proposal, FreezeML, a conservative extension of ML with two new features. First, let- and lambda-binders may be annotated with arbitrary System F types. Second, variable occurrences may be frozen, explicitly disabling instantiation. FreezeML is equipped with type-preserving translations back and forth between System F and admits a type inference algorithm, an extension of algorithm W, that is sound and complete and which yields principal types.},
  isbn = {978-1-4503-7613-6},
  keywords = {first-class polymorphism,impredicative types,type inference},
  file = {/home/vipa/Zotero/storage/54ZW9YPE/Emrich et al. - 2020 - FreezeML complete and easy type inference for fir.pdf}
}

@inproceedings{engelkeInstrewLeveragingLLVM2020,
  title = {Instrew: Leveraging {{LLVM}} for High Performance Dynamic Binary Instrumentation},
  shorttitle = {Instrew},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}/{{SIGOPS International Conference}} on {{Virtual Execution Environments}}},
  author = {Engelke, Alexis and Schulz, Martin},
  year = {2020},
  month = mar,
  series = {{{VEE}} '20},
  pages = {172--184},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3381052.3381319},
  urldate = {2020-11-20},
  abstract = {Dynamic binary instrumentation frameworks are popular tools to enhance programs with additional analysis, debugging, or profiling facilities or to add optimizations or translations without requiring recompilation or access to source code. They analyze the binary code, translate into a---typically low-level---intermediate representation, add the needed instrumentation or transformation and then generate new code on-demand and at run-time. Most tools thereby focus on a fast code rewriting process at the cost of lower quality code, leading to a significant slowdown in the instrumented code. Further, most tools run in the application's address space, making their development cumbersome. We propose a novel dynamic binary instrumentation framework, Instrew, which closes these gaps by (a) leveraging the LLVM compiler infrastructure for high-quality code optimization and generation and (b) enables process isolation between the target code and the instrumenter. Instead of using our own non-portable and low-level intermediate representation, our framework directly lifts the original machine code into LLVM-IR, where instrumentation and behavioral changes may be performed, and from which high quality code can be produced. Results on the SPEC CPU2017 benchmarks show that the rewriting overhead is only 1/5 of the overhead incurred using the state-of-the-art toolchain Valgrind.},
  isbn = {978-1-4503-7554-2},
  keywords = {client/server model,dynamic binary instrumentation,dynamic binary translation,LLVM,optimization},
  file = {/home/vipa/Zotero/storage/Y9FD5JA4/Engelke and Schulz - 2020 - Instrew leveraging LLVM for high performance dyna.pdf}
}

@inproceedings{erdwegLayoutSensitiveGeneralizedParsing2013,
  title = {Layout-{{Sensitive Generalized Parsing}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and K{\"a}stner, Christian and Ostermann, Klaus},
  editor = {Czarnecki, Krzysztof and Hedin, G{\"o}rel},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {244--263},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-36089-3_14},
  abstract = {The theory of context-free languages is well-understood and context-free parsers can be used as off-the-shelf tools in practice. In particular, to use a context-free parser framework, a user does not need to understand its internals but can specify a language declaratively as a grammar. However, many languages in practice are not context-free. One particularly important class of such languages is layout-sensitive languages, in which the structure of code depends on indentation and whitespace. For example, Python, Haskell, F\#, and Markdown use indentation instead of curly braces to determine the block structure of code. Their parsers (and lexers) are not declaratively specified but hand-tuned to account for layout-sensitivity.To support declarative specifications of layout-sensitive languages, we propose a parsing framework in which a user can annotate layout in a grammar. Annotations take the form of constraints on the relative positioning of tokens in the parsed subtrees. For example, a user can declare that a block consists of statements that all start on the same column. We have integrated layout constraints into SDF and implemented a layout-sensitive generalized parser as an extension of generalized LR parsing. We evaluate the correctness and performance of our parser by parsing 33 290 open-source Haskell files. Layout-sensitive generalized parsing is easy to use, and its performance overhead compared to layout-insensitive parsing is small enough for practical application.},
  isbn = {978-3-642-36089-3},
  langid = {english},
  keywords = {Abstract Syntax Tree,Curly Brace,Parse Time,Parse Tree,Statement List},
  file = {/home/vipa/Zotero/storage/FGX5HIV4/Erdweg et al. - 2013 - Layout-Sensitive Generalized Parsing.pdf}
}

@inproceedings{erdwegStateArtLanguage2013,
  title = {The {{State}} of the {{Art}} in {{Language Workbenches}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Erdweg, Sebastian and {van der Storm}, Tijs and V{\"o}lter, Markus and Boersma, Meinte and Bosman, Remi and Cook, William R. and Gerritsen, Albert and Hulshout, Angelo and Kelly, Steven and Loh, Alex and Konat, Gabri{\"e}l D. P. and Molina, Pedro J. and Palatnik, Martin and Pohjonen, Risto and Schindler, Eugen and Schindler, Klemens and Solmi, Riccardo and Vergu, Vlad A. and Visser, Eelco and {van der Vlist}, Kevin and Wachsmuth, Guido H. and {van der Woning}, Jimi},
  editor = {Erwig, Martin and Paige, Richard F. and Van Wyk, Eric},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {197--217},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-02654-1_11},
  abstract = {Language workbenches are tools that provide high-level mechanisms for the implementation of (domain-specific) languages. Language workbenches are an active area of research that also receives many contributions from industry. To compare and discuss existing language workbenches, the annual Language Workbench Challenge was launched in 2011. Each year, participants are challenged to realize a given domain-specific language with their workbenches as a basis for discussion and comparison. In this paper, we describe the state of the art of language workbenches as observed in the previous editions of the Language Workbench Challenge. In particular, we capture the design space of language workbenches in a feature model and show where in this design space the participants of the 2013 Language Workbench Challenge reside. We compare these workbenches based on a DSL for questionnaires that was realized in all workbenches.},
  isbn = {978-3-319-02654-1},
  langid = {english},
  keywords = {Design Space,Digital Forensic,Feature Model,Reference Resolution,Software Product Line},
  file = {/home/vipa/Zotero/storage/2TNXFWMF/Erdweg et al. - 2013 - The State of the Art in Language Workbenches.pdf}
}

@inproceedings{erdwegSugarJLibrarybasedSyntactic2011,
  title = {{{SugarJ}}: Library-Based Syntactic Language Extensibility},
  shorttitle = {{{SugarJ}}},
  booktitle = {Proceedings of the 2011 {{ACM}} International Conference on {{Object}} Oriented Programming Systems Languages and Applications},
  author = {Erdweg, Sebastian and Rendel, Tillmann and K{\"a}stner, Christian and Ostermann, Klaus},
  year = {2011},
  month = oct,
  series = {{{OOPSLA}} '11},
  pages = {391--406},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2048066.2048099},
  urldate = {2022-09-09},
  abstract = {Existing approaches to extend a programming language with syntactic sugar often leave a bitter taste, because they cannot be used with the same ease as the main extension mechanism of the programming language - libraries. Sugar libraries are a novel approach for syntactically extending a programming language within the language. A sugar library is like an ordinary library, but can, in addition, export syntactic sugar for using the library. Sugar libraries maintain the composability and scoping properties of ordinary libraries and are hence particularly well-suited for embedding a multitude of domain-specific languages into a host language. They also inherit self-applicability from libraries, which means that sugar libraries can provide syntactic extensions for the definition of other sugar libraries. To demonstrate the expressiveness and applicability of sugar libraries, we have developed SugarJ, a language on top of Java, SDF and Stratego, which supports syntactic extensibility. SugarJ employs a novel incremental parsing technique, which allows changing the syntax within a source file. We demonstrate SugarJ by five language extensions, including embeddings of XML and closures in Java, all available as sugar libraries. We illustrate the utility of self-applicability by embedding XML Schema, a metalanguage to define XML languages.},
  isbn = {978-1-4503-0940-0},
  keywords = {DSL embedding,language composition,language extensibility,libraries,SugarJ,syntactic sugar},
  file = {/home/vipa/Zotero/storage/7Y3JH843/Erdweg et al. - 2011 - SugarJ library-based syntactic language extensibi.pdf}
}

@inproceedings{erikssonPartialEvaluationAutomatic2023,
  title = {Partial {{Evaluation}} of {{Automatic Differentiation}} for {{Differential-Algebraic Equations Solvers}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Eriksson, Oscar and Palmkvist, Viktor and Broman, David},
  year = {2023},
  month = oct,
  series = {{{GPCE}} 2023},
  pages = {57--71},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3624007.3624054},
  urldate = {2024-07-29},
  abstract = {Differential-Algebraic Equations (DAEs) are the foundation of high-level equation-based languages for modeling physical dynamical systems. Simulating models in such languages requires a transformation known as index reduction that involves differentiating individual equations before numerical integration. Commercial and open-source implementations typically perform index reduction by symbolic differentiation (SD) and produce a Jacobian callback function with forward-mode automatic differentiation (AD). The former results in efficient runtime code, and the latter is asymptotically efficient in both runtime and code size. However, AD introduces runtime overhead caused by a non-standard representation of real numbers, and SD is not always applicable in models with general recursion. This work proposes a new approach that uses partial evaluation of AD in the context of numerical DAE solving to combine the strengths of the two differentiation methods while mitigating their weaknesses. Moreover, our approach selectively specializes partial derivatives of the Jacobian by exploiting structural knowledge while respecting a user-defined bound on the code size. Our evaluation shows that the new method both enables expressive modeling from AD and retains the efficiency of SD for many practical applications.},
  isbn = {9798400704062},
  file = {/home/vipa/Zotero/storage/XL8C8P6Z/Eriksson et al. - 2023 - Partial Evaluation of Automatic Differentiation fo.pdf}
}

@article{faberStrongEquivalenceQualitative2013,
  title = {Strong {{Equivalence}} of {{Qualitative Optimization Problems}}},
  author = {Faber, W. and Truszczy{\'n}ski, M. and Woltran, S.},
  year = {2013},
  month = jul,
  journal = {Journal of Artificial Intelligence Research},
  volume = {47},
  pages = {351--391},
  issn = {1076-9757},
  doi = {10.1613/jair.3991},
  urldate = {2020-11-20},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/home/vipa/Zotero/storage/82S7MRDD/Faber et al. - 2013 - Strong Equivalence of Qualitative Optimization Pro.pdf;/home/vipa/Zotero/storage/3LRQ7UEV/10822.html}
}

@inproceedings{farmerHERMITStreamFusing2014,
  title = {The {{HERMIT}} in the Stream: Fusing Stream Fusion's {{concatMap}}},
  shorttitle = {The {{HERMIT}} in the Stream},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2014 {{Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Farmer, Andrew and {Hoener zu Siederdissen}, Christian and Gill, Andy},
  year = {2014},
  month = jan,
  series = {{{PEPM}} '14},
  pages = {97--108},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2543728.2543736},
  urldate = {2020-11-20},
  abstract = {Stream Fusion, a popular deforestation technique in the Haskell community, cannot fuse the concatMap combinator. This is a serious limitation, as concatMap represents computations on nested streams. The original implementation of Stream Fusion used the Glasgow Haskell Compiler's user-directed rewriting system. A transformation which allows the compiler to fuse many uses of concatMap has previously been proposed, but never implemented, because the host rewrite system was not expressive enough to implement the proposed transformation. In this paper, we develop a custom optimization plugin which implements the proposed concatMap transformation, and study the effectiveness of the transformation in practice. We also provide a new translation scheme for list comprehensions which enables them to be optimized. Within this framework, we extend the transformation to monadic streams. Code featuring uses of concatMap experiences significant speedup when compiled with this optimization. This allows Stream Fusion to outperform its rival, foldr/build, on many list computations, and enables performance-sensitive code to be expressed at a higher level of abstraction.},
  isbn = {978-1-4503-2619-3},
  keywords = {deforestation,functional programming,ghc,haskell,optimization,program fusion,program transformation,stream fusion},
  file = {/home/vipa/Zotero/storage/4DDI7GA8/Farmer et al. - 2014 - The HERMIT in the stream fusing stream fusion's c.pdf}
}

@inproceedings{farrowComposableAttributeGrammars1992,
  title = {Composable {{Attribute Grammars}}: {{Support}} for {{Modularity}} in {{Translator Design}} and {{Implementation}}},
  shorttitle = {Composable {{Attribute Grammars}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Farrow, R. and Marlowe, T. J. and Yellin, D. M.},
  year = {1992},
  series = {{{POPL}} '92},
  pages = {223--234},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/143165.143210},
  urldate = {2018-10-29},
  abstract = {This paper introduces Composable Attribute Grammars (CAGs), a formalism that extends classical attribute grammars to allow for the modular composition of translation specifications and of translators. CAGs bring to complex translator writing systems the same benefits of modularity found in modern programming languages, including comprehensibility, reusability, and incremental meta-compilation. A CAG is built from several smaller component AGs, each of which solves a particular subproblem, such as name analysis or register allocation. A component AG is based upon a simplified phrase-structure that reflects the properties of its subproblem rather than the phrase-structure of the source language. Different component phrase-structures for various subproblems are combined by mapping them into a phrase-structure for the source language. Both input and output attributes can be associated with the terminal symbols of a component AG. Output attributes enable the results of solving a subproblem to be distributed back to anywhere that originally contributed part of the subproblem, e.g. transparently distributing the results of global name analysis back to every symbolic reference in the source program. After introducing CAGs by way of an example, we provide a formal definition of CAGs and their semantics. We describe a subclass of CAGs and their semantics. We describe a subclass of CAGs, called separable CAGs, that have favorable implementation properties. We discuss the novel aspects of CAGs, compare them to other proposals for inserting modularity into attribute grammars, and relate our experience using CAGs in the Linguist translator-writing system.},
  isbn = {978-0-89791-453-6},
  file = {/home/vipa/Zotero/storage/LWGKUVTC/Farrow et al. - 1992 - Composable Attribute Grammars Support for Modular.pdf}
}

@inproceedings{felleisenExpressivePowerProgramming1990,
  title = {On the Expressive Power of Programming Languages},
  booktitle = {{{ESOP}} '90},
  author = {Felleisen, Matthias},
  editor = {Jones, Neil},
  year = {1990},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {134--151},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-52592-0_60},
  abstract = {The literature on programming languages contains an abundance of informal claims on the relative expressive power of programming languages, but there is no framework for formalizing such statements nor for deriving interesting consequences. As a first step in this direction, we develop a formal notion of expressiveness and investigate its properties. To demonstrate the theory's closeness to published intuitions on expressiveness, we analyze the expressive power of several extensions of functional languages. Based on these results, we believe that our system correctly captures many of the informal ideas on expressiveness, and that it constitutes a good basis for further research in this direction.},
  isbn = {978-3-540-47045-8},
  langid = {english},
  keywords = {Expressive Power,Formal Framework,Operational Semantic,Programming Language,Pure Scheme},
  file = {/home/vipa/Zotero/storage/WDVEXDX2/Felleisen - 1990 - On the expressive power of programming languages.pdf}
}

@inproceedings{flattBindingSetsScopes2016,
  title = {Binding {{As Sets}} of {{Scopes}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Flatt, Matthew},
  year = {2016},
  series = {{{POPL}} '16},
  pages = {705--717},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2837614.2837620},
  urldate = {2018-10-18},
  abstract = {Our new macro expander for Racket builds on a novel approach to hygiene. Instead of basing macro expansion on variable renamings that are mediated by expansion history, our new expander tracks binding through a set of scopes that an identifier acquires from both binding forms and macro expansions. The resulting model of macro expansion is simpler and more uniform than one based on renaming, and it is sufficiently compatible with Racket's old expander to be practical.},
  isbn = {978-1-4503-3549-2},
  keywords = {binding,hygiene,Macros,scope},
  file = {/home/vipa/Zotero/storage/CC4NBR3P/Flatt - 2016 - Binding As Sets of Scopes.pdf}
}

@article{flattMacrosThatWork2012,
  title = {Macros That {{Work Together}}: {{Compile-time}} Bindings, Partial Expansion, and Definition Contexts},
  shorttitle = {Macros That {{Work Together}}},
  author = {Flatt, Matthew and Culpepper, Ryan and Darais, David and Findler, Robert Bruce},
  year = {2012},
  month = mar,
  journal = {Journal of Functional Programming},
  volume = {22},
  number = {2},
  pages = {181--216},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796812000093},
  urldate = {2018-10-18},
  abstract = {Racket is a large language that is built mostly within itself. Unlike the usual approach taken by non-Lisp languages, the self-hosting of Racket is not a matter of bootstrapping one implementation through a previous implementation, but instead a matter of building a tower of languages and libraries via macros. The upper layers of the tower include a class system, a component system, pedagogic variants of Scheme, a statically typed dialect of Scheme, and more. The demands of this language-construction effort require a macro system that is substantially more expressive than previous macro systems. In particular, while conventional Scheme macro systems handle stand-alone syntactic forms adequately, they provide weak support for macros that share information or macros that use existing syntactic forms in new contexts. This paper describes and models features of the Racket macro system, including support for general compile-time bindings, sub-form expansion and analysis, and environment management. The presentation assumes a basic familiarity with Lisp-style macros, and it takes for granted the need for macros that respect lexical scope. The model, however, strips away the pattern and template system that is normally associated with Scheme macros, isolating a core that is simpler, can support pattern and template forms themselves as macros, and generalizes naturally to Racket's other extensions.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/R4TIYA5U/Flatt et al. - 2012 - Macros that Work Together Compile-time bindings, .pdf;/home/vipa/Zotero/storage/7GE65QYL/375043C6746405B22014D235FA4C90C3.html}
}

@techreport{flattReferenceRacket2010,
  title = {Reference: {{Racket}}},
  author = {Flatt, Matthew and {PLT}},
  year = {2010},
  number = {PLT-TR-2010-1},
  institution = {PLT Design Inc.}
}

@article{flattRhombusNewSpin2023,
  title = {Rhombus: {{A New Spin}} on {{Macros}} without {{All}} the {{Parentheses}}},
  shorttitle = {Rhombus},
  author = {Flatt, Matthew and Allred, Taylor and Angle, Nia and De Gabrielle, Stephen and Findler, Robert Bruce and Firth, Jack and Gopinathan, Kiran and Greenman, Ben and Kasivajhula, Siddhartha and Knauth, Alex and McCarthy, Jay and Phillips, Sam and Porncharoenwase, Sorawee and S{\o}gaard, Jens Axel and {Tobin-Hochstadt}, Sam},
  year = {2023},
  month = oct,
  journal = {Artifact for "Rhombus: A New Spin on Macros without All the Parentheses"},
  volume = {7},
  number = {OOPSLA2},
  pages = {242:574--242:603},
  doi = {10.1145/3622818},
  urldate = {2024-07-10},
  abstract = {Rhombus is a new language that is built on Racket. It offers the same    kind of language extensibility as Racket itself, but using traditional    (infix) notation. Although Rhombus is far from the first language to    support Lisp-style macros without Lisp-style parentheses, Rhombus offers    a novel synthesis of macro technology that is practical and expressive.    A key element is the use of multiple binding spaces    for context-specific sublanguages. For example, expressions and    pattern-matching forms can use the    same operators with different meanings and without creating conflicts.    Context-sensitive bindings, in turn, facilitate a language design that    reduces the notational distance between the core language and macro    facilities. For example, repetitions can be defined and used in binding    and expression contexts generally, which enables a smoother transition    from programming to metaprogramming.    Finally, since handling static information (such as types) is also a    necessary part of growing macros beyond Lisp, Rhombus includes support    in its expansion protocol for communicating static information among    bindings and expressions.    The Rhombus implementation    demonstrates that all of these pieces can work together in a coherent    and user-friendly language.},
  file = {/home/vipa/Zotero/storage/HSMNGHUZ/Flatt et al. - 2023 - Rhombus A New Spin on Macros without All the Pare.pdf}
}

@article{floydSyntacticAnalysisOperator1963,
  title = {Syntactic {{Analysis}} and {{Operator Precedence}}},
  author = {Floyd, Robert W.},
  year = {1963},
  month = jul,
  journal = {Journal of the ACM},
  volume = {10},
  number = {3},
  pages = {316--333},
  issn = {0004-5411},
  doi = {10.1145/321172.321179},
  urldate = {2022-09-07},
  file = {/home/vipa/Zotero/storage/AMTLXR4U/Floyd - 1963 - Syntactic Analysis and Operator Precedence.pdf}
}

@inproceedings{fordParsingExpressionGrammars2004,
  title = {Parsing {{Expression Grammars}}: {{A Recognition-based Syntactic Foundation}}},
  shorttitle = {Parsing {{Expression Grammars}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ford, Bryan},
  year = {2004},
  series = {{{POPL}} '04},
  pages = {111--122},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/964001.964011},
  urldate = {2019-10-23},
  abstract = {For decades we have been using Chomsky's generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machine-oriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.},
  isbn = {978-1-58113-729-3},
  keywords = {BNF,context-free grammars,GTDPL,lexical analysis,packrat parsing,parsing expression grammars,regular expressions,scannerless parsing,syntactic predicates,TDPL,unified grammars},
  file = {/home/vipa/Zotero/storage/ZU3DSLAW/Ford - 2004 - Parsing Expression Grammars A Recognition-based S.pdf}
}

@misc{fowlerLanguageWorkbenchesKillerApp2005,
  title = {Language {{Workbenches}}: {{The Killer-App}} for {{Domain Specific Languages}}?},
  shorttitle = {Language {{Workbenches}}},
  author = {Fowler, Martin},
  year = {2005},
  journal = {martinfowler.com},
  urldate = {2024-06-24},
  howpublished = {https://martinfowler.com/articles/languageWorkbench.html},
  file = {/home/vipa/Zotero/storage/TT7CY9Q5/languageWorkbench.html}
}

@inproceedings{franchettiFFTProgramGeneration2006,
  title = {{{FFT}} Program Generation for Shared Memory: {{SMP}} and Multicore},
  shorttitle = {{{FFT}} Program Generation for Shared Memory},
  booktitle = {Proceedings of the 2006 {{ACM}}/{{IEEE}} Conference on {{Supercomputing}}},
  author = {Franchetti, Franz and Voronenko, Yevgen and P{\"u}schel, Markus},
  year = {2006},
  month = nov,
  series = {{{SC}} '06},
  pages = {115--es},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1188455.1188575},
  urldate = {2020-11-27},
  abstract = {The chip maker's response to the approaching end of CPU frequency scaling are multicore systems, which offer the same programming paradigm as traditional shared memory platforms but have different performance characteristics. This situation considerably increases the burden on library developers and strengthens the case for automatic performance tuning frameworks like Spiral, a program generator and optimizer for linear transforms such as the discrete Fourier transform (DFT). We present a shared memory extension of Spiral. The extension within Spiral consists of a rewriting system that manipulates the structure of transform algorithms to achieve load balancing and avoids false sharing, and of a backend to generate multithreaded code. Application to the DFT produces a novel class of algorithms suitable for multicore systems as validated by experimental results: we demonstrate a parallelization speed-up already for sizes that fit into L1 cache and compare favorably to other DFT libraries across all small and midsize DFTs and considered platforms.},
  isbn = {978-0-7695-2700-0},
  keywords = {automatic parallelization,chip multiprocessor,fast fourier transform,multicore,shared memory},
  file = {/home/vipa/Zotero/storage/KU8VGHU3/Franchetti et al. - 2006 - FFT program generation for shared memory SMP and .pdf}
}

@inproceedings{francoYouCanHave2017,
  title = {You Can Have It All: Abstraction and Good Cache Performance},
  shorttitle = {You Can Have It All},
  booktitle = {Proceedings of the 2017 {{ACM SIGPLAN International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {Franco, Juliana and Hagelin, Martin and Wrigstad, Tobias and Drossopoulou, Sophia and Eisenbach, Susan},
  year = {2017},
  month = oct,
  series = {Onward! 2017},
  pages = {148--167},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3133850.3133861},
  urldate = {2024-01-06},
  abstract = {On current architectures, the optimisation of an application's performance often involves data being stored according to access affinity --- what is accessed together should be stored together, rather than logical affinity --- what belongs together logically stays together. Such low level techniques lead to faster, but more error prone code, and end up tangling the program's logic with low-level data layout details. Our vision, which we call SHAPES --- Safe, High-level, Abstractions for oPtimisation of mEmory cacheS --- is that the layout of a data structure should be defined only once, upon instantiation, and the remainder of the code should be layout agnostic. This enables performance improvements while also guaranteeing memory safety, and supports the separation of program logic from low level concerns. In this paper we investigate how this vision can be supported by extending a programming language. We describe the core language features supporting this vision: classes can be customized to support different layouts, and layout information is carried around in types; the remaining source code is layout-unaware and the compiler emits layout-aware code. We then discuss our SHAPES implementation through a prototype library, which we also used for preliminary evaluations. Finally, we discuss how the core could be expanded so as to deliver SHAPES's full potential: the incorporation of compacting garbage collection, ad hoc polymorphism and late binding, synchronization of representations of different collections, support for dynamic change of representation, etc.},
  isbn = {978-1-4503-5530-8},
  keywords = {object layout},
  file = {/home/vipa/Zotero/storage/WUIRXU5K/Franco et al. - 2017 - You can have it all abstraction and good cache pe.pdf}
}

@inproceedings{garciaComparativeStudyLanguage2003,
  title = {A Comparative Study of Language Support for Generic Programming},
  booktitle = {Proceedings of the 18th Annual {{ACM SIGPLAN}} Conference on {{Object-oriented}} Programing, Systems, Languages, and Applications},
  author = {Garcia, Ronald and Jarvi, Jaakko and Lumsdaine, Andrew and Siek, Jeremy G. and Willcock, Jeremiah},
  year = {2003},
  month = oct,
  series = {{{OOPSLA}} '03},
  pages = {115--134},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/949305.949317},
  urldate = {2022-09-01},
  abstract = {Many modern programming languages support basic generic programming, sufficient to implement type-safe polymorphic containers. Some languages have moved beyond this basic support to a broader, more powerful interpretation of generic programming, and their extensions have proven valuable in practice. This paper reports on a comprehensive comparison of generics in six programming languages: C++, Standard ML, Haskell, Eiffel, Java (with its proposed generics extension), and Generic C. By implementing a substantial example in each of these languages, we identify eight language features that support this broader view of generic programming. We find these features are necessary to avoid awkward designs, poor maintainability, unnecessary run-time checks, and painfully verbose code. As languages increasingly support generics, it is important that language designers understand the features necessary to provide powerful generics and that their absence causes serious difficulties for programmers.},
  isbn = {978-1-58113-712-5},
  keywords = {C,C++,Eiffel,generic programming,generics,Haskell,Java,polymorphism,standard ML},
  file = {/home/vipa/Zotero/storage/XVY7YSET/Garcia et al. - 2003 - A comparative study of language support for generi.pdf}
}

@inproceedings{georgakoudisFastDynamicBinary2013,
  title = {Fast Dynamic Binary Rewriting to Support Thread Migration in Shared-{{ISA}} Asymmetric Multicores},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Code OptimiSation}} for {{MultI}} and Many {{Cores}}},
  author = {Georgakoudis, Giorgis and Nikolopoulos, Dimitrios S. and Lalis, Spyros},
  year = {2013},
  month = feb,
  series = {{{COSMIC}} '13},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2446920.2446924},
  urldate = {2020-11-20},
  abstract = {Asymmetric multicore processors have demonstrated a strong potential for improving performance and energy-efficiency. Shared-ISA asymmetric multicore processors overcome programmability problems in disjoint-ISA systems and enhance single-ISA architectures with instruction based asymmetry. In such a design, processors share a common, baseline ISA and performance enhanced (PE) cores extend the baseline ISA with instructions that accelerate performance-critical operations. To exploit asymmetry, the scheduler should be able to migrate threads based on their acceleration potential. The contribution of this paper is a low overhead binary code rewriting method for shared-ISA multicore processors that transforms a binary executable at runtime, according to the scheduled processor's PE capabilities. The mutable binary code can be re-targeted among heterogeneous cores at any point in execution while preserving functional equivalence and using PE instructions, transparently, when available, thus enabling migrations among heterogeneous cores. We emulate a realistic shared-ISA asymmetric multicore system using actual hardware -- an FPGA experimental prototype. Experimental analysis shows that dynamic binary rewriting is feasible with little overhead. Rewritten code speeds up successfully baseline code while performing close, with 70\% average efficiency, to non-portable, compiler generated code, statically optimized to use PE instructions.},
  isbn = {978-1-4503-1971-3},
  keywords = {binary rewriting,code optimization,heterogeneous multicore,shared asymmetric ISA},
  file = {/home/vipa/Zotero/storage/SKRE4V7T/Georgakoudis et al. - 2013 - Fast dynamic binary rewriting to support thread mi.pdf}
}

@article{ginsburgAmbiguityContextFree1966,
  title = {Ambiguity in {{Context Free Languages}}},
  author = {Ginsburg, Seymour and Ullian, Joseph},
  year = {1966},
  month = jan,
  journal = {Journal of the ACM},
  volume = {13},
  number = {1},
  pages = {62--89},
  issn = {0004-5411},
  doi = {10.1145/321312.321318},
  urldate = {2018-10-18},
  abstract = {Four principal results about ambiguity in languages (i.e., context free languages) are proved. It is first shown that the problem of determining whether an arbitrary language is inherently ambiguous is recursively unsolvable. Then a decision procedure is presented for determining whether an arbitrary bounded grammar is ambiguous. Next, a necessary and sufficient algebraic condition is given for a bounded language to be inherently ambiguous. Finally, it is shown that no language contained in w1*w2*, each w1 a word, is inherently ambiguous.},
  file = {/home/vipa/Zotero/storage/E4B6B9IG/Ginsburg and Ullian - 1966 - Ambiguity in Context Free Languages.pdf}
}

@inproceedings{giorgidzeEmbeddingFunctionalHybrid2011,
  title = {Embedding a {{Functional Hybrid Modelling Language}} in {{Haskell}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Giorgidze, George and Nilsson, Henrik},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {138--155},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  abstract = {In this paper we present the first investigation into the implementation of a Functional Hybrid Modelling language for non-causal modelling and simulation of physical systems. In particular, we present a simple way to handle connect constructs: a facility for composing model fragments present in some form in most non-causal modelling languages. Our implementation is realised as a domain-specific language embedded in Haskell. The method of embedding employs quasiquoting, thus demonstrating the effectiveness of this approach for languages that are not suitable for embedding in more traditional ways. Our implementation is available on-line, and thus the first publicly available prototype implementation of a Functional Hybrid Modelling language.},
  isbn = {978-3-642-24452-0},
  langid = {english},
  keywords = {Abstract Syntax,Functional Programming,Modelling Language,Signal Function,Signal Relation},
  file = {/home/vipa/Zotero/storage/4URDAB46/Giorgidze and Nilsson - 2011 - Embedding a Functional Hybrid Modelling Language i.pdf}
}

@article{gomesCoSimulationSurvey2018,
  title = {Co-{{Simulation}}: {{A Survey}}},
  shorttitle = {Co-{{Simulation}}},
  author = {Gomes, Cl{\'a}udio and Thule, Casper and Broman, David and Larsen, Peter Gorm and Vangheluwe, Hans},
  year = {2018},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {3},
  pages = {49:1--49:33},
  issn = {0360-0300},
  doi = {10.1145/3179993},
  urldate = {2022-08-24},
  abstract = {Modeling and simulation techniques are today extensively used both in industry and science. Parts of larger systems are, however, typically modeled and simulated by different techniques, tools, and algorithms. In addition, experts from different disciplines use various modeling and simulation techniques. Both these facts make it difficult to study coupled heterogeneous systems. Co-simulation is an emerging enabling technique, where global simulation of a coupled system can be achieved by composing the simulations of its parts. Due to its potential and interdisciplinary nature, co-simulation is being studied in different disciplines but with limited sharing of findings. In this survey, we study and survey the state-of-the-art techniques for co-simulation, with the goal of enhancing future research and highlighting the main challenges. To study this broad topic, we start by focusing on discrete-event-based co-simulation, followed by continuous-time-based co-simulation. Finally, we explore the interactions between these two paradigms, in hybrid co-simulation. To survey the current techniques, tools, and research challenges, we systematically classify recently published research literature on co-simulation, and summarize it into a taxonomy. As a result, we identify the need for finding generic approaches for modular, stable, and accurate coupling of simulation units, as well as expressing the adaptations required to ensure that the coupling is correct.},
  keywords = {Co-simulation,compositionality,simulation},
  file = {/home/vipa/Zotero/storage/B82CGYPM/Gomes et al. - 2018 - Co-Simulation A Survey.pdf}
}

@article{grafLowerYourGuards2020,
  title = {Lower Your Guards: A Compositional Pattern-Match Coverage Checker},
  shorttitle = {Lower Your Guards},
  author = {Graf, Sebastian and Peyton Jones, Simon and Scott, Ryan G.},
  year = {2020},
  month = aug,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {ICFP},
  pages = {107:1--107:30},
  doi = {10.1145/3408989},
  urldate = {2021-08-20},
  abstract = {A compiler should warn if a function defined by pattern matching does not cover its inputs---that is, if there are missing or redundant patterns. Generating such warnings accurately is difficult for modern languages due to the myriad of language features that interact with pattern matching. This is especially true in Haskell, a language with a complicated pattern language that is made even more complex by extensions offered by the Glasgow Haskell Compiler (GHC). Although GHC has spent a significant amount of effort towards improving its pattern-match coverage warnings, there are still several cases where it reports inaccurate warnings. We introduce a coverage checking algorithm called Lower Your Guards, which boils down the complexities of pattern matching into guard trees. While the source language may have many exotic forms of patterns, guard trees only have three different constructs, which vastly simplifies the coverage checking process. Our algorithm is modular, allowing for new forms of source-language patterns to be handled with little changes to the overall structure of the algorithm. We have implemented the algorithm in GHC and demonstrate places where it performs better than GHC's current coverage checker, both in accuracy and performance.},
  keywords = {guards,Haskell,pattern matching,strictness},
  file = {/home/vipa/Zotero/storage/Z3RPKVAQ/Graf et al. - 2020 - Lower your guards a compositional pattern-match c.pdf}
}

@article{griffithElizabethHolmesTrial2022,
  title = {Elizabeth {{Holmes Trial}}: {{Elizabeth Holmes Found Guilty}} of {{Four Charges}} of {{Fraud}}},
  shorttitle = {Elizabeth {{Holmes Trial}}},
  author = {Griffith, Erin and Woo, Erin},
  year = {2022},
  month = jan,
  journal = {The New York Times},
  issn = {0362-4331},
  urldate = {2023-03-17},
  chapter = {Technology},
  langid = {american}
}

@book{gruneParsingTechniquesPractical2008,
  title = {Parsing Techniques: A Practical Guide},
  shorttitle = {Parsing Techniques},
  author = {Grune, Dick and Jacobs, Ceriel J. H.},
  year = {2008},
  series = {Monographs in Computer Science},
  edition = {2nd ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-20248-8 978-0-387-68954-8},
  langid = {english},
  lccn = {P98.5.P38 G78 2008},
  keywords = {Parsing,Parsing (Computer grammar),Syntaktische Analyse},
  annotation = {OCLC: ocn191726482},
  file = {/home/vipa/Zotero/storage/3DZNU4BV/Grune and Jacobs - 2008 - Parsing techniques a practical guide.pdf}
}

@inproceedings{guibasDichromaticFrameworkBalanced1978,
  title = {A Dichromatic Framework for Balanced Trees},
  booktitle = {19th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1978)},
  author = {Guibas, Leo J. and Sedgewick, Robert},
  year = {1978},
  month = oct,
  pages = {8--21},
  issn = {0272-5428},
  doi = {10.1109/SFCS.1978.3},
  urldate = {2024-02-24},
  abstract = {In this paper we present a uniform framework for the implementation and study of balanced tree algorithms. We show how to imbed in this framework the best known balanced tree techniques and then use the framework to develop new algorithms which perform the update and rebalancing in one pass, on the way down towards a leaf. We conclude with a study of performance issues and concurrent updating.},
  keywords = {Algorithm design and analysis,Computer science,Particle measurements,Performance analysis,Petroleum},
  file = {/home/vipa/Zotero/storage/CAXYX4YS/4567957.html}
}

@article{hagendorffEthicsAIEthics2020,
  title = {The {{Ethics}} of {{AI Ethics}}: {{An Evaluation}} of {{Guidelines}}},
  shorttitle = {The {{Ethics}} of {{AI Ethics}}},
  author = {Hagendorff, Thilo},
  year = {2020},
  month = mar,
  journal = {Minds and Machines},
  volume = {30},
  number = {1},
  pages = {99--120},
  issn = {1572-8641},
  doi = {10.1007/s11023-020-09517-8},
  urldate = {2024-06-22},
  abstract = {Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the ``disruptive'' potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems---and how the effectiveness in the demands of AI ethics can be improved.},
  langid = {english},
  keywords = {Artificial intelligence,Ethics,Guidelines,Implementation,Machine learning},
  file = {/home/vipa/Zotero/storage/F49VUKQX/Hagendorff - 2020 - The Ethics of AI Ethics An Evaluation of Guidelin.pdf}
}

@inproceedings{hawkinsDataRepresentationSynthesis2011,
  title = {Data Representation Synthesis},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Hawkins, Peter and Aiken, Alex and Fisher, Kathleen and Rinard, Martin and Sagiv, Mooly},
  year = {2011},
  month = jun,
  series = {{{PLDI}} '11},
  pages = {38--49},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1993498.1993504},
  urldate = {2022-11-15},
  abstract = {We consider the problem of specifying combinations of data structures with complex sharing in a manner that is both declarative and results in provably correct code. In our approach, abstract data types are specified using relational algebra and functional dependencies. We describe a language of decompositions that permit the user to specify different concrete representations for relations, and show that operations on concrete representations soundly implement their relational specification. It is easy to incorporate data representations synthesized by our compiler into existing systems, leading to code that is simpler, correct by construction, and comparable in performance to the code it replaces.},
  isbn = {978-1-4503-0663-8},
  keywords = {composite data structures,synthesis},
  file = {/home/vipa/Zotero/storage/TE9Y2BRI/Hawkins et al. - 2011 - Data representation synthesis.pdf}
}

@article{heeringSyntaxDefinitionFormalism1989,
  title = {The {{Syntax Definition Formalism SDF}}---{{Reference Manual}}---},
  author = {Heering, J. and Hendriks, P. R. H. and Klint, P. and Rekers, J.},
  year = {1989},
  month = nov,
  journal = {SIGPLAN Not.},
  volume = {24},
  number = {11},
  pages = {43--75},
  issn = {0362-1340},
  doi = {10.1145/71605.71607},
  urldate = {2018-10-18},
  abstract = {SDF is a formalism for the definition of syntax which is comparable to BNF in some respects, but has a wider scope in that it also covers the definition of lexical and abstract syntax. Its design and implementation are tailored towards the language designer who wants to develop new languages as well as implement existing ones in a highly interactive manner. It emphasizes compactness of syntax definitions by offering (a) a standard interface between lexical and context-free syntax; (b) a standard correspondence between context-free and abstract syntax; (c) powerful disambiguation and list constructs; and (d) an efficient incremental implementation which accepts arbitrary context-free syntax definitions. SDF can be combined with a variety of programming and specification languages. In this way these obtain fully general user-definable syntax.},
  file = {/home/vipa/Zotero/storage/CFXEDL3D/Heering et al. - 1989 - The Syntax Definition Formalism SDF—Reference Manu.pdf}
}

@misc{heiserGernotListSystems,
  title = {Gernot's {{List}} of {{Systems Benchmarking Crimes}}},
  author = {Heiser, Gernot},
  urldate = {2024-06-28},
  howpublished = {https://gernot-heiser.org/benchmarking-crimes.html},
  file = {/home/vipa/Zotero/storage/BZMBLQTQ/benchmarking-crimes.html}
}

@inproceedings{hermanTheoryHygienicMacros2008,
  title = {A {{Theory}} of {{Hygienic Macros}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {48--62},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Hygienic macro systems, such as Scheme's, automatically rename variables to prevent unintentional variable capture---in short, they ``just work.'' Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  isbn = {978-3-540-78739-6},
  langid = {english},
  keywords = {Core Form,Pattern Variable,Scheme Program,Shape Type,Type Annotation},
  file = {/home/vipa/Zotero/storage/2WZMV7AL/Herman and Wand - 2008 - A Theory of Hygienic Macros.pdf}
}

@article{hermanTheoryTypedHygienic2010,
  title = {A {{Theory}} of {{Typed Hygienic Macros}}},
  author = {Herman, David and Wand, Mitchell},
  year = {2010},
  journal = {Proceedings of the 17th European Symposium on Programming},
  volume = {4960},
  pages = {48},
  issn = {03029743},
  doi = {10.1007/978-3-540-78739-6_4},
  abstract = {We present the {$\lambda$}m-calculus, a semantics for a language of hygienic macros with a non-trivial theory. Unlike Scheme, where programs must be macro- expanded to be analyzed, our semantics admits reasoning about programs as they appear to programmers. Our contributions include a semantics of hygienic macro expansion, a formal definition of {$\alpha$}-equivalence that is independent of expansion, and a proof that expansion preserves {$\alpha$}-equivalence. The key technical component of our language is a type system similar to Culpepper and Felleisens shape types, but with the novel contribution of binding signature types, which specify the bindings and scope of a macros arguments.}
}

@inproceedings{hickeyClojureProgrammingLanguage2008,
  title = {The {{Clojure Programming Language}}},
  booktitle = {Proceedings of the 2008 {{Symposium}} on {{Dynamic Languages}}},
  author = {Hickey, Rich},
  year = {2008},
  series = {{{DLS}} '08},
  pages = {1:1--1:1},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1408681.1408682},
  urldate = {2018-10-18},
  abstract = {Customers and stakeholders have substantial investments in, and are comfortable with the performance, security and stability of, industry-standard platforms like the JVM and CLR. While Java and C\# developers on those platforms may envy the succinctness, flexibility and productivity of dynamic languages, they have concerns about running on customer-approved infrastructure, access to their existing code base and libraries, and performance. In addition, they face ongoing problems dealing with concurrency using native threads and locking. Clojure is an effort in pragmatic dynamic language design in this context. It endeavors to be a general-purpose language suitable in those areas where Java is suitable. It reflects the reality that, for the concurrent programming future, pervasive, unmoderated mutation simply has to go. Clojure meets its goals by: embracing an industry-standard, open platform - the JVM; modernizing a venerable language - Lisp; fostering functional programming with immutable persistent data structures; and providing built-in concurrency support via software transactional memory and asynchronous agents. The result is robust, practical, and fast. This talk will focus on the motivations, mechanisms and experiences of the implementation of Clojure.},
  isbn = {978-1-60558-270-2}
}

@inproceedings{hidakaMarkerDirectedOptimizationUnCAL2012,
  title = {Marker-{{Directed Optimization}} of {{UnCAL Graph Transformations}}},
  booktitle = {Logic-{{Based Program Synthesis}} and {{Transformation}}},
  author = {Hidaka, Soichiro and Hu, Zhenjiang and Inaba, Kazuhiro and Kato, Hiroyuki and Matsuda, Kazutaka and Nakano, Keisuke and Sasano, Isao},
  editor = {Vidal, Germ{\'a}n},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {123--138},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-32211-2_9},
  abstract = {Buneman et al. proposed a graph algebra called UnCAL (Unstructured CALculus) for compositional graph transformations based on structural recursion, and we have recently applied to model transformations. The compositional nature of the algebra greatly enhances the modularity of transformations. However, intermediate results generated between composed transformations cause overhead. Buneman et al. proposed fusion rules that eliminate the intermediate results, but auxiliary rewriting rules that enable the actual application of the fusion rules are not apparent so far. UnCAL graph model includes the concept of markers, which correspond to recursive function call in the structural recursion. We have found that there are many optimization opportunities at rewriting level based on static analysis, especially focusing on markers. The analysis can safely eliminate redundant function calls. Performance evaluation shows its practical effectiveness for non-trivial examples in model transformations.},
  isbn = {978-3-642-32211-2},
  langid = {english},
  keywords = {graph transformations,program transformations,UnCAL},
  file = {/home/vipa/Zotero/storage/6K7WIQ2L/Hidaka et al. - 2012 - Marker-Directed Optimization of UnCAL Graph Transf.pdf}
}

@article{hindleyPrincipalTypeSchemeObject1969,
  title = {The {{Principal Type-Scheme}} of an {{Object}} in {{Combinatory Logic}}},
  author = {Hindley, R.},
  year = {1969},
  journal = {Transactions of the American Mathematical Society},
  volume = {146},
  eprint = {1995158},
  eprinttype = {jstor},
  pages = {29--60},
  publisher = {American Mathematical Society},
  issn = {0002-9947},
  doi = {10.2307/1995158},
  urldate = {2023-10-16},
  file = {/home/vipa/Zotero/storage/3V9HEXK8/Hindley - 1969 - The Principal Type-Scheme of an Object in Combinat.pdf}
}

@article{hoosProgrammingOptimization2012,
  title = {Programming by Optimization},
  author = {Hoos, Holger H.},
  year = {2012},
  month = feb,
  journal = {Communications of the ACM},
  volume = {55},
  number = {2},
  pages = {70--80},
  issn = {0001-0782},
  doi = {10.1145/2076450.2076469},
  urldate = {2020-03-16},
  abstract = {Avoid premature commitment, seek design alternatives, and automatically generate performance-optimized software.},
  file = {/home/vipa/Zotero/storage/J8P7QN75/Hoos - 2012 - Programming by optimization.pdf}
}

@inproceedings{huangExtendingMagicSets2010,
  title = {Extending Magic Sets Technique to Deductive Databases with Uncertainty},
  booktitle = {{{CTIT}} Workshop Proceedings Series},
  author = {Huang, Q. and Shiri, N.},
  year = {2010},
  volume = {WP10},
  pages = {19--34},
  abstract = {The magic sets (MS) rewriting technique was proposed to optimize the efficiency of bottom-up evaluation of datalog programs. This technique has been extended to logic programs with uncertainty, but its application is restricted to frameworks with set based semantics such as fuzzy logic. We show that for a more general case of multi-set semantics, a "straightforward" extension of MS technique could lead to incorrect computation. In this work, we propose an extension of the generalized magic sets technique to deductive databases with uncertainty which use multi-sets as the semantics structure, and establish its correctness. We have developed a testing platform and conducted numerous experiments to evaluate the performance of the proposed technique. The experimental results indicate that different programs enjoy different efficiency gain, depending on the potential facts ratio, which intuitively measures the capacity to improve efficiency. We observed that when this ratio ranges from 1\% to 20\%, the proposed optimization results in 1 to 550 times speed-up compared to evaluation of the original program. Our results also indicate that semi-naive combined with predicate partitioning technique yields the best performance.},
  file = {/home/vipa/Zotero/storage/7X9GE8RK/Huang and Shiri - 2010 - Extending magic sets technique to deductive databa.pdf;/home/vipa/Zotero/storage/DMV7N6WK/display.html}
}

@inproceedings{huBayerImageParallel2012,
  title = {Bayer Image Parallel Decoding Based on {{GPU}}},
  booktitle = {Optoelectronic {{Imaging}} and {{Multimedia Technology II}}},
  author = {Hu, Rihui and Xu, Zhiyong and Wei, Yuxing and Sun, Shaohua},
  year = {2012},
  month = nov,
  volume = {8558},
  pages = {85581T},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.999431},
  urldate = {2020-11-20},
  abstract = {In the photoelectrical tracking system, Bayer image is decompressed in traditional method, which is CPU-based. However, it is too slow when the images become large, for example, 2K\&times;2K\&times;16bit. In order to accelerate the Bayer image decoding, this paper introduces a parallel speedup method for NVIDA's Graphics Processor Unit (GPU) which supports CUDA architecture. The decoding procedure can be divided into three parts: the first is serial part, the second is task-parallelism part, and the last is data-parallelism part including inverse quantization, inverse discrete wavelet transform (IDWT) as well as image post-processing part. For reducing the execution time, the task-parallelism part is optimized by OpenMP techniques. The data-parallelism part could advance its efficiency through executing on the GPU as CUDA parallel program. The optimization techniques include instruction optimization, shared memory access optimization, the access memory coalesced optimization and texture memory optimization. In particular, it can significantly speed up the IDWT by rewriting the 2D (Tow-dimensional) serial IDWT into 1D parallel IDWT. Through experimenting with 1K\&times;1K\&times;16bit Bayer image, data-parallelism part is 10 more times faster than CPU-based implementation. Finally, a CPU+GPU heterogeneous decompression system was designed. The experimental result shows that it could achieve 3 to 5 times speed increase compared to the CPU serial method.},
  file = {/home/vipa/Zotero/storage/CSCI98TB/Hu et al. - 2012 - Bayer image parallel decoding based on GPU.pdf;/home/vipa/Zotero/storage/ZEBNA2KQ/12.999431.html}
}

@article{hudakBuildingDomainspecificEmbedded1996,
  title = {Building {{Domain-specific Embedded Languages}}},
  author = {Hudak, Paul},
  year = {1996},
  month = dec,
  journal = {ACM Comput. Surv.},
  volume = {28},
  number = {4es},
  issn = {0360-0300},
  doi = {10.1145/242224.242477},
  urldate = {2018-10-18},
  file = {/home/vipa/Zotero/storage/R5SFEV8C/Hudak - 1996 - Building Domain-specific Embedded Languages.pdf}
}

@inproceedings{idreosDataCalculatorData2018,
  title = {The {{Data Calculator}}: {{Data Structure Design}} and {{Cost Synthesis}} from {{First Principles}} and {{Learned Cost Models}}},
  shorttitle = {The {{Data Calculator}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Idreos, Stratos and Zoumpatianos, Kostas and Hentschel, Brian and Kester, Michael S. and Guo, Demi},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {535--550},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3183713.3199671},
  urldate = {2022-11-15},
  abstract = {Data structures are critical in any data-driven scenario, but they are notoriously hard to design due to a massive design space and the dependence of performance on workload and hardware which evolve continuously. We present a design engine, the Data Calculator, which enables interactive and semi-automated design of data structures. It brings two innovations. First, it offers a set of fine-grained design primitives that capture the first principles of data layout design: how data structure nodes lay data out, and how they are positioned relative to each other. This allows for a structured description of the universe of possible data structure designs that can be synthesized as combinations of those primitives. The second innovation is computation of performance using learned cost models. These models are trained on diverse hardware and data profiles and capture the cost properties of fundamental data access primitives (e.g., random access). With these models, we synthesize the performance cost of complex operations on arbitrary data structure designs without having to: 1) implement the data structure, 2) run the workload, or even 3) access the target hardware. We demonstrate that the Data Calculator can assist data structure designers and researchers by accurately answering rich what-if design questions on the order of a few seconds or minutes, i.e., computing how the performance (response time) of a given data structure design is impacted by variations in the: 1) design, 2) hardware, 3) data, and 4) query workloads. This makes it effortless to test numerous designs and ideas before embarking on lengthy implementation, deployment, and hardware acquisition steps. We also demonstrate that the Data Calculator can synthesize entirely new designs, auto-complete partial designs, and detect suboptimal design choices.},
  isbn = {978-1-4503-4703-7},
  keywords = {data structure synthesis,learned cost models},
  file = {/home/vipa/Zotero/storage/WDR35XDR/Idreos et al. - 2018 - The Data Calculator Data Structure Design and Cos.pdf}
}

@article{idreosLearningDataStructure2019,
  title = {Learning Data Structure Alchemy},
  author = {Idreos, Stratos and Zoumpatianos, Kostas and Chatterjee, Subarna and Qin, Wilson and Wasay, Abdul and Hentschel, Brian and Kester, Mike and Dayan, Niv and Guo, Demi and Kang, Minseo and Sun, Yiyou},
  year = {2019},
  journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
  volume = {42},
  number = {2},
  pages = {46--57},
  abstract = {We propose a solution based on first principles and AI to the decades-old problem of data structure design. Instead of working on individual designs that each can only be helpful in a small set of environments, we propose the construction of an engine, a Data Alchemist, which learns how to blend fine-grained data structure design principles to automatically synthesize brand new data structures.},
  file = {/home/vipa/Zotero/storage/ZFDZUVNJ/Idreos et al. - 2019 - Learning data structure alchemy.pdf}
}

@article{idreosPeriodicTableData2018,
  title = {The Periodic Table of Data Structures},
  author = {Idreos, Stratos and Zoumpatianos, Kostas and Athanassoulis, Manos and Dayan, Niv and Hentschel, Brian and Kester, Michael S. and Guo, Demi and Maas, Lukas M. and Qin, Wilson and Wasay, Abdul and Sun, Yiyou},
  year = {2018},
  journal = {IEEE Data Eng. Bull.},
  volume = {41},
  number = {3},
  pages = {64--75},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Tue, 10 Mar 2020 16:23:50 +0100}
}

@article{jarzabekLLregularGrammars1975,
  title = {{{LL-regular}} Grammars},
  author = {Jarzabek, Stanislaw and Krawczyk, Tomasz},
  year = {1975},
  month = nov,
  journal = {Information Processing Letters},
  volume = {4},
  number = {2},
  pages = {31--37},
  issn = {0020-0190},
  doi = {10.1016/0020-0190(75)90009-5},
  urldate = {2024-06-24},
  keywords = {LL() language,LLR grammar,LLR language,LLR production,LLR rule,LLS() grammar,regular partition,separating set},
  file = {/home/vipa/Zotero/storage/PYSXLZWB/Jarzabek and Krawczyk - 1975 - LL-regular grammars.pdf}
}

@misc{JavaCollectionsFramework,
  type = {Topic},
  title = {Java {{Collections Framework}}},
  journal = {Oracle Help Center},
  publisher = {April2024},
  urldate = {2024-05-01},
  abstract = {The Java platform includes a collections framework that provides developers with a unified architecture for representing and manipulating collections, enabling them to be manipulated independently of the details of their representation. A collection is an object that represents a group of objects (such as the classic ArrayList class).},
  howpublished = {https://docs.oracle.com/en/java/javase/22/core/java-collections-framework.html},
  langid = {american},
  file = {/home/vipa/Zotero/storage/XIEUI6HW/java-collections-framework.html}
}

@article{jimEfficientEarleyParsing2010,
  title = {Efficient {{Earley Parsing}} with {{Regular Right-hand Sides}}},
  author = {Jim, Trevor and Mandelbaum, Yitzhak},
  year = {2010},
  month = sep,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)},
  volume = {253},
  number = {7},
  pages = {135--148},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2010.08.037},
  urldate = {2019-10-29},
  abstract = {We present a new variant of the Earley parsing algorithm capable of efficiently supporting context-free grammars with regular right hand-sides. We present the core state-machine driven algorithm, the translation of grammars into state machines, and the reconstruction algorithm. We also include a theoretical framework for presenting the algorithm and for evaluating optimizations. Finally, we evaluate the algorithm by testing its implementation.},
  langid = {english},
  keywords = {augmented transition networks,Context-free grammars,Earley parsing,regular right sides,scannerless parsing,transducers},
  file = {/home/vipa/Zotero/storage/ZIWMLAC4/Jim and Mandelbaum - 2010 - Efficient Earley Parsing with Regular Right-hand S.pdf;/home/vipa/Zotero/storage/3IXX4VGV/S1571066110001167.html}
}

@inproceedings{jimNewMethodDependent2011,
  title = {A {{New Method}} for {{Dependent Parsing}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Jim, Trevor and Mandelbaum, Yitzhak},
  editor = {Barthe, Gilles},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {378--397},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-19718-5_20},
  abstract = {Dependent grammars extend context-free grammars by allowing semantic values to be bound to variables and used to constrain parsing. Dependent grammars can cleanly specify common features that cannot be handled by context-free grammars, such as length fields in data formats and significant indentation in programming languages. Few parser generators support dependent parsing, however. To address this shortcoming, we have developed a new method for implementing dependent parsers by extending existing parsing algorithms. Our method proposes a point-free language of dependent grammars, which we believe closely corresponds to existing context-free parsing algorithms, and gives a novel transformation from conventional dependent grammars to point-free ones.To validate our technique, we have specified the semantics of both source and target dependent grammar languages, and proven our transformation sound and complete with respect to those semantics. Furthermore, we have empirically validated the suitability of our point-free language by adapting four parsing engines to support it: an Earley parsing engine; a GLR parsing engine; memoizing, arrow-style parser combinators; and PEG parser combinators.},
  isbn = {978-3-642-19718-5},
  langid = {english},
  keywords = {Attribute Grammar,Empty String,Lambda Calculus,Semantic Action,Target Language},
  file = {/home/vipa/Zotero/storage/NLMLQJXF/Jim and Mandelbaum - 2011 - A New Method for Dependent Parsing.pdf}
}

@article{johannFusingLogicControl2001,
  title = {Fusing {{Logic}} and {{Control}} with {{Local Transformations}}: {{An Example Optimization}}},
  shorttitle = {Fusing {{Logic}} and {{Control}} with {{Local Transformations}}},
  author = {Johann, Patricia and Visser, Eelco},
  year = {2001},
  month = dec,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{WRS}} 2001, 1st {{International Workshop}} on {{Reduction Strategies}} in {{Rewriting}} and {{Programming}}},
  volume = {57},
  pages = {144--162},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00271-3},
  urldate = {2020-11-27},
  abstract = {programming supports the separation of logical concerns from issues of control in program construction. While this separation of concerns leads to reduced code size and increased reusability of code, its main disadvantage is the computational overhead it incurs. Fusion techniques can be used to combine the reusability of abstract programs with the efficiency of specialized programs. In this paper we illustrate some of the ways in which rewriting strategies can be used to separate the definition of program transformation rules from the strategies under which they are applied. Doing so supports the generic definition of program transformation components. Fusion techniques for strategies can then be used to specialize such generic components. We show how the generic innermost rewriting strategy can be optimized by fusing it with the rules to which it is applied. Both the optimization and the programs to which the optimization applies are specified in the strategy language Stratego. The optimization is based on small transformation rules that are applied locally under the control of strategies, using special knowledge about the contexts in which the rules are applied.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/WUHYD9RK/Johann and Visser - 2001 - Fusing Logic and Control with Local Transformation.pdf;/home/vipa/Zotero/storage/JH5DLQAE/S1571066104002713.html}
}

@inproceedings{jungBrainyEffectiveSelection2011,
  title = {Brainy: Effective Selection of Data Structures},
  shorttitle = {Brainy},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Jung, Changhee and Rus, Silvius and Railing, Brian P. and Clark, Nathan and Pande, Santosh},
  year = {2011},
  month = jun,
  series = {{{PLDI}} '11},
  pages = {86--97},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1993498.1993509},
  urldate = {2024-01-06},
  abstract = {Data structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input. This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecture-specific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several real-world applications with various program input sets on two different state-of-the-art microarchitectures. Consequently, Brainy achieved an average performance improvement of 27\% and 33\% on both microarchitectures, respectively.},
  isbn = {978-1-4503-0663-8},
  keywords = {application generator,data structure selection,performance counters,training framework},
  file = {/home/vipa/Zotero/storage/3SP74624/Jung et al. - 2011 - Brainy effective selection of data structures.pdf}
}

@inproceedings{kamdemkengneEfficientlyRewritingLarge2013,
  title = {Efficiently Rewriting Large Multimedia Application Execution Traces with Few Event Sequences},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Kamdem Kengne, Christiane and Fopa, Leon Constantin and Termier, Alexandre and Ibrahim, Noha and Rousset, Marie-Christine and Washio, Takashi and Santana, Miguel},
  year = {2013},
  month = aug,
  series = {{{KDD}} '13},
  pages = {1348--1356},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2487575.2488211},
  urldate = {2020-11-20},
  abstract = {The analysis of multimedia application traces can reveal important information to enhance program execution comprehension. However typical size of traces can be in gigabytes, which hinders their effective exploitation by application developers. In this paper, we study the problem of finding a set of sequences of events that allows a reduced-size rewriting of the original trace. These sequences of events, that we call blocks, can simplify the exploration of large execution traces by allowing application developers to see an abstraction instead of low-level events. The problem of computing such set of blocks is NP-hard and naive approaches lead to prohibitive running times that prevent analysing real world traces. We propose a novel algorithm that directly mines the set of blocks. Our experiments show that our algorithm can analyse real traces of up to two hours of video. We also show experimentally the quality of the set of blocks proposed, and the interest of the rewriting to understand actual trace data.},
  isbn = {978-1-4503-2174-7},
  keywords = {combinatorial optimization,execution traces,multimedia apllications,pattern mining},
  file = {/home/vipa/Zotero/storage/3Q9MLWVI/Kamdem Kengne et al. - 2013 - Efficiently rewriting large multimedia application.pdf}
}

@inproceedings{kaminskiModularWellDefinednessAnalysis2013,
  title = {Modular {{Well-Definedness Analysis}} for {{Attribute Grammars}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Kaminski, Ted and Van Wyk, Eric},
  editor = {Czarnecki, Krzysztof and Hedin, G{\"o}rel},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {352--371},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-36089-3_20},
  abstract = {We present a modular well-definedness analysis for attribute grammars. The global properties of completeness and non-circularity are ensured with checks on grammar modules that require only additional information from their dependencies. Local checks to ensure global properties are crucial for specifying extensible languages. They allow independent developers of language extensions to verify that their extension, when combined with other independently developed and similarly verified extensions to a specified host language, will result in a composed grammar that is well-defined. Thus, the composition of the host language and user-selected extensions can safely be performed by someone with no expertise in language design and implementation. The analysis is necessarily conservative and imposes some restrictions on the grammar. We argue that the analysis is practical and the restrictions are natural and not burdensome by applying it to the Silver specifications of Silver, our boot-strapped extensible attribute grammar system.},
  isbn = {978-3-642-36089-3},
  langid = {english},
  keywords = {Attribute Equation,Attribute Grammar,Concrete Syntax,Language Extension,Modular Analysis},
  file = {/home/vipa/Zotero/storage/NWACDCKZ/Kaminski and Van Wyk - 2013 - Modular Well-Definedness Analysis for Attribute Gr.pdf}
}

@article{kaminskiReliableAutomaticComposition2017,
  title = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}: {{The ableC Extensible Language Framework}}},
  shorttitle = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}},
  author = {Kaminski, Ted and Kramer, Lucas and Carlson, Travis and Van Wyk, Eric},
  year = {2017},
  month = oct,
  journal = {Proc. ACM Program. Lang.},
  volume = {1},
  number = {OOPSLA},
  pages = {98:1--98:29},
  issn = {2475-1421},
  doi = {10.1145/3138224},
  urldate = {2018-10-29},
  abstract = {This paper describes an extensible language framework, ableC, that allows programmers to import new, domain-specific, independently-developed language features into their programming language, in this case C. Most importantly, this framework ensures that the language extensions will automatically compose to form a working translator that does not terminate abnormally. This is possible due to two modular analyses that extension developers can apply to their language extension to check its composability. Specifically, these ensure that the composed concrete syntax specification is non-ambiguous and the composed attribute grammar specifying the semantics is well-defined. This assurance and the expressiveness of the supported extensions is a distinguishing characteristic of the approach.   The paper describes a number of techniques for specifying a host language, in this case C at the C11 standard, to make it more amenable to language extension. These include techniques that make additional extensions pass these modular analyses, refactorings of the host language to support a wider range of extensions, and the addition of semantic extension points to support, for example, operator overloading and non-local code transformations.},
  keywords = {attribute grammars,context-aware scanning,domain specific languages,extensible compiler frameworks,language composition},
  file = {/home/vipa/Zotero/storage/J5Q5UTZY/Kaminski et al. - 2017 - Reliable and Automatic Composition of Language Ext.pdf}
}

@article{kaminskiReliablyComposableLanguage2017,
  title = {Reliably Composable Language Extensions},
  author = {Kaminski, Ted},
  year = {2017},
  month = may,
  doi = {10.24926/2017.188954},
  urldate = {2018-10-18},
  abstract = {Many programming tasks are dramatically simpler when an appropriate domain-specific language can be used to accomplish them. These languages offer a variety of potential advantages, including programming at a higher level of abstraction, custom analyses specific to the problem domain, and the ability to generate very efficient code. But they also suffer many disadvantages as a result of their implementation techniques. Fully separate languages (such as YACC, or SQL) are quite flexible, but these are distinct monolithic entities and thus we are unable to draw on the features of several in combination to accomplish a single task. That is, we cannot compose their domain-specific features. "Embedded" DSLs (such as parsing combinators) accomplish something like a different language, but are actually implemented simply as libraries within a flexible host language. This approach allows different libraries to be imported and used together, enabling composition, but it is limited in analysis and translation capabilities by the host language they are embedded within. A promising combination of these two approaches is to allow a host language to be directly extended with new features (syntactic and semantic.) However, while there are plausible ways to attempt to compose language extensions, they can easily fail, making this approach unreliable. Previous methods of assuring reliable composition impose onerous restrictions, such as throwing out entirely the ability to introduce new analysis. This thesis introduces reliably composable language extensions as a technique for the implementation of DSLs. This technique preserves most of the advantages of both separate and "embedded" DSLs. Unlike many prior approaches to language extension, this technique ensures composition of multiple language extensions will succeed, and preserves strong properties about the behavior of the resulting composed compiler. We define an analysis on language extensions that guarantees the composition of several extensions will be well-defined, and we further define a set of testable properties that ensure the resulting compiler will behave as expected, along with a principle that assigns "blame" for bugs that may ultimately appear as a result of composition. Finally, to concretely compare our approach to our original goals for reliably composable language extension, we use these techniques to develop an extensible C compiler front-end, together with several example composable language extensions.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/AYLWF7HR/Kaminski - 2017 - Reliably composable language extensions.pdf;/home/vipa/Zotero/storage/YEY5CYUS/188954.html}
}

@inproceedings{karachaliasGADTsMeetTheir2015,
  title = {{{GADTs}} Meet Their Match: Pattern-Matching Warnings That Account for {{GADTs}}, Guards, and Laziness},
  shorttitle = {{{GADTs}} Meet Their Match},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton},
  year = {2015},
  month = aug,
  series = {{{ICFP}} 2015},
  pages = {424--436},
  publisher = {Association for Computing Machinery},
  address = {Vancouver, BC, Canada},
  doi = {10.1145/2784731.2784748},
  urldate = {2020-04-21},
  abstract = {For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.},
  isbn = {978-1-4503-3669-7},
  keywords = {Generalized Algebraic Data Types,Haskell,OutsideIn(X),pattern matching},
  file = {/home/vipa/Zotero/storage/8KZMQ2PU/Karachalias et al. - 2015 - GADTs meet their match pattern-matching warnings .pdf}
}

@inproceedings{katsPureDeclarativeSyntax2010,
  title = {Pure and {{Declarative Syntax Definition}}: {{Paradise Lost}} and {{Regained}}},
  shorttitle = {Pure and {{Declarative Syntax Definition}}},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Kats, Lennart C.L. and Visser, Eelco and Wachsmuth, Guido},
  year = {2010},
  series = {{{OOPSLA}} '10},
  pages = {918--932},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1869459.1869535},
  urldate = {2019-07-04},
  abstract = {Syntax definitions are pervasive in modern software systems, and serve as the basis for language processing tools like parsers and compilers. Mainstream parser generators pose restrictions on syntax definitions that follow from their implementation algorithm. They hamper evolution, maintainability, and compositionality of syntax definitions. The pureness and declarativity of syntax definitions is lost. We analyze how these problems arise for different aspects of syntax definitions, discuss their consequences for language engineers, and show how the pure and declarative nature of syntax definitions can be regained.},
  isbn = {978-1-4503-0203-6},
  keywords = {declarative,grammars,grammarware,parsers,sdf,sglr,syntax definition},
  file = {/home/vipa/Zotero/storage/S7PDXPE4/Kats et al. - 2010 - Pure and Declarative Syntax Definition Paradise L.pdf}
}

@inproceedings{katsSpoofaxLanguageWorkbench2010,
  title = {The {{Spoofax Language Workbench}}: {{Rules}} for {{Declarative Specification}} of {{Languages}} and {{IDEs}}},
  shorttitle = {The {{Spoofax Language Workbench}}},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Kats, Lennart C.L. and Visser, Eelco},
  year = {2010},
  series = {{{OOPSLA}} '10},
  pages = {444--463},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1869459.1869497},
  urldate = {2019-10-25},
  abstract = {Spoofax is a language workbench for efficient, agile development of textual domain-specific languages with state-of-the-art IDE support. Spoofax integrates language processing techniques for parser generation, meta-programming, and IDE development into a single environment. It uses concise, declarative specifications for languages and IDE services. In this paper we describe the architecture of Spoofax and introduce idioms for high-level specifications of language semantics using rewrite rules, showing how analyses can be reused for transformations, code generation, and editor services such as error marking, reference resolving, and content completion. The implementation of these services is supported by language-parametric editor service classes that can be dynamically loaded by the Eclipse IDE, allowing new languages to be developed and used side-by-side in the same Eclipse environment.},
  isbn = {978-1-4503-0203-6},
  keywords = {domain-specific language,dsl,eclipse,IDE,language workbench,meta-tooling,sdf,sglr,spoofax,stratego},
  file = {/home/vipa/Zotero/storage/9BPPXULR/Kats and Visser - 2010 - The Spoofax Language Workbench Rules for Declarat.pdf}
}

@inproceedings{keOptimusDynamicRewriting2013,
  title = {Optimus: A Dynamic Rewriting Framework for Data-Parallel Execution Plans},
  shorttitle = {Optimus},
  booktitle = {Proceedings of the 8th {{ACM European Conference}} on {{Computer Systems}}},
  author = {Ke, Qifa and Isard, Michael and Yu, Yuan},
  year = {2013},
  month = apr,
  series = {{{EuroSys}} '13},
  pages = {15--28},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2465351.2465354},
  urldate = {2020-11-20},
  abstract = {In distributed data-parallel computing, a user program is compiled into an execution plan graph (EPG), typically a directed acyclic graph. This EPG is the core data structure used by modern distributed execution engines for task distribution, job management, and fault tolerance. Once submitted for execution, the EPG remains largely unchanged at runtime except for some limited modifications. This makes it difficult to employ dynamic optimization techniques that could substantially improve the distributed execution based on runtime information. This paper presents Optimus, a framework for dynamically rewriting an EPG at runtime. Optimus extends dynamic rewrite mechanisms present in systems such as Dryad and CIEL by integrating rewrite policy with a high-level data-parallel language, in this case DryadLINQ. This integration enables optimizations that require knowledge of the semantics of the computation, such as language customizations for domain-specific computations including matrix algebra. We describe the design and implementation of Optimus, outline its interfaces, and detail a number of rewriting techniques that address problems arising in distributed execution including data skew, dynamic data re-partitioning, handling unbounded iterative computations, and protecting important intermediate data for fault tolerance. We evaluate Optimus with real applications and data and show significant performance gains compared to manual optimization or customized systems. We demonstrate the versatility of dynamic EPG rewriting for data-parallel computing, and argue that it is an essential feature of any general-purpose distributed dataflow execution engine.},
  isbn = {978-1-4503-1994-2},
  file = {/home/vipa/Zotero/storage/HX5MDVFX/Ke et al. - 2013 - Optimus a dynamic rewriting framework for data-pa.pdf}
}

@inproceedings{kerschbaumAutomaticallyOptimizingSecure2011,
  title = {Automatically Optimizing Secure Computation},
  booktitle = {Proceedings of the 18th {{ACM}} Conference on {{Computer}} and Communications Security},
  author = {Kerschbaum, Florian},
  year = {2011},
  month = oct,
  series = {{{CCS}} '11},
  pages = {703--714},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2046707.2046786},
  urldate = {2020-11-20},
  abstract = {On the one hand, compilers for secure computation protocols, such as FairPlay or FairPlayMP, have significantly simplified the development of such protocols. On the other hand, optimized protocols with high performance for special problems demand manual development and security verification. The question considered in this paper is: Can we construct a compiler that produces optimized protocols? We present an optimization technique based on logic inference about what is known from input and output. Using the example of median computation we can show that our program analysis and rewriting technique translates a FairPlay program into an equivalent -- in functionality and security -- program that corresponds to the protocol by Aggarwal et al. Nevertheless our technique is general and can be applied to optimize a wide variety of secure computation protocols.},
  isbn = {978-1-4503-0948-6},
  keywords = {optimization,programming,secure two-party computation},
  file = {/home/vipa/Zotero/storage/WVX9EN7M/Kerschbaum - 2011 - Automatically optimizing secure computation.pdf}
}

@inproceedings{kerschbaumExpressionRewritingOptimizing2013,
  title = {Expression Rewriting for Optimizing Secure Computation},
  booktitle = {Proceedings of the Third {{ACM}} Conference on {{Data}} and Application Security and Privacy},
  author = {Kerschbaum, Florian},
  year = {2013},
  month = feb,
  series = {{{CODASPY}} '13},
  pages = {49--58},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2435349.2435356},
  urldate = {2020-11-20},
  abstract = {In theory secure computation offers a solution for privacy in many collaborative applications. However, in practice poor efficiency of the protocols prevents their use. Hand-crafted protocols are more efficient than those implemented in compilers, but they require significantly more development effort in programming and verification. Recently, Kerschbaum introduced an automatic compiler optimization technique for secure computations that can make compilers as efficient as hand-crafted protocols. This optimization relies on the structure of the secure computation program. The programmer has to implement the program in such a way, such that the optimization can yield the optimal performance. In this paper we present an algorithm that rewrites the program -- most notably its expressions -- optimizing their efficiency in secure computation protocols. We give a heuristic for whole-program optimization and show the resulting performance gains using examples from the literature.},
  isbn = {978-1-4503-1890-7},
  keywords = {optimization,programming,secure two-party computation},
  file = {/home/vipa/Zotero/storage/27JVHVKL/Kerschbaum - 2013 - Expression rewriting for optimizing secure computa.pdf}
}

@misc{khorramElizabethHolmesDenies2021,
  title = {Elizabeth {{Holmes}} Denies Destroying Evidence in {{Theranos}} Case},
  author = {Khorram, Yasmin},
  year = {2021},
  month = feb,
  journal = {CNBC},
  urldate = {2023-03-17},
  abstract = {Prosecutors allege executives at Theranos destroyed a database which contained three years worth of accuracy and failure rates.},
  howpublished = {https://www.cnbc.com/2021/02/24/elizabeth-holmes-denies-destroying-evidence-in-theranos-case.html},
  langid = {english}
}

@misc{kiselyovEfficientInsightfulGeneralization2013,
  title = {Efficient and {{Insightful Generalization}}},
  author = {Kiselyov, Oleg},
  year = {2013},
  urldate = {2023-10-31},
  howpublished = {https://okmij.org/ftp/ML/generalization.html},
  file = {/home/vipa/Zotero/storage/CEALTB4I/generalization.html}
}

@article{kiselyovLightweightStaticCapabilities2007,
  title = {Lightweight {{Static Capabilities}}},
  author = {Kiselyov, Oleg and Shan, Chung-chieh},
  year = {2007},
  month = jun,
  journal = {Electronic Notes in Theoretical Computer Science (ENTCS)},
  volume = {174},
  number = {7},
  pages = {79--104},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2006.10.039},
  urldate = {2022-08-24},
  abstract = {We describe a modular programming style that harnesses modern type systems to verify safety conditions in practical systems. This style has three ingredients:(i)A compact kernel of trust that is specific to the problem domain. (ii)Unique names (capabilities) that confer rights and certify properties, so as to extend the trust from the kernel to the rest of the application. (iii)Static (type) proxies for dynamic values. We illustrate our approach using examples from the dependent-type literature, but our programs are written in Haskell and OCaml today, so our techniques are compatible with imperative code, native mutable arrays, and general recursion. The three ingredients of this programming style call for (1) an expressive core language, (2) higher-rank polymorphism, and (3) phantom types.},
  keywords = {Modular programming,safety property,static types,verification},
  file = {/home/vipa/Zotero/storage/A6BXFKQC/Kiselyov and Shan - 2007 - Lightweight Static Capabilities.pdf}
}

@incollection{kitchinOrc2009,
  title = {The {{Orc}} Programming Language},
  booktitle = {Formal Techniques for {{Distributed Systems}}},
  author = {Kitchin, David and Quark, Adrian and Cook, William and Misra, Jayadev},
  year = {2009},
  pages = {1--25},
  publisher = {Springer}
}

@inproceedings{klintRASCALDomainSpecific2009,
  title = {{{RASCAL}}: {{A Domain Specific Language}} for {{Source Code Analysis}} and {{Manipulation}}},
  shorttitle = {{{RASCAL}}},
  booktitle = {2009 {{Ninth IEEE International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  year = {2009},
  month = sep,
  pages = {168--177},
  issn = {null},
  doi = {10.1109/SCAM.2009.28},
  abstract = {Many automated software engineering tools require tight integration of techniques for source code analysis and manipulation. State-of-the-art tools exist for both, but the domains have remained notoriously separate because different computational paradigms fit each domain best. This impedance mismatch hampers the development of new solutions because the desired functionality and scalability can only be achieved by repeated and ad hoc integration of different techniques. RASCAL is a domain-specific language that takes away most of this boilerplate by integrating source code analysis and manipulation at the conceptual, syntactic, semantic and technical level. We give an overview of the language and assess its merits by implementing a complex refactoring.},
  keywords = {ad hoc integration,automated software engineering tool,complex software refactoring,conceptual-syntactic-semantic-technical level,domain specific language,Domain specific languages,Impedance,impedance mismatch,Informatics,Java,Libraries,Logic programming,meta-programming,object-oriented languages,Pattern matching,program diagnostics,RASCAL,Scalability,Software engineering,software maintenance,source code analysis,source code manipulation,Storms,transformation},
  file = {/home/vipa/Zotero/storage/6YINBL9U/Klint et al. - 2009 - RASCAL A Domain Specific Language for Source Code.pdf;/home/vipa/Zotero/storage/5Z5UHPEW/5279910.html}
}

@inproceedings{klintUsingFiltersDisambiguation1994,
  title = {Using Filters for the Disambiguation of Context-Free Grammars},
  booktitle = {Proceedings of the {{ASMICS}} Workshop on Parsing Theory},
  author = {Klint, Paul and Visser, Eelco},
  year = {1994},
  month = oct,
  publisher = {Tech. Rep. 126--1994, Dipartimento di Scienze dell'Informazione, Universit{\`a} di Milano},
  address = {Milano, Italy},
  citedby = {0},
  cites = {0},
  researchr = {https://researchr.org/publication/KlintV94},
  tags = {case study, disambiguation, context-aware, parsing, grammar, domain-specific language},
  file = {/home/vipa/Zotero/storage/CU7RB7JZ/Klint and Visser - 1994 - Using filters for the disambiguation of context-fr.pdf}
}

@article{klonatosBuildingEfficientQuery2014,
  title = {Building Efficient Query Engines in a High-Level Language},
  author = {Klonatos, Yannis and Koch, Christoph and Rompf, Tiark and Chafi, Hassan},
  year = {2014},
  month = jun,
  journal = {Proceedings of the VLDB Endowment},
  volume = {7},
  number = {10},
  pages = {853--864},
  issn = {2150-8097},
  doi = {10.14778/2732951.2732959},
  urldate = {2024-06-04},
  abstract = {In this paper we advocate that it is time for a radical rethinking of database systems design. Developers should be able to leverage high-level programming languages without having to pay a price in efficiency. To realize our vision of abstraction without regret, we present LegoBase, a query engine written in the high-level programming language Scala. The key technique to regain efficiency is to apply generative programming: the Scala code that constitutes the query engine, despite its high-level appearance, is actually a program generator that emits specialized, low-level C code. We show how the combination of high-level and generative programming allows to easily implement a wide spectrum of optimizations that are difficult to achieve with existing low-level query compilers, and how it can continuously optimize the query engine. We evaluate our approach with the TPC-H benchmark and show that: (a) with all optimizations enabled, our architecture significantly outperforms a commercial in-memory database system as well as an existing query compiler, (b) these performance improvements require programming just a few hundred lines of high-level code instead of complicated low-level code that is required by existing query compilers and, finally, that (c) the compilation overhead is low compared to the overall execution time, thus making our approach usable in practice for efficiently compiling query engines.},
  file = {/home/vipa/Zotero/storage/46UJEARM/Klonatos et al. - 2014 - Building efficient query engines in a high-level l.pdf}
}

@article{knuthSemanticsContextfreeLanguages1968,
  title = {Semantics of Context-Free Languages},
  author = {Knuth, Donald E.},
  year = {1968},
  month = jun,
  journal = {Mathematical systems theory},
  volume = {2},
  number = {2},
  pages = {127--145},
  issn = {1433-0490},
  doi = {10.1007/BF01692511},
  urldate = {2024-07-29},
  abstract = {``Meaning'' may be assigned to a string in a context-free language by defining ``attributes'' of the symbols in a derivation tree for that string. The attributes can be defined by functions associated with each production in the grammar. This paper examines the implications of this process when some of the attributes are ``synthesized'', i.e., defined solely in terms of attributes of thedescendants of the corresponding nonterminal symbol, while other attributes are ``inherited'', i.e., defined in terms of attributes of theancestors of the nonterminal symbol. An algorithm is given which detects when such semantic rules could possibly lead to circular definition of some attributes. An example is given of a simple programming language defined with both inherited and synthesized attributes, and the method of definition is compared to other techniques for formal specification of semantics which have appeared in the literature.},
  langid = {english},
  keywords = {Computational Mathematic,Derivation Tree,Formal Specification,Programming Language,Simple Programming},
  file = {/home/vipa/Zotero/storage/C6P4Z8I9/Knuth - 1968 - Semantics of context-free languages.pdf}
}

@article{knuthTranslationLanguagesLeft1965,
  title = {On the Translation of Languages from Left to Right},
  author = {Knuth, Donald E.},
  year = {1965},
  month = dec,
  journal = {Information and Control},
  volume = {8},
  number = {6},
  pages = {607--639},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(65)90426-2},
  urldate = {2024-04-29},
  abstract = {There has been much recent interest in languages whose grammar is sufficiently simple that an efficient left-to-right parsing algorithm can be mechanically produced from the grammar. In this paper, we define LR(k) grammars, which are perhaps the most general ones of this type, and they provide the basis for understanding all of the special tricks which have been used in the construction of parsing algorithms for languages with simple structure, e.g. algebraic languages. We give algorithms for deciding if a given grammar satisfies the LR(k) condition, for given k, and also give methods for generating recognizes for LR(k) grammars. It is shown that the problem of whether or not a grammar is LR(k) for some k is undecidable, and the paper concludes by establishing various connections between LR(k) grammars and deterministic languages. In particular, the LR(k) condition is a natural analogue, for grammars, of the deterministic condition, for languages.},
  file = {/home/vipa/Zotero/storage/QVXABNVZ/Knuth - 1965 - On the translation of languages from left to right.pdf;/home/vipa/Zotero/storage/NUZRFGXS/S0019995865904262.html}
}

@inproceedings{kohlbeckerHygienicMacroExpansion1986,
  title = {Hygienic Macro Expansion},
  booktitle = {Proceedings of the 1986 {{ACM}} Conference on {{LISP}} and Functional Programming},
  author = {Kohlbecker, Eugene and Friedman, Daniel P. and Felleisen, Matthias and Duba, Bruce},
  year = {1986},
  month = aug,
  series = {{{LFP}} '86},
  pages = {151--161},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/319838.319859},
  urldate = {2024-05-06},
  isbn = {978-0-89791-200-6},
  file = {/home/vipa/Zotero/storage/24NLEF6T/Kohlbecker et al. - 1986 - Hygienic macro expansion.pdf}
}

@inproceedings{koliaiQuantifyingPerformanceBottleneck2013,
  title = {Quantifying Performance Bottleneck Cost through Differential Analysis},
  booktitle = {Proceedings of the 27th International {{ACM}} Conference on {{International}} Conference on Supercomputing},
  author = {Kolia{\"i}, Souad and Bendifallah, Zakaria and Tribalat, Mathieu and Valensi, C{\'e}dric and Acquaviva, Jean-Thomas and Jalby, William},
  year = {2013},
  month = jun,
  series = {{{ICS}} '13},
  pages = {263--272},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2464996.2465440},
  urldate = {2020-11-20},
  abstract = {Accurate performance analysis is critical for understanding application efficiency and then driving software or hardware optimizations. Although most of static and dynamic performance analysis tools provide useful information, they are not completely satisfactory. Static performance analysis does not provide an accurate view due to the lack of runtime information (eg: cache behavior). On the other hand, profilers, generally mixed with hardware counters, provide a wide range of performance metrics but lack the ability to correlate performance informations with the appropriate code fragment, data structure or instruction. Finally, cycle accurate simulators are too complex and too costly to be used routinely for optimization of real life applications. This paper presents the Differential Analysis method, an approach designed for simple and automatic detection of performance bottlenecks. This approach relies on DECAN, a tool which generates different binary variants obtained by patching individual or groups of instructions. The different variants are then measured and compared, allowing to evaluate the cost of an instruction group and therefore its optimization potential benefit. Differential analysis is illustrated by the use of DECAN on a range of HPC applications to detect performance bottlenecks.},
  isbn = {978-1-4503-2130-3},
  keywords = {binary rewriting,bottleneck detection,differential analysis,performance evaluation},
  file = {/home/vipa/Zotero/storage/9Q8ETAK2/Koliaï et al. - 2013 - Quantifying performance bottleneck cost through di.pdf}
}

@article{krawiecProvablyCorrectAsymptotically2022,
  title = {Provably Correct, Asymptotically Efficient, Higher-Order Reverse-Mode Automatic Differentiation},
  author = {Krawiec, Faustyna and Peyton Jones, Simon and Krishnaswami, Neel and Ellis, Tom and Eisenberg, Richard A. and Fitzgibbon, Andrew},
  year = {2022},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {POPL},
  pages = {1--30},
  issn = {2475-1421},
  doi = {10.1145/3498710},
  urldate = {2023-02-09},
  abstract = {In this paper, we give a simple and efficient implementation of reverse-mode automatic differentiation, which both extends easily to higher-order functions, and has run time and memory consumption linear in the run time of the original program. In addition to a formal description of the translation, we also describe an implementation of this algorithm, and prove its correctness by means of a logical relations argument.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/CXYRBUPB/Krawiec et al. - 2022 - Provably correct, asymptotically efficient, higher.pdf}
}

@misc{kusumAdaptingGraphApplication2014,
  title = {Adapting {{Graph Application Performance}} via {{Alternate Data Structure Representation}}},
  author = {Kusum, Amlan and Neamtiu, Iulian and Gupta, Rajiv},
  year = {2014},
  month = dec,
  number = {arXiv:1412.8120},
  eprint = {1412.8120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.8120},
  urldate = {2024-01-06},
  abstract = {Graph processing is used extensively in areas from social networking mining to web indexing. We demonstrate that the performance and dependability of such applications critically hinges on the graph data structure used, because a fixed, compile-time choice of data structure can lead to poor performance or applications unable to complete. To address this problem, we introduce an approach that helps programmers transform regular, off-the-shelf graph applications into adaptive, more dependable applications where adaptations are performed via runtime selection from alternate data structure representations. Using our approach, applications dynamically adapt to the input graph's characteristics and changes in available memory so they continue to run when faced with adverse conditions such as low memory. Experiments with graph algorithms on real-world (e.g., Wikipedia metadata, Gnutella topology) and synthetic graph datasets show that our adaptive applications run to completion with lower execution time and/or memory utilization in comparison to their non-adaptive versions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/home/vipa/Zotero/storage/4XD8UP3Q/Kusum et al. - 2014 - Adapting Graph Application Performance via Alterna.pdf;/home/vipa/Zotero/storage/J2KQF57Q/1412.html}
}

@inproceedings{kusumSafeFlexibleAdaptation2016,
  title = {Safe and Flexible Adaptation via Alternate Data Structure Representations},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Compiler Construction}}},
  author = {Kusum, Amlan and Neamtiu, Iulian and Gupta, Rajiv},
  year = {2016},
  month = mar,
  series = {{{CC}} 2016},
  pages = {34--44},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2892208.2892220},
  urldate = {2024-01-06},
  abstract = {The choice of data structures is crucial for achieving high performance. For applications that are long-running and/or operate on large data sets, the best choice for main data structures can change multiple times over the course of a single execution. For example, in a graph-processing application where the graph evolves over time, the best data structure for representing the graph may change as the program executes. Similarly, in a database or a key-value store application, with changes in relative frequencies of different types of queries over time, the most efficient data structure changes as well. We introduce an approach that allows applications to adapt to current conditions (input characteristics, operations on data, state) by switching their data structures on-the-fly with little overhead and without the developer worrying about safety or specifying adaptation points (this is handled by our compiler infrastructure). We use our approach on different classes of problems that are compute- and memory-intensive: graph algorithms, database indexing, and two real-world applications, the Memcached object cache and the Space Tyrant online game server. Our results show that off-the-shelf applications can be transformed into adaptive applications with modest programmer effort; that the adaptive versions outperform the original, fixed-representation versions; and that adaptation can be performed on-the-fly safely and with very little runtime overhead.},
  isbn = {978-1-4503-4241-4},
  keywords = {adaptation,input characteristics,runtime data structure selection,space-time trade-off},
  file = {/home/vipa/Zotero/storage/5HGCC3ZB/Kusum et al. - 2016 - Safe and flexible adaptation via alternate data st.pdf}
}

@inproceedings{lammichAutomaticDataRefinement2013,
  title = {Automatic {{Data Refinement}}},
  booktitle = {Interactive {{Theorem Proving}}},
  author = {Lammich, Peter},
  editor = {Blazy, Sandrine and {Paulin-Mohring}, Christine and Pichardie, David},
  year = {2013},
  pages = {84--99},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-39634-2_9},
  abstract = {We present the Autoref tool for Isabelle/HOL, which automatically refines algorithms specified over abstract concepts like maps and sets to algorithms over concrete implementations like red-black-trees, and produces a refinement theorem. It is based on ideas borrowed from relational parametricity due to Reynolds and Wadler.},
  isbn = {978-3-642-39634-2},
  langid = {english},
  keywords = {Automatic Data,Executable Code,Side Condition,Synthesis Problem,Type Constructor},
  file = {/home/vipa/Zotero/storage/DFDABYBJ/Lammich - 2013 - Automatic Data Refinement.pdf}
}

@inproceedings{langDeterministicTechniquesEfficient1974,
  ids = {langDeterministicTechniquesEfficient1974a},
  title = {Deterministic {{Techniques}} for {{Efficient Non-Deterministic Parsers}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Lang, Bernard},
  editor = {Loeckx, Jacques},
  year = {1974},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {255--269},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-21545-6_18},
  abstract = {A general study of parallel non-deterministic parsing and translation {\`a} la Earley is developped formally, based on non-deterministic pushdown acceptor-transducers. Several results (camplexity and efficiency) are established, same new and other previously proved only in special cases. As an application, we show that for every family of deterministic context-free pushdown parsers (e.g. precedence, LR(k), LL(k), ...) there is a family of general context-free parallel parsers that have the same efficiency in most practical cases (e.g. analysis of programming languages).},
  isbn = {978-3-662-21545-6},
  langid = {english},
  file = {/home/vipa/Zotero/storage/SJL9D9NZ/Lang - 1974 - Deterministic Techniques for Efficient Non-Determi.pdf;/home/vipa/Zotero/storage/WVUWCYD9/Lang - 1974 - Deterministic Techniques for Efficient Non-Determi.pdf}
}

@techreport{lattnerDataStructureAnalysis2003,
  type = {Tech. {{Report}}},
  title = {Data {{Structure Analysis}}: {{A Fast}} and {{Scalable Context-Sensitive Heap Analysis}}},
  author = {Lattner, Chris and Adve, Vikram},
  year = {2003},
  month = apr,
  number = {UIUCDCS-R-2003-2340},
  institution = {Computer Science Dept., Univ. of Illinois at Urbana-Champaign},
  abstract = {This paper describes a scalable heap analysis algorithm, Data Structure Analysis, designed to enable analyses and transformations of programs at the level of entire logical data structures. Data Structure Analysis attempts to identify disjoint instances of logical program data structures and their internal and external connectivity properties (without trying to categorize their ``shape''). To achieve this, Data Structure Analysis is fully context-sensitive (in the sense that it names memory objects by entire acyclic call paths), is fieldsensitive, builds an explicit model of the heap, and is robust enough to handle the full generality of C.},
  file = {/home/vipa/Zotero/storage/AJRZG6FN/Lattner and Adve - Data Structure Analysis A Fast and Scalable Conte.pdf}
}

@inproceedings{leijenExtensibleRecordsScoped2005,
  title = {Extensible Records with Scoped Labels},
  booktitle = {Proceedings of the 2005 {{Symposium}} on {{Trends}} in {{Functional Programming}} ({{TFP}}'05), {{Tallin}}, {{Estonia}}},
  author = {Leijen, Daan},
  year = {2005},
  month = sep,
  urldate = {2024-07-04},
  abstract = {Records provide a safe and flexible way to construct data structures. We describe a natural approach to typing polymorphic and extensible records that is simple, easy to use in practice, and straightforward to implement. A novel aspect of this work is that records can contain duplicate labels, effectively introducing a form of scoping over the [{\dots}]},
  langid = {american},
  file = {/home/vipa/Zotero/storage/93KP7I2U/Leijen - 2005 - Extensible records with scoped labels.pdf}
}

@inproceedings{leinoDafnyAutomaticProgram2010,
  title = {Dafny: {{An Automatic Program Verifier}} for {{Functional Correctness}}},
  shorttitle = {Dafny},
  booktitle = {Logic for {{Programming}}, {{Artificial Intelligence}}, and {{Reasoning}}},
  author = {Leino, K. Rustan M.},
  editor = {Clarke, Edmund M. and Voronkov, Andrei},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {348--370},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-17511-4_20},
  abstract = {Traditionally, the full verification of a program's functional correctness has been obtained with pen and paper or with interactive proof assistants, whereas only reduced verification tasks, such as extended static checking, have enjoyed the automation offered by satisfiability-modulo-theories (SMT) solvers. More recently, powerful SMT solvers and well-designed program verifiers are starting to break that tradition, thus reducing the effort involved in doing full verification.},
  isbn = {978-3-642-17511-4},
  langid = {english},
  keywords = {Automatic Program,Call Graph,Java Modeling Language,Proof Obligation,Separation Logic},
  file = {/home/vipa/Zotero/storage/AXTV5N65/Leino - 2010 - Dafny An Automatic Program Verifier for Functiona.pdf}
}

@article{leoGeneralContextfreeParsing1991,
  title = {A General Context-Free Parsing Algorithm Running in Linear Time on Every {{LR}}(k) Grammar without Using Lookahead},
  author = {Leo, Joop M. I. M.},
  year = {1991},
  month = may,
  journal = {Theoretical Computer Science},
  volume = {82},
  number = {1},
  pages = {165--176},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(91)90180-A},
  urldate = {2018-10-18},
  abstract = {A new general context-free parsing algorithm is presented which runs in linear time and space on every LR(k) grammar without using any lookahead and without making use of the LR property. Most of the existing implementations of tabular parsing algorithms, including those using lookahead, can easily be adapted to this new algorithm without a noteworthy loss of efficiency. For some natural right recursive grammars both the time and space complexity will be improved from {\textohm}(n2) to O(n). This makes this algorithm not only of theoretical but probably of practical interest as well.},
  file = {/home/vipa/Zotero/storage/38QMSIME/Leo - 1991 - A general context-free parsing algorithm running i.pdf;/home/vipa/Zotero/storage/TZZXSRUT/030439759190180A.html}
}

@techreport{leroyOCamlSystemRelease2018,
  type = {Report},
  title = {The {{OCaml}} System Release 4.07: {{Documentation}} and User's Manual},
  shorttitle = {The {{OCaml}} System Release 4.07},
  author = {Leroy, Xavier and Doligez, Damien and Frisch, Alain and Garrigue, Jacques and R{\'e}my, Didier and Vouillon, J{\'e}r{\^o}me},
  year = {2018},
  month = jul,
  urldate = {2019-09-30},
  abstract = {This manual documents the release 4.07 of the OCaml system. It is organized as follows. Part I, "An introduction to OCaml", gives an overview of the language. Part II, "The OCaml language", is the reference description of the language. Part III, "The OCaml tools", documents the compilers, toplevel system, and programming utilities. Part IV, "The OCaml library", describes the modules provided in the standard library.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/EYVAA7FR/Leroy et al. - 2018 - The OCaml system release 4.07 Documentation and u.pdf;/home/vipa/Zotero/storage/3CKP5URF/hal-00930213v5.html}
}

@article{lewisSyntaxDirectedTransduction1968,
  title = {Syntax-{{Directed Transduction}}},
  author = {Lewis, P. M. and Stearns, R. E.},
  year = {1968},
  month = jul,
  journal = {J. ACM},
  volume = {15},
  number = {3},
  pages = {465--488},
  issn = {0004-5411},
  doi = {10.1145/321466.321477},
  urldate = {2024-06-24},
  abstract = {A transduction is a mapping from one set of sequences to another. A syntax-directed transduction is a particular type of transduction which is defined on the grammar of a context-free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. Special consideration is given to machines called translators which both transduce and recognize. In particular, some special conditions are investigated under which syntax-directed translations can be performed on (deterministic) pushdown machines. In addition, some time bounds for translations on Turing machines are derived.},
  file = {/home/vipa/Zotero/storage/JDDNUBFI/Lewis and Stearns - 1968 - Syntax-Directed Transduction.pdf}
}

@inproceedings{LLVM:CGO04,
  title = {{{LLVM}}: {{A}} Compilation Framework for Lifelong Program Analysis and Transformation},
  booktitle = {{{CGO}}},
  author = {Lattner, Chris and Adve, Vikram},
  year = {2004},
  month = mar,
  pages = {75--88},
  address = {San Jose, CA, USA}
}

@article{loncaricFastSynthesisFast2016,
  title = {Fast Synthesis of Fast Collections},
  author = {Loncaric, Calvin and Torlak, Emina and Ernst, Michael D.},
  year = {2016},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {51},
  number = {6},
  pages = {355--368},
  issn = {0362-1340},
  doi = {10.1145/2980983.2908122},
  urldate = {2024-01-06},
  abstract = {Many applications require specialized data structures not found in the standard libraries, but implementing new data structures by hand is tedious and error-prone. This paper presents a novel approach for synthesizing efficient implementations of complex collection data structures from high-level specifications that describe the desired retrieval operations. Our approach handles a wider range of data structures than previous work, including structures that maintain an order among their elements or have complex retrieval methods. We have prototyped our approach in a data structure synthesizer called Cozy. Four large, real-world case studies compare structures generated by Cozy against handwritten implementations in terms of correctness and performance. Structures synthesized by Cozy match the performance of handwritten data structures while avoiding human error.},
  keywords = {Data structure synthesis},
  file = {/home/vipa/Zotero/storage/DLFZB6RQ/Loncaric et al. - 2016 - Fast synthesis of fast collections.pdf}
}

@inproceedings{loncaricGeneralizedDataStructure2018,
  title = {Generalized Data Structure Synthesis},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}},
  author = {Loncaric, Calvin and Ernst, Michael D. and Torlak, Emina},
  year = {2018},
  month = may,
  series = {{{ICSE}} '18},
  pages = {958--968},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3180155.3180211},
  urldate = {2024-01-06},
  abstract = {Data structure synthesis is the task of generating data structure implementations from high-level specifications. Recent work in this area has shown potential to save programmer time and reduce the risk of defects. Existing techniques focus on data structures for manipulating subsets of a single collection, but real-world programs often track multiple related collections and aggregate properties such as sums, counts, minimums, and maximums. This paper shows how to synthesize data structures that track subsets and aggregations of multiple related collections. Our technique decomposes the synthesis task into alternating steps of query synthesis and incrementalization. The query synthesis step implements pure operations over the data structure state by leveraging existing enumerative synthesis techniques, specialized to the data structures domain. The incrementalization step implements imperative state modifications by re-framing them as fresh queries that determine what to change, coupled with a small amount of code to apply the change. As an added benefit of this approach over previous work, the synthesized data structure is optimized for not only the queries in the specification but also the required update operations. We have evaluated our approach in four large case studies, demonstrating that these extensions are broadly applicable.},
  isbn = {978-1-4503-5638-1},
  keywords = {automatic programming,data structures,program synthesis},
  file = {/home/vipa/Zotero/storage/VP3SDXW6/Loncaric et al. - 2018 - Generalized data structure synthesis.pdf}
}

@inproceedings{lorchArmadaLoweffortVerification2020,
  title = {Armada: Low-Effort Verification of High-Performance Concurrent Programs},
  shorttitle = {Armada},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Lorch, Jacob R. and Chen, Yixuan and Kapritsos, Manos and Parno, Bryan and Qadeer, Shaz and Sharma, Upamanyu and Wilcox, James R. and Zhao, Xueyuan},
  year = {2020},
  month = jun,
  series = {{{PLDI}} 2020},
  pages = {197--210},
  publisher = {Association for Computing Machinery},
  address = {London, UK},
  doi = {10.1145/3385412.3385971},
  urldate = {2020-06-08},
  abstract = {Safely writing high-performance concurrent programs is notoriously difficult. To aid developers, we introduce Armada, a language and tool designed to formally verify such programs with relatively little effort. Via a C-like language and a small-step, state-machine-based semantics, Armada gives developers the flexibility to choose arbitrary memory layout and synchronization primitives so they are never constrained in their pursuit of performance. To reduce developer effort, Armada leverages SMT-powered automation and a library of powerful reasoning techniques, including rely-guarantee, TSO elimination, reduction, and alias analysis. All these techniques are proven sound, and Armada can be soundly extended with additional strategies over time. Using Armada, we verify four concurrent case studies and show that we can achieve performance equivalent to that of unverified code.},
  isbn = {978-1-4503-7613-6},
  keywords = {refinement,weak memory models,x86-TSO}
}

@inproceedings{lorenzenSoundTypedependentSyntactic2016,
  title = {Sound {{Type-dependent Syntactic Language Extension}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lorenzen, Florian and Erdweg, Sebastian},
  year = {2016},
  series = {{{POPL}} '16},
  pages = {204--216},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2837614.2837644},
  urldate = {2018-10-18},
  abstract = {Syntactic language extensions can introduce new facilities into a programming language while requiring little implementation effort and modest changes to the compiler. It is typical to desugar language extensions in a distinguished compiler phase after parsing or type checking, not affecting any of the later compiler phases. If desugaring happens before type checking, the desugaring cannot depend on typing information and type errors are reported in terms of the generated code. If desugaring happens after type checking, the code generated by the desugaring is not type checked and may introduce vulnerabilities. Both options are undesirable. We propose a system for syntactic extensibility where desugaring happens after type checking and desugarings are guaranteed to only generate well-typed code. A major novelty of our work is that desugarings operate on typing derivations instead of plain syntax trees. This provides desugarings access to typing information and forms the basis for the soundness guarantee we provide, namely that a desugaring generates a valid typing derivation. We have implemented our system for syntactic extensibility in a language-independent fashion and instantiated it for a substantial subset of Java, including generics and inheritance. We provide a sound Java extension for Scala-like for-comprehensions.},
  isbn = {978-1-4503-3549-2},
  keywords = {automatic verification,Language extensibility,macros,metaprogramming,type soundness,type-dependent desugaring},
  file = {/home/vipa/Zotero/storage/6AIKV522/Lorenzen and Erdweg - 2016 - Sound Type-dependent Syntactic Language Extension.pdf}
}

@book{lowAutomaticCodingChoice1976,
  title = {Automatic {{Coding}}: {{Choice}} of {{Data Structures}}},
  shorttitle = {Automatic {{Coding}}},
  author = {Low, James Richard},
  year = {1976},
  publisher = {Birkh{\"a}user},
  address = {Basel},
  doi = {10.1007/978-3-0348-5504-4},
  urldate = {2022-11-15},
  abstract = {A system is described which automatically chooses representations for high-level information structures, such as sets, sequences, and relations for a given computer program. Representations are picked from a fixed library of low- level data structures including linked-lists, binary trees and hash tables. The representations are chosen by attempting to minimize the predicted space-time integral of the users program execution. Predictions are based upon statistics of information structure use provided directly by the user and collected by monitoring executions of the user program using default representations for the high-level structures. A demonstration system has been constructed. Results using that system are presented.},
  isbn = {978-3-7643-0818-6 978-3-0348-5504-4},
  langid = {english},
  keywords = {character,coding,data structure,data structures,efficiency,implementation,information,language,Matching,programming,programming language,SAIL,sets,system,time},
  file = {/home/vipa/Zotero/storage/3KNLSYQ3/JMialiliftlilni - National Technical Information Service U. S. DEPAR.pdf;/home/vipa/Zotero/storage/5X9BRRRI/Low - 1976 - Automatic Coding Choice of Data Structures.pdf}
}

@article{lowAutomaticDataStructure1978,
  title = {Automatic Data Structure Selection: An Example and Overview},
  shorttitle = {Automatic Data Structure Selection},
  author = {Low, James R.},
  year = {1978},
  month = may,
  journal = {Communications of the ACM},
  volume = {21},
  number = {5},
  pages = {376--385},
  issn = {0001-0782},
  doi = {10.1145/359488.359498},
  urldate = {2022-11-15},
  abstract = {The use of several levels of abstraction has proved to be very helpful in constructing and maintaining programs. When programs are designed with abstract data types such as sets and lists, programmer time can be saved by automating the process of filling in low-level implementation details. In the past, programming systems have provided only a single general purpose implementation for an abstract type. Thus the programs produced using abstract types were often inefficient in space or time. In this paper a system for automatically choosing efficient implementations for abstract types from a library of implementations is discussed. This process is discussed in detail for an example program. General issues in data structure selection are also reviewed.},
  keywords = {abstract data types,automatic programming,data structures,lists,optimizing compilers,sets},
  file = {/home/vipa/Zotero/storage/ISSZ7KBB/Low - 1978 - Automatic data structure selection an example and.pdf}
}

@inproceedings{lundenAutomaticAlignmentHigherOrder2023,
  title = {Automatic {{Alignment}} in {{Higher-Order Probabilistic Programming Languages}}},
  booktitle = {Programming {{Languages}} and {{Systems}}: 32nd {{European Symposium}} on {{Programming}}, {{ESOP}} 2023, {{Held}} as {{Part}} of the {{European Joint Conferences}} on {{Theory}} and {{Practice}} of {{Software}}, {{ETAPS}} 2023, {{Paris}}, {{France}}, {{April}} 22--27, 2023, {{Proceedings}}},
  author = {Lund{\'e}n, Daniel and {\c C}aylak, Gizem and Ronquist, Fredrik and Broman, David},
  year = {2023},
  month = apr,
  pages = {535--563},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-031-30044-8_20},
  urldate = {2024-01-09},
  abstract = {Probabilistic Programming Languages (PPLs) allow users to encode statistical inference problems and automatically apply an inference algorithm to solve them. Popular inference algorithms for PPLs, such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC), are built around checkpoints---relevant events for the inference algorithm during the execution of a probabilistic program. Deciding the location of checkpoints is, in current PPLs, not done optimally. To solve this problem, we present a static analysis technique that automatically determines checkpoints in programs, relieving PPL users of this task. The analysis identifies a set of checkpoints that execute in the same order in every program run---they are aligned. We formalize alignment, prove the correctness of the analysis, and implement the analysis as part of the higher-order functional PPL Miking CorePPL. By utilizing the alignment analysis, we design two novel inference algorithm variants: aligned SMC and aligned lightweight MCMC. We show, through real-world experiments, that they significantly improve inference execution time and accuracy compared to standard PPL versions of SMC and MCMC.},
  isbn = {978-3-031-30043-1},
  keywords = {Operational semantics,Probabilistic programming,Static analysis},
  file = {/home/vipa/Zotero/storage/26H3C3EY/Lundén et al. - 2023 - Automatic Alignment in Higher-Order Probabilistic .pdf}
}

@inproceedings{lundenCompilingUniversalProbabilistic2022,
  title = {Compiling {{Universal Probabilistic Programming Languages}} with {{Efficient Parallel Sequential Monte Carlo Inference}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Lund{\'e}n, Daniel and {\"O}hman, Joey and Kudlicka, Jan and Senderov, Viktor and Ronquist, Fredrik and Broman, David},
  editor = {Sergey, Ilya},
  year = {2022},
  pages = {29--56},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-99336-8_2},
  abstract = {Probabilistic programming languages (PPLs) allow users to encode arbitrary inference problems, and PPL implementations provide general-purpose automatic inference for these problems. However, constructing inference implementations that are efficient enough is challenging for many real-world problems. Often, this is due to PPLs not fully exploiting available parallelization and optimization opportunities. For example, handling probabilistic checkpoints in PPLs through continuation-passing style transformations or non-preemptive multitasking---as is done in many popular PPLs---often disallows compilation to low-level languages required for high-performance platforms such as GPUs. To solve the checkpoint problem, we introduce the concept of PPL control-flow graphs (PCFGs)---a simple and efficient approach to checkpoints in low-level languages. We use this approach to implement RootPPL: a low-level PPL built on CUDA and C++ with OpenMP, providing highly efficient and massively parallel SMC inference. We also introduce a general method of compiling universal high-level PPLs to PCFGs and illustrate its application when compiling Miking CorePPL---a high-level universal PPL---to RootPPL. The approach is the first to compile a universal PPL to GPUs with SMC inference. We evaluate RootPPL and the CorePPL compiler through a set of real-world experiments in the domains of phylogenetics and epidemiology, demonstrating up to 6\$\${\textbackslash}times \$\${\texttimes}speedups over state-of-the-art PPLs implementing SMC inference.},
  isbn = {978-3-030-99336-8},
  langid = {english},
  keywords = {Compilers,GPU Compilation,Probabilistic Programming Languages,Sequential Monte Carlo},
  file = {/home/vipa/Zotero/storage/QJAN9RJA/Lundén et al. - 2022 - Compiling Universal Probabilistic Programming Lang.pdf}
}

@inproceedings{macedoInDubioCombinatorLibrary2020,
  title = {{{InDubio}}: {{A Combinator Library}} to {{Disambiguate Ambiguous Grammars}}},
  shorttitle = {{{InDubio}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2020},
  author = {Macedo, Jos{\'e} Nuno and Saraiva, Jo{\~a}o},
  editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Ble{\v c}i{\'c}, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria and Karaca, Yeliz},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1002--1018},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58811-3_71},
  abstract = {To infer an abstract model from source code is one of the main tasks of most software quality analysis methods. Such abstract model is called Abstract Syntax Tree and the inference task is called parsing. A parser is usually generated from a grammar specification of a (programming) language and it converts source code of that language into said abstract tree representation. Then, several techniques traverse this tree to assess the quality of the code (for example by computing source code metrics), or by building new data structures (e.g, flow graphs) to perform further analysis (such as, code cloning, dead code, etc). Parsing is a well established technique. In recent years, however, modern languages are inherently ambiguous which can only be fully handled by ambiguous grammars.In this setting disambiguation rules, which are usually included as part of the grammar specification of the ambiguous language, need to be defined. This approach has a severe limitation: disambiguation rules are not first class citizens. Parser generators offer a small set of rules that can not be extended or changed. Thus, grammar writers are not able to manipulate nor define a new specific rule that the language he is considering requires.In this paper we present a tool, name InDubio, that consists of an extensible combinator library of disambiguation filters together with a generalized parser generator for ambiguous grammars. InDubio defines a set of basic disambiguation rules as abstract syntax tree filters that can be combined into more powerful rules. Moreover, the filters are independent of the parser generator and parsing technology, and consequently, they can be easily extended and manipulated. This paper presents InDubio in detail and also presents our first experimental results.},
  isbn = {978-3-030-58811-3},
  langid = {english},
  keywords = {Combinators,Disambiguation filters,Parsing},
  file = {/home/vipa/Zotero/storage/24V6AUZY/Macedo and Saraiva - 2020 - InDubio A Combinator Library to Disambiguate Ambi.pdf}
}

@article{malietModelManySmall2019,
  title = {A Model with Many Small Shifts for Estimating Species-Specific Diversification Rates},
  author = {Maliet, Odile and Hartig, Florian and Morlon, H{\'e}l{\`e}ne},
  year = {2019},
  month = jul,
  journal = {Nature Ecology \& Evolution},
  volume = {3},
  number = {7},
  pages = {1086--1092},
  publisher = {Nature Publishing Group},
  issn = {2397-334X},
  doi = {10.1038/s41559-019-0908-0},
  urldate = {2023-02-03},
  abstract = {Understanding how and why diversification rates vary through time and space and across species groups is key to understanding the emergence of today's biodiversity. Phylogenetic approaches aimed at identifying variations in diversification rates during the evolutionary history of clades have focused on exceptional shifts subtending evolutionary radiations. While such shifts have undoubtedly affected the history of life, identifying smaller but more frequent changes is important as well. We developed ClaDS---a new Bayesian approach for estimating branch-specific diversification rates on a phylogeny that relies on a model with changes in diversification rates at each speciation event. We show, using Monte Carlo simulations, that the approach performs well at inferring both small and large changes in diversification. Applying our approach to bird phylogenies covering the entire avian radiation, we find that diversification rates are remarkably heterogeneous within evolutionarily restricted species groups. Some groups such as Accipitridae (hawks and allies) cover almost the full range of speciation rates found across the entire bird radiation. As much as 76\% of the variation in branch-specific rates across this radiation is due to intraclade variation, suggesting that a large part of the variation in diversification rates is due to many small, rather than few large, shifts.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Evolutionary theory,Phylogenetics},
  file = {/home/vipa/Zotero/storage/VCPSZ34W/Maliet et al. - 2019 - A model with many small shifts for estimating spec.pdf}
}

@article{marangetWarningsPatternMatching2007,
  title = {Warnings for Pattern Matching},
  author = {Maranget, Luc},
  year = {2007},
  month = may,
  journal = {Journal of Functional Programming},
  volume = {17},
  number = {3},
  pages = {387--421},
  publisher = {Cambridge University Press},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796807006223},
  urldate = {2020-03-20},
  abstract = {We examine the ML pattern-matching anomalies of useless clauses and non-exhaustive matches. We state the definition of these anomalies, building upon pattern matching semantics, and propose a simple algorithm to detect them. We have integrated the algorithm in the Objective Caml compiler, but we show that the same algorithm is also usable in a non-strict language such as Haskell. Or-patterns are considered for both strict and non-strict languages.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/B52XCBSF/Maranget - 2007 - Warnings for pattern matching.pdf;/home/vipa/Zotero/storage/UD38JMEC/3165B75113781E2431E3856972940347.html}
}

@inproceedings{marrFewVersatileVs2018,
  title = {Few Versatile vs. Many Specialized Collections: How to Design a Collection Library for Exploratory Programming?},
  shorttitle = {Few Versatile vs. Many Specialized Collections},
  booktitle = {Companion {{Proceedings}} of the 2nd {{International Conference}} on the {{Art}}, {{Science}}, and {{Engineering}} of {{Programming}}},
  author = {Marr, Stefan and Daloze, Benoit},
  year = {2018},
  month = apr,
  series = {Programming '18},
  pages = {135--143},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3191697.3214334},
  urldate = {2024-01-06},
  abstract = {While an integral part of all programming languages, the design of collection libraries is rarely studied. This work briefly reviews the collection libraries of 14 languages to identify possible design dimensions. Some languages have surprisingly few but versatile collections, while others have large libraries with many specialized collections. Based on the identified design dimensions, we argue that a small collection library with only a sequence, a map, and a set type are a suitable choice to facilitate exploratory programming. Such a design minimizes the number of decisions programmers have to make when dealing with collections, and it improves discoverability of collection operations. We further discuss techniques that make their implementation practical from a performance perspective. Based on these arguments, we conclude that languages which aim to support exploratory programming should strive for small and versatile collection libraries.},
  isbn = {978-1-4503-5513-1},
  keywords = {Collection Libraries,Design,Exploratory Programming,Implementation},
  file = {/home/vipa/Zotero/storage/HYNT8LYG/Marr and Daloze - 2018 - Few versatile vs. many specialized collections ho.pdf}
}

@inproceedings{matsakisRustLanguage2014,
  title = {The {{Rust Language}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGAda Annual Conference}} on {{High Integrity Language Technology}}},
  author = {Matsakis, Nicholas D. and Klock, II, Felix S.},
  year = {2014},
  series = {{{HILT}} '14},
  pages = {103--104},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2663171.2663188},
  urldate = {2018-10-18},
  abstract = {Rust is a new programming language for developing reliable and efficient systems. It is designed to support concurrency and parallelism in building applications and libraries that take full advantage of modern hardware. Rust's static type system is safe and expressive and provides strong guarantees about isolation, concurrency, and memory safety. Rust also offers a clear performance model, making it easier to predict and reason about program efficiency. One important way it accomplishes this is by allowing fine-grained control over memory representations, with direct support for stack allocation and contiguous record storage. The language balances such controls with the absolute requirement for safety: Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and accesses to uninitialized or deallocated memory.},
  isbn = {978-1-4503-3217-0},
  keywords = {affine type systems,memory management,rust,systems programming},
  file = {/home/vipa/Zotero/storage/5V24PA6K/Matsakis, Klock II - 2014 - The rust language.pdf;/home/vipa/Zotero/storage/D8HZIDXS/Matsakis and Klock - 2014 - The Rust Language.pdf}
}

@article{mckayPracticalGraphIsomorphism2014,
  title = {Practical Graph Isomorphism, {{II}}},
  author = {McKay, Brendan D. and Piperno, Adolfo},
  year = {2014},
  month = jan,
  journal = {Journal of Symbolic Computation},
  volume = {60},
  pages = {94--112},
  issn = {0747-7171},
  doi = {10.1016/j.jsc.2013.09.003},
  urldate = {2024-03-15},
  abstract = {We report the current state of the graph isomorphism problem from the practical point of view. After describing the general principles of the refinement-individualization paradigm and pro ving its validity, we explain how it is implemented in several of the key implementations. In particular, we bring the description of the best known program nauty up to date and describe an innovative approach called Traces that outperforms the competitors for many difficult graph classes. Detailed comparisons against saucy, Bliss and conauto are presented.},
  keywords = {Canonical labelling,Graph isomorphism,Nauty,Partition refinement,Traces},
  file = {/home/vipa/Zotero/storage/2WIKJJDQ/McKay and Piperno - 2014 - Practical graph isomorphism, II.pdf;/home/vipa/Zotero/storage/KL8C85ZK/S0747717113001193.html}
}

@misc{menhir,
  title = {The {{Menhir}} Parser Generator},
  author = {Pottier, Fran{\c c}ois and {R{\'e}gis-Gianas}, Yann},
  year = {2005/2022}
}

@article{milnerTheoryTypePolymorphism1978,
  title = {A Theory of Type Polymorphism in Programming},
  author = {Milner, Robin},
  year = {1978},
  month = dec,
  journal = {Journal of Computer and System Sciences},
  volume = {17},
  number = {3},
  pages = {348--375},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(78)90014-4},
  urldate = {2023-10-16},
  abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple programming language, and a compile time type-checking algorithm W which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot ``go wrong'' and a Syntactic Soundness Theorem states that if W accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on W is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system.},
  file = {/home/vipa/Zotero/storage/ZL267ADG/Milner - 1978 - A theory of type polymorphism in programming.pdf;/home/vipa/Zotero/storage/YZSU94XR/0022000078900144.html}
}

@article{mokhovSelectiveApplicativeFunctors2019,
  title = {Selective Applicative Functors},
  author = {Mokhov, Andrey and Lukyanov, Georgy and Marlow, Simon and Dimino, Jeremie},
  year = {2019},
  month = jul,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {ICFP},
  pages = {90:1--90:29},
  doi = {10.1145/3341694},
  urldate = {2020-08-26},
  abstract = {Applicative functors and monads have conquered the world of functional programming by providing general and powerful ways of describing effectful computations using pure functions. Applicative functors provide a way to compose independent effects that cannot depend on values produced by earlier computations, and all of which are declared statically. Monads extend the applicative interface by making it possible to compose dependent effects, where the value computed by one effect determines all subsequent effects, dynamically. This paper introduces an intermediate abstraction called selective applicative functors that requires all effects to be declared statically, but provides a way to select which of the effects to execute dynamically. We demonstrate applications of the new abstraction on several examples, including two industrial case studies.},
  keywords = {applicative functors,effects,monads,selective functors},
  file = {/home/vipa/Zotero/storage/XLHKT6QV/Mokhov et al. - 2019 - Selective applicative functors.pdf}
}

@article{monnierSMIEWeaknessPower2020,
  title = {{{SMIE}}: {{Weakness}} Is {{Power}}!},
  shorttitle = {{{SMIE}}},
  author = {Monnier, Stefan},
  year = {2020},
  month = jun,
  journal = {The Art, Science, and Engineering of Programming},
  volume = {5},
  number = {1},
  pages = {1:1-1:26},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2021/5/1},
  urldate = {2022-10-26},
  abstract = {Automatic indentation of source code is fundamentally a simple matter of parsing the code and then applying language- and style-specific ...},
  langid = {english},
  file = {/home/vipa/Zotero/storage/KBH87JJ3/Monnier - 2020 - SMIE Weakness is Power!.pdf;/home/vipa/Zotero/storage/2FR9CJ6D/1.html}
}

@article{morrisAbstractingExtensibleData2019,
  title = {Abstracting Extensible Data Types: Or, Rows by Any Other Name},
  shorttitle = {Abstracting Extensible Data Types},
  author = {Morris, J. Garrett and McKinna, James},
  year = {2019},
  month = jan,
  journal = {Proc. ACM Program. Lang.},
  volume = {3},
  number = {POPL},
  pages = {12:1--12:28},
  doi = {10.1145/3290325},
  urldate = {2024-07-04},
  abstract = {We present a novel typed language for extensible data types, generalizing and abstracting existing systems of row types and row polymorphism. Extensible data types are a powerful addition to traditional functional programming languages, capturing ideas from OOP-like record extension and polymorphism to modular compositional interpreters. We introduce row theories, a monoidal generalization of row types, giving a general account of record concatenation and projection (dually, variant injection and branching). We realize them via qualified types, abstracting the interpretation of records and variants over different row theories. Our approach naturally types terms untypable in other systems of extensible data types, while maintaining strong metatheoretic properties, such as coherence and principal types. Evidence for type qualifiers has computational content, determining the implementation of record and variant operations; we demonstrate this in giving a modular translation from our calculus, instantiated with various row theories, to polymorphic {$\lambda$}-calculus.},
  file = {/home/vipa/Zotero/storage/D5RZCB2F/Morris and McKinna - 2019 - Abstracting extensible data types or, rows by any.pdf}
}

@inproceedings{morrisExploringRegularTree2006,
  title = {Exploring the {{Regular Tree Types}}},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {Morris, Peter and Altenkirch, Thorsten and McBride, Conor},
  editor = {Filli{\^a}tre, Jean-Christophe and {Paulin-Mohring}, Christine and Werner, Benjamin},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {252--267},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11617990_16},
  abstract = {In this paper we use the Epigram language to define the universe of regular tree types---closed under empty, unit, sum, product and least fixpoint. We then present a generic decision procedure for Epigram's in-built equality at each type, taking a complementary approach to that of Benke, Dybjer and Jansson [7]. We also give a generic definition of map, taking our inspiration from Jansson and Jeuring [21]. Finally, we equip the regular universe with the partial derivative which can be interpreted functionally as Huet's notion of `zipper', as suggested by McBride in [27] and implemented (without the fixpoint case) in Generic Haskell by Hinze, Jeuring and L{\"o}h [18]. We aim to show through these examples that generic programming can be ordinary programming in a dependently typed language.},
  isbn = {978-3-540-31429-5},
  langid = {english},
  keywords = {Functional Programming,Functional Programming Language,International Summer School,Type Constructor,Type Theory},
  file = {/home/vipa/Zotero/storage/W6MM9P33/Morris et al. - 2006 - Exploring the Regular Tree Types.pdf}
}

@article{morrisonPATRICIAPracticalAlgorithm1968,
  title = {{{PATRICIA}}---{{Practical Algorithm To Retrieve Information Coded}} in {{Alphanumeric}}},
  author = {Morrison, Donald R.},
  year = {1968},
  month = oct,
  journal = {Journal of the ACM},
  volume = {15},
  number = {4},
  pages = {514--534},
  issn = {0004-5411},
  doi = {10.1145/321479.321481},
  urldate = {2023-10-18},
  abstract = {PATRICIA is an algorithm which provides a flexible means of storing, indexing, and retrieving information in a large file, which is economical of index space and of reindexing time. It does not require rearrangement of text or index as new material is added. It requires a minimum restriction of format of text and of keys; it is extremely flexible in the variety of keys it will respond to. It retrieves information in response to keys furnished by the user with a quantity of computation which has a bound which depends linearly on the length of keys and the number of their proper occurrences and is otherwise independent of the size of the library. It has been implemented in several variations as FORTRAN programs for the CDC-3600, utilizing disk file storage of text. It has been applied to several large information-retrieval problems and will be applied to others.},
  file = {/home/vipa/Zotero/storage/LYLKIX5J/Morrison - 1968 - PATRICIA—Practical Algorithm To Retrieve Informati.pdf}
}

@article{mullerHippocraticOathMathematicians2022,
  title = {A {{Hippocratic Oath}} for {{Mathematicians}}? {{Mapping}} the {{Landscape}} of {{Ethics}} in {{Mathematics}}},
  shorttitle = {A {{Hippocratic Oath}} for {{Mathematicians}}?},
  author = {M{\"u}ller, Dennis and Chiodo, Maurice and Franklin, James},
  year = {2022},
  journal = {Science and Engineering Ethics},
  volume = {28},
  number = {5},
  pages = {41},
  issn = {1353-3452},
  doi = {10.1007/s11948-022-00389-y},
  urldate = {2024-06-22},
  abstract = {While the consequences of mathematically-based software, algorithms and strategies have become ever wider and better appreciated, ethical reflection on mathematics has remained primitive. We review the somewhat disconnected suggestions of commentators in recent decades with a view to piecing together a coherent approach to ethics in mathematics. Calls for a Hippocratic Oath for mathematicians are examined and it is concluded that while lessons can be learned from the medical profession, the relation of mathematicians to those affected by their work is significantly different. There is something to be learned also from the codes of conduct of cognate but professionalised quantitative disciplines such as engineering and accountancy, as well as from legal principles bearing on professional work. We conclude with recommendations that professional societies in mathematics should sponsor an (international) code of ethics, institutional mission statements for mathematicians and syllabuses of ethics courses for incorporation into mathematics degrees.},
  pmcid = {PMC9427075},
  pmid = {36042113},
  file = {/home/vipa/Zotero/storage/JCD4NYC2/Müller et al. - 2022 - A Hippocratic Oath for Mathematicians Mapping the.pdf}
}

@misc{nastExclusiveHowElizabeth2016,
  title = {Exclusive: {{How Elizabeth Holmes}}'s {{House}} of {{Cards Came Tumbling Down}}},
  shorttitle = {Exclusive},
  author = {Nast, Cond{\'e}},
  year = {2016},
  month = sep,
  journal = {Vanity Fair},
  urldate = {2023-03-17},
  abstract = {In a searing investigation into the once lauded biotech start-up Theranos, Nick Bilton discovers that its precocious founder defied medical experts---even her own chief scientist---about the veracity of its now discredited blood-testing technology.},
  chapter = {tags},
  howpublished = {https://www.vanityfair.com/news/2016/09/elizabeth-holmes-theranos-exclusive},
  langid = {american}
}

@inproceedings{neronTheoryNameResolution2015,
  title = {A {{Theory}} of {{Name Resolution}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Neron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  editor = {Vitek, Jan},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {205--231},
  publisher = {Springer Berlin Heidelberg},
  abstract = {We describe a language-independent theory for name binding and resolution, suitable for programming languages with complex scoping rules including both lexical scoping and modules. We formulate name resolution as a two-stage problem. First a language-independent scope graph is constructed using language-specific rules from an abstract syntax tree. Then references in the scope graph are resolved to corresponding declarations using a language-independent resolution process. We introduce a resolution calculus as a concise, declarative, and languageindependent specification of name resolution. We develop a resolution algorithm that is sound and complete with respect to the calculus. Based on the resolution calculus we develop language-independent definitions of {$\alpha$}-equivalence and rename refactoring. We illustrate the approach using a small example language with modules. In addition, we show how our approach provides a model for a range of name binding patterns in existing languages.},
  isbn = {978-3-662-46669-8},
  langid = {english},
  keywords = {Abstract Syntax Tree,Binding Pattern,Code Completion,Resolution Algorithm,Visibility Policy},
  file = {/home/vipa/Zotero/storage/9JGIUAPL/Neron et al. - 2015 - A Theory of Name Resolution.pdf}
}

@article{obuaLocalLexing2017,
  title = {Local {{Lexing}}},
  author = {Obua, Steven and Scott, Phil and Fleuriot, Jacques},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.03277 [cs]},
  eprint = {1702.03277},
  primaryclass = {cs},
  urldate = {2021-08-10},
  abstract = {We introduce a novel parsing concept called local lexing. It integrates the classically separated stages of lexing and parsing by allowing lexing to be dependent upon the parsing progress and by providing a simple mechanism for constraining lexical ambiguity. This makes it possible for language design to be composable not only at the level of context-free grammars, but also at the lexical level. It also makes it possible to include lightweight error-handling directly as part of the language specification instead of leaving it up to the implementation. We present a high-level algorithm for local lexing, which is an extension of Earley's algorithm. We have formally verified the correctness of our algorithm with respect to its local lexing semantics in Isabelle/HOL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science},
  file = {/home/vipa/Zotero/storage/3UKVGJDA/Obua et al. - 2017 - Local Lexing.pdf;/home/vipa/Zotero/storage/HTBS7IKF/1702.html}
}

@article{oderskySimplicitlyFoundationsApplications2017,
  title = {Simplicitly: {{Foundations}} and {{Applications}} of {{Implicit Function Types}}},
  shorttitle = {Simplicitly},
  author = {Odersky, Martin and Blanvillain, Olivier and Liu, Fengyun and Biboudis, Aggelos and Miller, Heather and Stucki, Sandro},
  year = {2017},
  month = dec,
  journal = {Proc. ACM Program. Lang.},
  volume = {2},
  number = {POPL},
  pages = {42:1--42:29},
  issn = {2475-1421},
  doi = {10.1145/3158130},
  urldate = {2018-10-18},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over.  This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler.  To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  keywords = {Dotty,implicit parameters,Scala},
  file = {/home/vipa/Zotero/storage/B8BTVUNF/Odersky et al. - 2017 - Simplicitly Foundations and Applications of Impli.pdf}
}

@article{ohoriPolymorphicRecordCalculus1995,
  title = {A Polymorphic Record Calculus and Its Compilation},
  author = {Ohori, Atsushi},
  year = {1995},
  month = nov,
  journal = {ACM Trans. Program. Lang. Syst.},
  volume = {17},
  number = {6},
  pages = {844--895},
  issn = {0164-0925},
  doi = {10.1145/218570.218572},
  urldate = {2024-07-03},
  file = {/home/vipa/Zotero/storage/DCS3J9KX/Ohori - 1995 - A polymorphic record calculus and its compilation.pdf}
}

@incollection{ohoriRecordPolymorphismIts2013,
  title = {Record {{Polymorphism}}: {{Its Development}} and {{Applications}}},
  shorttitle = {Record {{Polymorphism}}},
  booktitle = {In {{Search}} of {{Elegance}} in the {{Theory}} and {{Practice}} of {{Computation}}: {{Essays Dedicated}} to {{Peter Buneman}}},
  author = {Ohori, Atsushi},
  editor = {Tannen, Val and Wong, Limsoon and Libkin, Leonid and Fan, Wenfei and Tan, Wang-Chiew and Fourman, Michael},
  year = {2013},
  pages = {432--444},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-41660-6_23},
  urldate = {2024-07-03},
  abstract = {Record polymorphism plays an essential role in developing a static type system for labeled record structures such as relational databases. Moreover, compilation method for record polymorphism serves as the basis for efficiently compiling various advanced features in statically typed polymorphic programming languages. This article overviews the power and applicability of record polymorphism that have been implemented in SML\${\textbackslash}sharp\$, an extension of Standard ML been developed at RIEC, Tohoku University.},
  isbn = {978-3-642-41660-6},
  langid = {english},
  keywords = {Compilation Method,Functional Language,Garbage Collector,Type Inference,Type System},
  file = {/home/vipa/Zotero/storage/HE9R7W2V/Ohori - 2013 - Record Polymorphism Its Development and Applicati.pdf}
}

@inproceedings{okasakiFastMergeableInteger1998,
  title = {Fast Mergeable Integer Maps},
  booktitle = {Workshop on {{ML}}},
  author = {Okasaki, Chris and Gill, Andrew},
  year = {1998},
  pages = {77--86},
  urldate = {2023-10-18},
  file = {/home/vipa/Zotero/storage/6EMUIWP8/Okasaki and Gill - 1998 - Fast mergeable integer maps.pdf}
}

@inproceedings{okhotinUnderlyingPrinciplesRecurring2018,
  title = {Underlying {{Principles}} and {{Recurring Ideas}} of {{Formal Grammars}}},
  booktitle = {Language and {{Automata Theory}} and {{Applications}}},
  author = {Okhotin, Alexander},
  editor = {Klein, Shmuel Tomi and {Mart{\'i}n-Vide}, Carlos and Shapira, Dana},
  year = {2018},
  pages = {36--59},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-77313-1_3},
  abstract = {The paper investigates some of the fundamental ideas of the context-free grammar theory, as they are applied to several extensions and subclasses of context-free grammars. For these grammar families, including multi-component grammars, tree-adjoining grammars, conjunctive grammars and Boolean grammars, a summary of the following properties is given: parse trees, language equations, closure under several operations, normal forms, parsing algorithms, representation in the FO(LFP) logic, representations by automata and by categorial grammars, homomorphic characterizations, hardest language theorems, pumping lemmata and other limitations, computational complexity.},
  isbn = {978-3-319-77313-1},
  langid = {english},
  file = {/home/vipa/Zotero/storage/PZ7BEZU7/Okhotin - 2018 - Underlying Principles and Recurring Ideas of Forma.pdf}
}

@article{oliveiraImplicitCalculusNew2012,
  title = {The Implicit Calculus: A New Foundation for Generic Programming},
  shorttitle = {The Implicit Calculus},
  author = {Oliveira, Bruno C.d.S. and Schrijvers, Tom and Choi, Wontae and Lee, Wonchan and Yi, Kwangkeun},
  year = {2012},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {47},
  number = {6},
  pages = {35--44},
  issn = {0362-1340},
  doi = {10.1145/2345156.2254070},
  urldate = {2022-09-01},
  abstract = {Generic programming (GP) is an increasingly important trend in programming languages. Well-known GP mechanisms, such as type classes and the C++0x concepts proposal, usually combine two features: 1) a special type of interfaces; and 2) implicit instantiation of implementations of those interfaces. Scala implicits are a GP language mechanism, inspired by type classes, that break with the tradition of coupling implicit instantiation with a special type of interface. Instead, implicits provide only implicit instantiation, which is generalized to work for any types. This turns out to be quite powerful and useful to address many limitations that show up in other GP mechanisms. This paper synthesizes the key ideas of implicits formally in a minimal and general core calculus called the implicit calculus ({$\lambda\Rightarrow$}), and it shows how to build source languages supporting implicit instantiation on top of it. A novelty of the calculus is its support for partial resolution and higher-order rules (a feature that has been proposed before, but was never formalized or implemented). Ultimately, the implicit calculus provides a formal model of implicits, which can be used by language designers to study and inform implementations of similar mechanisms in their own languages.},
  keywords = {c++ concepts,generic programming,haskell,implicit parameters,scala,type classes},
  file = {/home/vipa/Zotero/storage/KZYBIK3I/Oliveira et al. - 2012 - The implicit calculus a new foundation for generi.pdf}
}

@article{oliveiraImprovingEnergyefficiencyRecommending2021,
  title = {Improving Energy-Efficiency by Recommending {{Java}} Collections},
  author = {Oliveira, Wellington and Oliveira, Renato and Castor, Fernando and Pinto, Gustavo and Fernandes, Jo{\~a}o Paulo},
  year = {2021},
  month = apr,
  journal = {Empirical Software Engineering},
  volume = {26},
  number = {3},
  pages = {55},
  issn = {1573-7616},
  doi = {10.1007/s10664-021-09950-y},
  urldate = {2024-01-07},
  abstract = {Over the last years, increasing attention has been given to creating energy-efficient software systems. However, developers still lack the knowledge and the tools to support them in that task. In this work, we explore our vision that non-specialists can build software that consumes less energy by alternating diversely-designed pieces of software without increasing the development complexity. To support our vision, we propose an approach for energy-aware development that combines the construction of application-independent energy profiles of Java collections and static analysis to produce an estimate of in which ways and how intensively a system employs these collections. We implement this approach in a tool named CT+ that works with both desktop and mobile Java systems and is capable of analyzing 39 different collection implementations of lists, maps, and sets. We applied CT+ to seventeen software systems: two mobile-based, twelve desktop-based, and three that can run in both environments. Our evaluation infrastructure involved a high-end server, two notebooks, three smartphones, and a tablet. Overall, 2295 recommendations were applied, achieving up to 16.34\% reduction in energy consumption, usually changing a single line of code per recommendation. Even for a real-world, mature system such as Tomcat, CT+ could achieve a 4.12\% reduction in energy consumption. Our results indicate that some widely used collections, e.g., ArrayList, HashMap, and Hashtable, are not energy- efficient and sometimes should be avoided when energy consumption is a major concern.},
  langid = {english},
  keywords = {Collections,Energy consumption,Recommendation systems},
  file = {/home/vipa/Zotero/storage/P9S5HE2V/Oliveira et al. - 2021 - Improving energy-efficiency by recommending Java c.pdf}
}

@inproceedings{oliveiraRecommendingEnergyEfficientJava2019,
  title = {Recommending {{Energy-Efficient Java Collections}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 16th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Oliveira, Wellington and Oliveira, Renato and Castor, Fernando and Fernandes, Benito and Pinto, Gustavo},
  year = {2019},
  month = may,
  pages = {160--170},
  issn = {2574-3864},
  doi = {10.1109/MSR.2019.00033},
  urldate = {2024-01-07},
  abstract = {Over the last years, increasing attention has been given to creating energy-efficient software systems. However, developers still lack the knowledge and the tools to support them in that task. In this work, we explore our vision that energy consumption non-specialists can build software that consumes less energy by alternating, at development time, between third-party, readily available, diversely-designed pieces of software, without increasing the development complexity. To support our vision, we propose an approach for energy-aware development that combines the construction of application-independent energy profiles of Java collections and static analysis to produce an estimate of in which ways and how intensively a system employs these collections. By combining these two pieces of information, it is possible to produce energy-saving recommendations for alternative collection implementations to be used in different parts of the system. We implement this approach in a tool named CT+ that works with both desktop and mobile Java systems, and is capable of analyzing 40 different collection implementations of lists, maps, and sets. We applied CT+ to twelve software systems: two mobile-based, seven desktop-based, and three that can run in both environments. Our evaluation infrastructure involved a high-end server, a notebook, and three mobile devices. When applying the (mostly trivial) recommendations, we achieved up to 17.34\% reduction in energy consumption just by replacing collection implementations. Even for a real world, mature, highly-optimized system such as Xalan, CT+ could achieve a 5.81\% reduction in energy consumption. Our results indicate that some widely used collections, e.g., ArrayList, HashMap, and HashTable, are not energy-efficient and sometimes should be avoided when energy consumption is a major concern.},
  file = {/home/vipa/Zotero/storage/XY8F6C5U/Oliveira et al. - 2019 - Recommending Energy-Efficient Java Collections.pdf;/home/vipa/Zotero/storage/86UMVNUT/8816747.html}
}

@inproceedings{olivoStaticDetectionAsymptotic2015,
  title = {Static Detection of Asymptotic Performance Bugs in Collection Traversals},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Olivo, Oswaldo and Dillig, Isil and Lin, Calvin},
  year = {2015},
  month = jun,
  series = {{{PLDI}} '15},
  pages = {369--378},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2737924.2737966},
  urldate = {2022-11-15},
  abstract = {This paper identifies and formalizes a prevalent class of asymptotic performance bugs called redundant traversal bugs and presents a novel static analysis for automatically detecting them. We evaluate our technique by implementing it in a tool called CLARITY and applying it to widely-used software packages such as the Google Core Collections Library, the Apache Common Collections, and the Apache Ant build tool. Across 1.6M lines of Java code, CLARITY finds 92 instances of redundant traversal bugs, including 72 that have never been previously reported, with just 5 false positives. To evaluate the performance impact of these bugs, we manually repair these programs and find that for an input size of 50,000, all repaired programs are at least 2.45 faster than their original code.},
  isbn = {978-1-4503-3468-6},
  keywords = {performance bugs,program analysis,static analysis},
  file = {/home/vipa/Zotero/storage/CQZP275L/Olivo et al. - 2015 - Static detection of asymptotic performance bugs in.pdf}
}

@inproceedings{olmosComposingSourcetoSourceDataFlow2005,
  title = {Composing {{Source-to-Source Data-Flow Transformations}} with {{Rewriting Strategies}} and {{Dependent Dynamic Rewrite Rules}}},
  booktitle = {Compiler {{Construction}}},
  author = {Olmos, Karina and Visser, Eelco},
  editor = {Bodik, Rastislav},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {204--220},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-31985-6_14},
  abstract = {Data-flow transformations used in optimizing compilers are also useful in other programming tools such as code generators, aspect weavers, domain-specific optimizers, and refactoring tools. These applications require source-to-source transformations rather than transformations on a low-level intermediate representation. In this paper we describe the composition of source-to-source data-flow transformations in the program transformation language Stratego. The language supports the high-level specification of transformations by means of rewriting strategy combinators that allow a natural modeling of data- and control-flow without committing to a specific source language. Data-flow facts are propagated using dynamic rewriting rules. In particular, we introduce the concept of dependent dynamic rewrite rules for modeling the dependencies of data-flow facts on program entities such as variables. The approach supports the combination of analysis and transformation, the combination of multiple transformations, the combination with other types of transformations, and the correct treatment of variable binding constructs and lexical scope to avoid free variable capture.},
  isbn = {978-3-540-31985-6},
  langid = {english},
  keywords = {Abstract Syntax,Concrete Syntax,Constant Propagation,Program Transformation,Source Language},
  file = {/home/vipa/Zotero/storage/XLPAMKYM/Olmos and Visser - 2005 - Composing Source-to-Source Data-Flow Transformatio.pdf}
}

@article{olmosStrategiesSourcetoSourceConstant2002,
  title = {Strategies for {{Source-to-Source Constant Propagation}}},
  author = {Olmos, Karina and Visser, Eelco},
  year = {2002},
  month = dec,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{WRS}} 2002, 2nd {{International Workshop}} on {{Reduction Strategies}} in {{Rewriting}} and {{Programming}} - {{Final Proceedings}} ({{FLoC Satellite Event}})},
  volume = {70},
  number = {6},
  pages = {156--175},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)80605-4},
  urldate = {2020-11-27},
  abstract = {Data-flow optimizations are usually implemented on low-level intermediate representations. This is not appropriate for source-to-source optimizations, which reconstruct a source level program after transformation. In this paper we show how constant propagation, a well known data-flow optimization problem, can be implemented on abstract syntax trees in Stratego, a rewriting system extended with programmable rewriting strategies for the control over the application of rules and dynamic rewrite rules for the propagation of information.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/N22386AU/Olmos and Visser - 2002 - Strategies for Source-to-Source Constant Propagati.pdf;/home/vipa/Zotero/storage/LRI86PA2/S1571066104806054.html}
}

@inproceedings{omarSafelyComposableTypeSpecific2014,
  title = {Safely {{Composable Type-Specific Languages}}},
  booktitle = {{{ECOOP}} 2014 -- {{Object-Oriented Programming}}},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  editor = {Jones, Richard},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {105--130},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  isbn = {978-3-662-44202-9},
  langid = {english},
  keywords = {bidirectional typechecking,extensible languages,hygiene,parsing},
  file = {/home/vipa/Zotero/storage/KT9AALQD/Omar et al. - 2014 - Safely Composable Type-Specific Languages.pdf}
}

@inproceedings{osterlundDynamicallyTransformingData2013,
  title = {Dynamically Transforming Data Structures},
  booktitle = {2013 28th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {{\"O}sterlund, Erik and L{\"o}we, Welf},
  year = {2013},
  month = nov,
  pages = {410--420},
  doi = {10.1109/ASE.2013.6693099},
  urldate = {2024-01-06},
  abstract = {Fine-tuning which data structure implementation to use for a given problem is sometimes tedious work since the optimum solution depends on the context, i.e., on the operation sequences, actual parameters as well as on the hardware available at run time. Sometimes a data structure with higher asymptotic time complexity performs better in certain contexts because of lower constants. The optimal solution may not even be possible to determine at compile time. We introduce transformation data structures that dynamically change their internal representation variant based on a possibly changing context. The most suitable variant is selected at run time rather than at compile time. We demonstrate the effect on performance with a transformation ArrayList data structure using an array variant and a linked hash bag variant as alternative internal representations. Using our transformation ArrayList, the standard DaCapo benchmark suite shows a performance gain of 5.19\% in average.},
  file = {/home/vipa/Zotero/storage/9HGLLGVI/Österlund and Löwe - 2013 - Dynamically transforming data structures.pdf;/home/vipa/Zotero/storage/QUYHSZZP/6693099.html}
}

@inproceedings{pachecoCalculatingLensesOptimising2011,
  title = {Calculating with Lenses: Optimising Bidirectional Transformations},
  shorttitle = {Calculating with Lenses},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN}} Workshop on {{Partial}} Evaluation and Program Manipulation},
  author = {Pacheco, Hugo and Cunha, Alcino},
  year = {2011},
  month = jan,
  series = {{{PEPM}} '11},
  pages = {91--100},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1929501.1929520},
  urldate = {2020-11-20},
  abstract = {This paper presents an equational calculus to reason about bidirectional transformations specified in the point-free style. In particular, it focuses on the so-called lenses as a bidirectional idiom, and shows that many standard laws characterising point-free combinators and recursion patterns are also valid in that setting. A key result is that uniqueness also holds for bidirectional folds and unfolds, thus unleashing the power of fusion as a program optimisation technique. A rewriting system for automatic lens optimisation is also presented, to prove the usefulness of the proposed calculus.},
  isbn = {978-1-4503-0485-6},
  keywords = {bidirectional transformation,point-free programming,program calculation},
  file = {/home/vipa/Zotero/storage/6CDMD6FN/Pacheco and Cunha - 2011 - Calculating with lenses optimising bidirectional .pdf}
}

@book{palmkvistBuildingProgrammingLanguages2018,
  title = {Building {{Programming Languages}}, {{Construction}} by {{Construction}}},
  author = {Palmkvist, Viktor},
  year = {2018},
  series = {{{TRITA-EECS-EX}}},
  number = {2018:408},
  publisher = {{KTH, School of Electrical Engineering and Computer Science (EECS)}},
  urldate = {2018-10-18},
  abstract = {The task of implementing a programming language is a task that entails a great deal of work. Yet much of this work is similar for different programming languages: most languages require, e.g., parsing, name resolution, type-checking, and optimization. When implementing domain-specific languages (DSLs) the reimplementation of these largely similar tasks seems especially redundant. A number of approaches exist to alleviate this issue, including embedded DSLs, macro-rewriting systems, and more general systems intended for language implementation. However, these tend to have at least one of the following limitations: They present a leaky abstraction, e.g., error messages do not refer to the DSL but rather some other programming language, namely the one used to implement the DSL. They limit the flexibility of the DSL, either to the constructs present in another language, or merely to the syntax of some other language. They see an entire language as the unit of composition. Complete languages are extended with other complete language extensions. Instead, this thesis introduces the concept of a syntax construction, which represents a smaller unit of composition. A syntax construction defines a single language feature, e.g., an if-statement, an anonymous function, or addition. Each syntax construction specifies its own syntax, binding semantics, and runtime semantics, independent of the rest of the language. The runtime semantics are defined using a translation into another target language, similarly to macros. These translations can then be checked to ensure that they preserve binding semantics and introduce no binding errors. This checking ensures that binding errors can be presented in terms of code the programmer wrote, rather than generated code in some underlying language. During evaluation several limitations are encountered. Removing or minimizing these limitations appears possible, but is left for future work},
  langid = {english},
  keywords = {domain-specific language,programming language construction},
  file = {/home/vipa/Zotero/storage/8VSGV9KR/Palmkvist - 2018 - Building Programming Languages, Construction by Co.pdf;/home/vipa/Zotero/storage/G6UC36JC/record.html}
}

@inproceedings{palmkvistCreatingDomainSpecificLanguages2019,
  title = {Creating {{Domain-Specific Languages}} by {{Composing Syntactical Constructs}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {Palmkvist, Viktor and Broman, David},
  editor = {Alferes, Jos{\'e} J{\'u}lio and Johansson, Moa},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {187--203},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-05998-9_12},
  abstract = {Creating a programming language is a considerable undertaking, even for relatively small domain-specific languages (DSLs). Most approaches to ease this task either limit the flexibility of the DSL or consider entire languages as the unit of composition. This paper presents a new approach using syntactical constructs (also called syncons) for defining DSLs in much smaller units of composition while retaining flexibility. A syntactical construct defines a single language feature, such as an if statement or an anonymous function. Each syntactical construct is fully self-contained: it specifies its own concrete syntax, binding semantics, and runtime semantics, independently of the rest of the language. The runtime semantics are specified as a translation to a user defined target language, while the binding semantics allow name resolution before expansion. Additionally, we present a novel approach for dealing with syntactical ambiguity that arises when combining languages, even if the languages are individually unambiguous. The work is implemented and evaluated in a case study, where small subsets of OCaml and Lua have been defined and composed using syntactical constructs.},
  isbn = {978-3-030-05998-9},
  langid = {english},
  file = {/home/vipa/Zotero/storage/T5FH2I62/Palmkvist and Broman - 2019 - Creating Domain-Specific Languages by Composing Sy.pdf}
}

@misc{palmkvistReprTypesOne2024,
  title = {Repr {{Types}}: {{One Abstraction}} to {{Rule Them All}}},
  shorttitle = {Repr {{Types}}},
  author = {Palmkvist, Viktor and Thun{\'e}, Anders {\AA}gren and Castegren, Elias and Broman, David},
  year = {2024},
  month = sep,
  number = {arXiv:2409.07950},
  eprint = {2409.07950},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.07950},
  urldate = {2024-09-19},
  abstract = {The choice of how to represent an abstract type can have a major impact on the performance of a program, yet mainstream compilers cannot perform optimizations at such a high level. When dealing with optimizations of data type representations, an important feature is having extensible representation-flexible data types; the ability for a programmer to add new abstract types and operations, as well as concrete implementations of these, without modifying the compiler or a previously defined library. Many research projects support high-level optimizations through static analysis, instrumentation, or benchmarking, but they are all restricted in at least one aspect of extensibility. This paper presents a new approach to representation-flexible data types without such restrictions and which still finds efficient optimizations. Our approach centers around a single built-in type \${\textbackslash}texttt\{repr\}\$ and function overloading with cost annotations for operation implementations. We evaluate our approach (i) by defining a universal collection type as a library, a single type for all conventional collections, and (ii) by designing and implementing a representation-flexible graph library. Programs using \${\textbackslash}texttt\{repr\}\$ types are typically faster than programs with idiomatic representation choices -- sometimes dramatically so -- as long as the compiler finds good implementations for all operations. Our compiler performs the analysis efficiently by finding optimized solutions quickly and by reusing previous results to avoid recomputations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Performance,Computer Science - Programming Languages,D.3.3,D.4.8},
  file = {/home/vipa/Zotero/storage/7SER7QCD/Palmkvist et al. - 2024 - Repr Types One Abstraction to Rule Them All.pdf;/home/vipa/Zotero/storage/NDL7YTS4/2409.html}
}

@misc{palmkvistResolvableAmbiguityCC2021,
  title = {Resolvable {{Ambiguity}} - {{CC Artifact}}},
  author = {Palmkvist, Viktor and Castegren, Elias and Haller, Philipp and Broman, David},
  year = {2021},
  month = jan,
  doi = {10.5281/zenodo.4458159},
  urldate = {2024-07-29},
  abstract = {This Docker image contains the software required to reproduce the results in the paper "Resolvable Ambiguity: Principled Resolution of Syntactically Ambiguous Programs" to be published in ACM SIGPLAN 2021 International Conference on Compiler Construction (CC 2021). Note that as there is an element of randomness in the composition of language fragments, we include the same configuration used for Section 6, but also allow selecting new random combinations. For completeness, we also include the source code of our tool, but as it is not the main focus, it will not be as approachable as running the experiments. Additionally, you can also find the supplementary material mentioned in the paper on the trickier cases of constructing AST-languages as a pdf below. Prerequisites Docker, see docs.docker.com/get-docker/ Details The image contains the implementation source code (under /home/syncon/syncon-parser) from https://github.com/miking-lang/syncon and a pre-built binary (available on the PATH as syncon-parser), as well as the data used in the paper, and the runner used to produce it (under /home/data). This includes the generated data from the experiment, i.e., it is possible to regenerate exactly the figures used in the paper. For additional information, there are README files in the image under/home/data, /home/syncon, and /home/syncon/syncon-parser. Running the Container To run the container, ensure that you have Docker installed (see the earlier link), then run: docker load --input resolvable-image.tar.gz docker run -p 8888:8888 -it --name resolvable-container resolvable-image If you at a later point wish to start the container again you can use: docker start -ia resolvable-container Inside the Container - Running the Experiments To reproduce the data in the paper run the following commands inside the container. Note that the command will print the folder in which the logs are placed (once when it starts, and once when it is finished), take note of this directory. cd /home/data/fragments runner reclassify-paper  \# Rerun the experiment on the languages examined in the paper.                          \# This has been tested on Linux and Mac, and takes {\textasciitilde}4h on a                          \# laptop with an Intel Core i7-8550U (4 cores) and 16 GiB RAM. To instead run a new experiment with newly generated language compositions: cd /home/data/fragments runner classify-many     \# Run the experiment with a new set of generated languages Inside the Container - Examining the Results cd /home/data runner jupyter           \# Open the Jupyter notebook used to analyze the data and generate                          \# graphs. Copy the link printed starting with 127.0.0.1                           \# and open it in your web browser; the 'docker run' command                          \# above exposes the port outside the container. Once inside Jupyter in your web browser, open 'Analysis.ipynb' in the file browser top-left, scroll down, and follow the instructions to add analysis of the new run. Remember to rerun everything to see the new results (in the menu, 'Run {$>$} Run All Cells'). The Docker image contains all the data from the runs used in the paper, thus the exact same data and graphs are available for comparison inside the Jupyter notebook. Since our approach is based on property based testing, which includes an element of randomness, we expect to see some variance in the classifications of the languages (see "Classification Comparisons" in the notebook). We also expect some variance in runtimes, partially due to potential hardware differences but also due to said randomness. In particular, better hardware may lead to more "ambiguous" classifications, as some additional analyses have time to finish before the timeout. However, the rough shape of the graphs should be similar.},
  howpublished = {Zenodo},
  file = {/home/vipa/Zotero/storage/T3HHTARM/Palmkvist et al. - 2021 - Resolvable Ambiguity - CC Artifact.pdf}
}

@inproceedings{palmkvistResolvableAmbiguityPrincipled2021,
  title = {Resolvable Ambiguity: Principled Resolution of Syntactically Ambiguous Programs},
  shorttitle = {Resolvable Ambiguity},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN International Conference}} on {{Compiler Construction}}},
  author = {Palmkvist, Viktor and Castegren, Elias and Haller, Philipp and Broman, David},
  year = {2021},
  month = mar,
  series = {{{CC}} 2021},
  pages = {153--164},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3446804.3446846},
  urldate = {2021-08-25},
  abstract = {When building a new programming language, it can be useful to compose parts of existing languages to avoid repeating implementation work. However, this is problematic already at the syntax level, as composing the grammars of language fragments can easily lead to an ambiguous grammar. State-of-the-art parser tools cannot handle ambiguity truly well: either the grammar cannot be handled at all, or the tools give little help to an end-user who writes an ambiguous program. This composability problem is twofold: (i) how can we detect if the composed grammar is ambiguous, and (ii) if it is ambiguous, how can we help a user resolve an ambiguous program? In this paper, we depart from the traditional view of unambiguous grammar design and enable a language designer to work with an ambiguous grammar, while giving users the tools needed to handle these ambiguities. We introduce the concept of resolvable ambiguity wherein a user can resolve an ambiguous program by editing it, as well as an approach to computing the resolutions of an ambiguous program. Furthermore, we present a method based on property-based testing to identify if a composed grammar is unambiguous, resolvably ambiguous, or unresolvably ambiguous. The method is implemented in Haskell and evaluated on a large set of language fragments selected from different languages. The evaluation shows that (i) the approach can handle significantly more cases of language compositions compared to approaches which ban ambiguity altogether, and (ii) that the approach is fast enough to be used in practice.},
  isbn = {978-1-4503-8325-7},
  keywords = {Ambiguity,Syntax},
  file = {/home/vipa/Zotero/storage/GLV3ECZB/Palmkvist et al. - 2021 - Resolvable ambiguity principled resolution of syn.pdf}
}

@article{palmkvistStaticallyResolvableAmbiguity2023,
  title = {Statically {{Resolvable Ambiguity}}},
  author = {Palmkvist, Viktor and Castegren, Elias and Haller, Philipp and Broman, David},
  year = {2023},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {7},
  number = {POPL},
  pages = {58:1686--58:1712},
  doi = {10.1145/3571251},
  urldate = {2023-02-15},
  abstract = {Traditionally, a grammar defining the syntax of a programming language is typically both context free and unambiguous. However, recent work suggests that an attractive alternative is to use ambiguous grammars,thus postponing the task of resolving the ambiguity to the end user. If all programs accepted by an ambiguous grammar can be rewritten unambiguously, then the parser for the grammar is said to be resolvably ambiguous. Guaranteeing resolvable ambiguity statically---for all programs---is hard, where previous work only solves it partially using techniques based on property-based testing. In this paper, we present the first efficient, practical, and proven correct solution to the statically resolvable ambiguity problem. Our approach introduces several key ideas, including splittable productions, operator sequences, and the concept of a grouper that works in tandem with a standard parser. We prove static resolvability using a Coq mechanization and demonstrate its efficiency and practical applicability by implementing and integrating resolvable ambiguity into an essential part of the standard OCaml parser.},
  keywords = {Coq,OCaml,Parser,Resolvable Ambiguity},
  file = {/home/vipa/Zotero/storage/JTQYMC42/Palmkvist et al. - 2023 - Statically Resolvable Ambiguity.pdf}
}

@article{parikhContextFreeLanguages1966,
  title = {On {{Context-Free Languages}}},
  author = {Parikh, Rohit J.},
  year = {1966},
  month = oct,
  journal = {Journal of the ACM},
  volume = {13},
  number = {4},
  pages = {570--581},
  issn = {0004-5411},
  doi = {10.1145/321356.321364},
  urldate = {2020-10-11},
  abstract = {In this report, certain properties of context-free (CF or type 2) grammars are investigated, like that of Chomsky. In particular, questions regarding structure, possible ambiguity and relationship to finite automata are considered. The following results are presented:},
  file = {/home/vipa/Zotero/storage/NTBP77C2/Parikh - 1966 - On Context-Free Languages.pdf}
}

@inproceedings{parrAdaptiveLLParsing2014,
  title = {Adaptive {{LL}}(*) {{Parsing}}: {{The Power}} of {{Dynamic Analysis}}},
  shorttitle = {Adaptive {{LL}}(*) {{Parsing}}},
  booktitle = {Proceedings of the 2014 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} \& {{Applications}}},
  author = {Parr, Terence and Harwell, Sam and Fisher, Kathleen},
  year = {2014},
  series = {{{OOPSLA}} '14},
  pages = {579--598},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2660193.2660202},
  urldate = {2019-10-25},
  abstract = {Despite the advances made by modern parsing strategies such as PEG, LL(*), GLR, and GLL, parsing is not a solved problem. Existing approaches suffer from a number of weaknesses, including difficulties supporting side-effecting embedded actions, slow and/or unpredictable performance, and counter-intuitive matching strategies. This paper introduces the ALL(*) parsing strategy that combines the simplicity, efficiency, and predictability of conventional top-down LL(k) parsers with the power of a GLR-like mechanism to make parsing decisions. The critical innovation is to move grammar analysis to parse-time, which lets ALL(*) handle any non-left-recursive context-free grammar. ALL(*) is O(n4) in theory but consistently performs linearly on grammars used in practice, outperforming general strategies such as GLL and GLR by orders of magnitude. ANTLR 4 generates ALL(*) parsers and supports direct left-recursion through grammar rewriting. Widespread ANTLR 4 use (5000 downloads/month in 2013) provides evidence that ALL(*) is effective for a wide variety of applications.},
  isbn = {978-1-4503-2585-1},
  keywords = {all(*),augmented transition networks,dfa,gll,glr,grammar,ll(*),nondeterministic parsing,peg},
  file = {/home/vipa/Zotero/storage/65HLSZSU/Parr et al. - 2014 - Adaptive LL() Parsing The Power of Dynamic Analy.pdf}
}

@inproceedings{parreauxQuotedStagedRewriting2017,
  title = {Quoted Staged Rewriting: A Practical Approach to Library-Defined Optimizations},
  shorttitle = {Quoted Staged Rewriting},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Parreaux, Lionel and Shaikhha, Amir and Koch, Christoph E.},
  year = {2017},
  month = oct,
  series = {{{GPCE}} 2017},
  pages = {131--145},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3136040.3136043},
  urldate = {2020-11-20},
  abstract = {Staging has proved a successful technique for programmatically removing code abstractions, thereby allowing for faster program execution while retaining a high-level interface for the programmer. Unfortunately, techniques based on staging suffer from a number of problems --- ranging from practicalities to fundamental limitations --- which have prevented their widespread adoption. We introduce Quoted Staged Rewriting (QSR), an approach that uses type-safe, pattern matching-enabled quasiquotes to define optimizations. The approach is ``staged'' in two ways: first, rewrite rules can execute arbitrary code during pattern matching and code reconstruction, leveraging the power and flexibility of staging; second, library designers can orchestrate the application of successive rewriting phases (stages). The advantages of using quasiquote-based rewriting are that library designers never have to deal directly with the intermediate representation (IR), and that it allows for non-intrusive optimizations --- in contrast with staging, it is not necessary to adapt the entire library and user programs to accommodate optimizations. We show how Squid, a Scala macro-based framework, enables QSR and renders library-defined optimizations more practical than ever before: library designers write domain-specific optimizers that users invoke transparently on delimited portions of their code base. As a motivating example we describe an implementation of stream fusion (a well-known deforestation technique) that is both simpler and more powerful than the state of the art, and can readily be used by Scala programmers with no knowledge of metaprogramming.},
  isbn = {978-1-4503-5524-7},
  keywords = {Optimization,Rewrite Rules,Staging},
  file = {/home/vipa/Zotero/storage/DI2FPVU8/Parreaux et al. - 2017 - Quoted staged rewriting a practical approach to l.pdf}
}

@inproceedings{parrLLFoundationANTLR2011,
  title = {{{LL}}(*): {{The Foundation}} of the {{ANTLR Parser Generator}}},
  shorttitle = {{{LL}}(*)},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Parr, Terence and Fisher, Kathleen},
  year = {2011},
  series = {{{PLDI}} '11},
  pages = {425--436},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1993498.1993548},
  urldate = {2018-10-18},
  abstract = {Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional LL and LR parsers can lead to unexpected parse-time behavior and introduces practical issues with error handling, single-step debugging, and side-effecting embedded grammar actions. This paper introduces the LL(*) parsing strategy and an associated grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional fixed k{$>$}=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. LL(*) parsing strength reaches into the context-sensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, LL(*) provides the expressivity of PEGs while retaining LL's good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.},
  isbn = {978-1-4503-0663-8},
  keywords = {augmented transition networks,backtracking,context-sensitive parsing,deterministic finite automata,glr,memoization,nondeterministic parsing,peg,semantic predicates,subset construction,syntactic predicates},
  file = {/home/vipa/Zotero/storage/Y9FRARB7/Parr and Fisher - 2011 - LL() The Foundation of the ANTLR Parser Generato.pdf}
}

@misc{peterHyperfine2023,
  title = {Hyperfine},
  author = {Peter, David},
  year = {2023},
  month = mar,
  copyright = {MIT}
}

@inproceedings{petkeUsingGeneticImprovement2014,
  title = {Using {{Genetic Improvement}} and {{Code Transplants}} to {{Specialise}} a {{C}}++ {{Program}} to a {{Problem Class}}},
  booktitle = {Genetic {{Programming}}},
  author = {Petke, Justyna and Harman, Mark and Langdon, William B. and Weimer, Westley},
  editor = {Nicolau, Miguel and Krawiec, Krzysztof and Heywood, Malcolm I. and Castelli, Mauro and {Garc{\'i}a-S{\'a}nchez}, Pablo and Merelo, Juan J. and Rivas Santos, Victor M. and Sim, Kevin},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {137--149},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-44303-3_12},
  abstract = {Genetic Improvement (GI) is a form of Genetic Programming that improves an existing program. We use GI to evolve a faster version of a C++ program, a Boolean satisfiability (SAT) solver called MiniSAT, specialising it for a particular problem class, namely Combinatorial Interaction Testing (CIT), using automated code transplantation. Our GI-evolved solver achieves overall 17\% improvement, making it comparable with average expert human performance. Additionally, this automatically evolved solver is faster than any of the human-improved solvers for the CIT problem.},
  isbn = {978-3-662-44303-3},
  langid = {english},
  keywords = {Boolean satisfiability,code specialisation,code transplants,genetic improvement},
  file = {/home/vipa/Zotero/storage/H593FWU5/Petke et al. - 2014 - Using Genetic Improvement and Code Transplants to .pdf}
}

@book{pierceAdvancedTopicsTypes2004,
  title = {Advanced {{Topics}} in {{Types}} and {{Programming Languages}}},
  author = {Pierce, Benjamin C.},
  year = {2004},
  month = nov,
  publisher = {The MIT Press},
  isbn = {978-0-262-16228-9}
}

@book{pierceTypesProgrammingLanguages2002,
  title = {Types and {{Programming Languages}}},
  author = {Pierce, Benjamin C.},
  year = {2002},
  journal = {Types and programming languages},
  publisher = {MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems--and of programming languages from a type-theoretic perspective -- -has important applications in software engineering, language design, high-performance compilers, and security.This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.},
  isbn = {978-0-262-25681-0},
  langid = {english},
  lccn = {2001044428 (print)},
  keywords = {Programming languages (Electronic computers)},
  file = {/home/vipa/Zotero/storage/I8MBQ3MV/Pierce - 2002 - Types and Programming Languages.pdf}
}

@inproceedings{pilDynamicTypesType1999,
  title = {Dynamic {{Types}} and {{Type Dependent Functions}}},
  booktitle = {Implementation of {{Functional Languages}}},
  author = {Pil, Marco},
  editor = {Hammond, Kevin and Davie, Tony and Clack, Chris},
  year = {1999},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {169--185},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-48515-5_11},
  abstract = {When programs communicate with other programs, flexibility is demanded. Programs do not necessarily have information about each other. When assigning types to these communications a certain amount of dynamic typing is unavoidable. But we do not want our entire language to become dynamically typed, and consequently an interface between the statically and dynamically typed parts of the program has to be defined. Such an interface, using dynamics, has been introduced by Abadi et al. Leroy and Mauni extended the system of dynamics to allow the inclusion of polymorphic objects in dynamics. In this paper we extend the system even further with a restricted form of type dependent functions, which allow us to abstract over the types of the dynamics on functional level. In Clean, these type dependent functions will be implemented by overloading},
  isbn = {978-3-540-48515-5},
  langid = {english},
  file = {/home/vipa/Zotero/storage/S8KSKAH4/Pil - 1999 - Dynamic Types and Type Dependent Functions.pdf}
}

@article{rademakerRewritingSemanticsSoftware2005,
  title = {A {{Rewriting Semantics}} for a {{Software Architecture Description Language}}},
  author = {Rademaker, Alexandre and Braga, Christiano and Sztajnberg, Alexandre},
  year = {2005},
  month = may,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the {{Brazilian Symposium}} on {{Formal Methods}} ({{SBMF}} 2004)},
  volume = {130},
  pages = {345--377},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2005.03.018},
  urldate = {2020-02-10},
  abstract = {Distributed and concurrent application invariably have coordination requirements. The design of those applications, composed by several (possibly distributed) components, has to consider coordination requirements comprising inter-component interaction styles, and intra-component concurrency and synchronization aspects. In our approach coordination aspects are treated in the software architecture level and can be specified in high-level contracts in CBabel ADL. A rewriting logic semantics for the software architecture description language CBabel is given, revisiting and extending previous work by some of the authors, which now includes a revision of the previous semantics and the addition of new features covering all the language. The CBabel tool is also presented. The CBabel tool is a prototype executable environment for CBabel, that implements the given CBabel's rewriting logic semantics and allows the execution and verification of CBabel descriptions in the Maude system, an implementation of rewriting logic. In this way, software architectures describing complex applications can be formally verified regarding properties such as deadlock and synchronization consistency in the software architecture design phase of its life cycle.},
  langid = {english},
  keywords = {contracts,Maude,rewriting logic,software architecture description languages},
  file = {/home/vipa/Zotero/storage/Q7INSQJA/Rademaker et al. - 2005 - A Rewriting Semantics for a Software Architecture .pdf;/home/vipa/Zotero/storage/S53WT9QZ/S157106610500229X.html}
}

@inproceedings{ramachandraHolisticOptimizationPrefetching2012,
  title = {Holistic Optimization by Prefetching Query Results},
  booktitle = {Proceedings of the 2012 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Ramachandra, Karthik and Sudarshan, S.},
  year = {2012},
  month = may,
  series = {{{SIGMOD}} '12},
  pages = {133--144},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2213836.2213852},
  urldate = {2020-11-20},
  abstract = {In this paper we address the problem of optimizing performance of database/web-service backed applications by means of automatically prefetching query results. Prefetching has been performed in earlier work based on predicting query access patterns; however such prediction is often of limited value, and can perform unnecessary prefetches. There has been some earlier work on program analysis and rewriting to automatically insert prefetch requests; however, such work has been restricted to rewriting of single procedures. In many cases, the query is in a procedure which does not offer much scope for prefetching within the procedure; in contrast, our approach can perform prefetching in a calling procedure, even when the actual query is in a called procedure, thereby greatly improving the benefits due to prefetching. Our approach does not perform any intrusive changes to the source code, and places prefetch instructions at the earliest possible points while avoiding wasteful prefetches. We have incorporated our techniques into a tool for holistic optimization called DBridge, to prefetch query results in Java programs that use JDBC. Our tool can be easily extended to handle Hibernate API calls as well as Web service requests. Our experiments on several real world applications demonstrate the applicability and significant performance gains due to our techniques.},
  isbn = {978-1-4503-1247-9},
  keywords = {holistic query optimization},
  file = {/home/vipa/Zotero/storage/YBMSXQ4E/Ramachandra and Sudarshan - 2012 - Holistic optimization by prefetching query results.pdf}
}

@inproceedings{ramachandraProgramAnalysisTransformation2012,
  title = {Program Analysis and Transformation for Holistic Optimization of Database Applications},
  booktitle = {Proceedings of the {{ACM SIGPLAN International Workshop}} on {{State}} of the {{Art}} in {{Java Program}} Analysis},
  author = {Ramachandra, Karthik and Guravannavar, Ravindra and Sudarshan, S.},
  year = {2012},
  month = jun,
  series = {{{SOAP}} '12},
  pages = {39--44},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2259051.2259057},
  urldate = {2020-11-20},
  abstract = {We describe DBridge, a novel program analysis and transformation tool to optimize database and web service access. Traditionally, rewrite of queries and programs are done independently, by the database query optimizer and the language compiler respectively, leaving out many optimization opportunities. Our tool aims to bridge this gap by performing holistic transformations, which include both program and query rewrite. There has been earlier research in this area involving program analysis and transformation for automatically rewriting database applications to perform optimizations; for example, our earlier work has addressed batching or asynchronous submission of iterative queries, and prefetching query results. DBridge implements these techniques for Java programs and internally uses Soot, a Java optimization framework, for static analysis and transformation. DBridge can perform such optimizations on Java programs that use the JDBC API to access the database. It is currently being extended to handle the Hibernate API, and Web Services. In this paper, we describe the program transformations that DBridge can perform. We then discuss the design and implementation of DBridge with a focus on how the Soot framework has been used to achieve these goals. Finally, we conclude by discussing some of the future directions for our tool.},
  isbn = {978-1-4503-1490-9},
  file = {/home/vipa/Zotero/storage/RJNBCZ82/Ramachandra et al. - 2012 - Program analysis and transformation for holistic o.pdf}
}

@article{redziejowskiAspectsParsingExpression2008,
  title = {Some {{Aspects}} of {{Parsing Expression Grammar}}},
  author = {Redziejowski, Roman R.},
  year = {2008},
  month = jan,
  journal = {Fundamenta Informaticae},
  volume = {85},
  number = {1-4},
  pages = {441--451},
  issn = {0169-2968},
  abstract = {Parsing Expression Grammar (PEG) is a new way to specify syntax, by means of a topdown processwith limited backtracking. It can be directly transcribed into a recursive-descent parser. The parser does not require a separate lexer, and backtracking removes the usual LL(1) constraint. This is convenient for many applications, but there are two problems: PEG is not well understood as a language specification tool, and backtracking may result in exponential processing time. The paper consists of two parts that address these problems. The first part is an attempt to find out the language actually defined by a given parsing expression. The second part reports measurements of backtracking activity in a PEG-derived parser for the programming language C.},
  file = {/home/vipa/Zotero/storage/7ZY87PXG/Redziejowski - 2008 - Some Aspects of Parsing Expression Grammar.pdf}
}

@inproceedings{rompfLightweightModularStaging2010,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  shorttitle = {Lightweight {{Modular Staging}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2010},
  series = {{{GPCE}} '10},
  pages = {127--136},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1868294.1868314},
  urldate = {2018-10-18},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  isbn = {978-1-4503-0154-1},
  keywords = {code generation,domain-specific languages,language virtualization,multi-stage programming},
  file = {/home/vipa/Zotero/storage/82G4CHYH/Rompf and Odersky - 2010 - Lightweight Modular Staging A Pragmatic Approach .pdf}
}

@article{rompfLightweightModularStaging2012,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  shorttitle = {Lightweight {{Modular Staging}}},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2012},
  month = jun,
  journal = {Commun. ACM},
  volume = {55},
  number = {6},
  pages = {121--130},
  issn = {0001-0782},
  doi = {10.1145/2184319.2184345},
  urldate = {2019-11-18},
  abstract = {Good software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources. Generative programming is a promising alternative to fully automatic translation. Instead of writing down the target program directly, developers write a program generator, which produces the target program as its output. The generator can be written in a high-level, generic style and can still produce efficient, specialized target programs. In practice, however, developing high-quality program generators requires a very large effort that is often hard to amortize. We present lightweight modular staging (LMS), a generative programming approach that lowers this effort significantly. LMS seamlessly combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework. LMS is well suited to develop embedded domain-specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.},
  file = {/home/vipa/Zotero/storage/DPBN8ZKB/Rompf and Odersky - 2012 - Lightweight Modular Staging A Pragmatic Approach .pdf}
}

@inproceedings{rompfOptimizingDataStructures2013,
  title = {Optimizing Data Structures in High-Level Programs: New Directions for Extensible Compilers Based on Staging},
  shorttitle = {Optimizing Data Structures in High-Level Programs},
  booktitle = {Proceedings of the 40th Annual {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Rompf, Tiark and Sujeeth, Arvind K. and Amin, Nada and Brown, Kevin J. and Jovanovic, Vojin and Lee, HyoukJoong and Jonnalagedda, Manohar and Olukotun, Kunle and Odersky, Martin},
  year = {2013},
  month = jan,
  series = {{{POPL}} '13},
  pages = {497--510},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2429069.2429128},
  urldate = {2020-11-20},
  abstract = {High level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user- or library-defined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts. Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops. We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments.},
  isbn = {978-1-4503-1832-7},
  keywords = {code generation,data structures,extensible compilers,staging},
  file = {/home/vipa/Zotero/storage/9ZHYFQ3Z/Rompf et al. - 2013 - Optimizing data structures in high-level programs.pdf}
}

@inproceedings{rompfReflectionsLMSExploring2016,
  title = {Reflections on {{LMS}}: {{Exploring Front-end Alternatives}}},
  shorttitle = {Reflections on {{LMS}}},
  booktitle = {Proceedings of the 2016 7th {{ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Rompf, Tiark},
  year = {2016},
  series = {{{SCALA}} 2016},
  pages = {41--50},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2998392.2998399},
  urldate = {2019-11-18},
  abstract = {Metaprogramming techniques to generate code at runtime in a general-purpose meta-language have seen a surge of interest in recent years, driven by the widening performance gap between high-level languages and emerging hardware platforms. In the context of Scala, the LMS (Lightweight Modular Staging) framework has contributed to ``abstraction without regret''--high-level programming without performance penalty--in a number of challenging domains, through runtime code generation and embedded compiler pipelines based on stacks of DSLs. Based on this experience, this paper crystallizes some of the design decisions of LMS and discusses potential alternatives, which maintain the underlying spirit but differ in implementation choices: specifically, strategies for realizing more flexible front-end embeddings using type classes instead of higher-kinded types, and strategies for type-safe metaprogramming with untyped intermediate representations.},
  isbn = {978-1-4503-4648-1},
  keywords = {domain-specific languages,intermediate representation,Multi-stage programming},
  file = {/home/vipa/Zotero/storage/KRNRLV6F/Rompf - 2016 - Reflections on LMS Exploring Front-end Alternativ.pdf}
}

@inproceedings{rosenkrantzPropertiesDeterministicTop1969,
  title = {Properties of Deterministic Top down Grammars},
  booktitle = {Proceedings of the First Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Rosenkrantz, D. J. and Stearns, R. E.},
  year = {1969},
  month = may,
  series = {{{STOC}} '69},
  pages = {165--180},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800169.805431},
  urldate = {2024-06-24},
  abstract = {The class of context free grammars that can be deterministically parsed in a top down manner with a fixed amount of look-ahead is investigated. These grammars, called LL(k) grammars where k is the amount of look-ahead are first defined and a procedure is given for determining if a context free grammar is LL(k) for a given value of k. It is shown that {$\varepsilon$}-rules can be eliminated from an LL(k) grammar, at the cost of increasing the value of k by one, and a description is given of a canonical pushdown machine for recognizing LL(k) languages. It is shown that for each value of k there are LL(k+l) languages that are not LL(k) languages. It is shown that the equivalence problem is decidable for LL(k) grammars. Additional properties are also given.},
  isbn = {978-1-4503-7478-1},
  file = {/home/vipa/Zotero/storage/FI945S87/Rosenkrantz and Stearns - 1969 - Properties of deterministic top down grammars.pdf}
}

@inproceedings{sakaiSyntaxUniversalTranslation1961,
  title = {Syntax in Universal Translation},
  booktitle = {Proceedings of the International Conference on Machine Translation and Applied Language Analysis},
  author = {Sakai, Itiroo},
  year = {1961},
  file = {/home/vipa/Zotero/storage/KL9Z4UE5/Sakai - 1961 - Syntax in universal translation.pdf}
}

@article{scherrAlmostFirstclassLanguage2015,
  title = {Almost First-Class Language Embedding: Taming Staged Embedded {{DSLs}}},
  shorttitle = {Almost First-Class Language Embedding},
  author = {Scherr, Maximilian and Chiba, Shigeru},
  year = {2015},
  month = oct,
  journal = {ACM SIGPLAN Notices},
  volume = {51},
  number = {3},
  pages = {21--30},
  issn = {0362-1340},
  doi = {10.1145/2936314.2814217},
  urldate = {2021-08-02},
  abstract = {Embedded domain-specific languages (EDSLs), inheriting a general-purpose language's features as well as look-and-feel, have traditionally been second-class or rather non-citizens in terms of host-language design. This makes sense when one regards them to be on the same level as traditional, non-EDSL library interfaces. However, this equivalence only applies to the simplest of EDSLs. In this paper we illustrate why this is detrimental when moving on to EDSLs that employ staging, i.e. program reification, by example of various issues that affect authors and users alike. We believe that if EDSLs are to be considered a reliable, language-like interface abstraction, they require exceptional attention and design scrutiny. Instead of unenforceable conventions, we advocate the acceptance of EDSLs as proper, i.e. almost first-class, citizens while retaining most advantages of pure embeddings. As a small step towards this goal, we present a pragmatic framework prototype for Java. It is based on annotations that explicate and document membership to explicit EDSL entities. In a nutshell, our framework identifies (annotated) method calls and field accesses as EDSL terms and dynamically constructs an abstract-syntax representation, which is eventually passed to a semantics-defining back end implemented by the EDSL author.},
  keywords = {design,embedded DSLs,implementation,Java,metaprogramming,program transformation,programming languages,staging},
  file = {/home/vipa/Zotero/storage/N4JNNSCQ/Scherr and Chiba - 2015 - Almost first-class language embedding taming stag.pdf}
}

@article{schillerCompileRuntimeApproaches2016,
  title = {Compile- and Run-Time Approaches for the Selection of Efficient Data Structures for Dynamic Graph Analysis},
  author = {Schiller, Benjamin and Deusser, Clemens and Castrillon, Jeronimo and Strufe, Thorsten},
  year = {2016},
  month = dec,
  journal = {Applied Network Science},
  volume = {1},
  number = {1},
  pages = {1--22},
  publisher = {SpringerOpen},
  issn = {2364-8228},
  doi = {10.1007/s41109-016-0011-2},
  urldate = {2024-01-06},
  abstract = {Graphs are used to model a wide range of systems from different disciplines including social network analysis, biology, and big data processing. When analyzing these constantly changing dynamic graphs at a high frequency, performance is the main concern. Depending on the graph size and structure, update frequency, and read accesses of the analysis, the use of different data structures can yield great performance variations. Even for expert programmers, it is not always obvious, which data structure is the best choice for a given scenario. In previous work, we presented an approach for handling the selection of the most efficient data structures automatically using a compile-time approach well-suited for constant workloads. We extend this work with a measurement study of seven data structures and use the results to fit actual cost estimation functions. In addition, we evaluate our approach for the computations of seven different graph metrics. In analyses of real-world dynamic graphs with a constant workload, our approach achieves a speedup of up to 5.4{\texttimes} compared to basic data structure configurations. Such a compile-time based approach cannot yield optimal results when the behavior of the system changes later and the workload becomes non-constant. To close this gap we present a run-time approach which provides live profiling and facilitates automatic exchanges of data structures during execution. We analyze the performance of this approach using an artificial, non-constant workload where our approach achieves speedups of up to 7.3{\texttimes} compared to basic configurations.},
  copyright = {2016 The Author(s)},
  langid = {english},
  file = {/home/vipa/Zotero/storage/TTAFBCSP/Schiller et al. - 2016 - Compile- and run-time approaches for the selection.pdf}
}

@inproceedings{schmitzConservativeAmbiguityDetection2007,
  title = {Conservative {{Ambiguity Detection}} in {{Context-Free Grammars}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Schmitz, Sylvain},
  editor = {Arge, Lars and Cachin, Christian and Jurdzi{\'n}ski, Tomasz and Tarlecki, Andrzej},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {692--703},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-73420-8_60},
  abstract = {The ability to detect ambiguities in context-free grammars is vital for their use in several fields, but the problem is undecidable in the general case. We present a safe, conservative approach, where the approximations cannot result in overlooked ambiguous cases . We analyze the complexity of the verification, and provide formal comparisons with several other ambiguity detection methods.},
  isbn = {978-3-540-73420-8},
  langid = {english},
  keywords = {Derivation Tree,Nondeterministic Automaton,Nonterminal Symbol,Position Graph,Regular Approximation},
  file = {/home/vipa/Zotero/storage/29B9ABA7/Schmitz - 2007 - Conservative Ambiguity Detection in Context-Free G.pdf}
}

@article{schonbergAutomaticTechniqueSelection1981,
  title = {An {{Automatic Technique}} for {{Selection}} of {{Data Representations}} in {{SETL Programs}}},
  author = {Schonberg, Edmond and Schwartz, Jacob T. and Sharir, Micha},
  year = {1981},
  month = apr,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {3},
  number = {2},
  pages = {126--143},
  issn = {0164-0925},
  doi = {10.1145/357133.357135},
  urldate = {2024-01-07},
  file = {/home/vipa/Zotero/storage/5EQ8K4H7/Schonberg et al. - 1981 - An Automatic Technique for Selection of Data Repre.pdf}
}

@inproceedings{schoneIncrementalRuntimegenerationOptimisation2016,
  title = {Incremental Runtime-Generation of Optimisation Problems Using {{RAG-controlled}} Rewriting},
  booktitle = {{{CEUR Workshop Proceedings}}},
  author = {Sch{\"o}ne, R. and G{\"o}tz, S. and A{\ss}mann, U. and B{\"u}rger, C.},
  year = {2016},
  volume = {1742},
  pages = {26--34},
  abstract = {In the era of Internet of Things, software systems need to interact with many physical entities and cope with new requirements at runtime. Self-Adaptive systems aim to tackle those challenges, often representing their context with a runtime model enabling better reasoning capabilities. However, those models quickly grow in size and need to be updated frequently with small changes due to a high number of physical entities changing constantly. This situation threatens the efficacy of analyses on such models, as they lack an efficient management of those changes leading to unnecessary computation overhead. We propose applying scalable, incremental change management of runtime models in the presence of a complex model to text transformation. In this paper, we present and evaluate an example of code generation of integer linear programs. In our case study using synthesized models, we saved 35 - 83\% processing time compared to a non-incremental approach. Using our approach, future self-Adaptive systems can handle and analyze large-scale runtime models, even if they change frequently.},
  file = {/home/vipa/Zotero/storage/SV3WVV56/Schöne et al. - 2016 - Incremental runtime-generation of optimisation pro.pdf;/home/vipa/Zotero/storage/JG2XD48G/display.html}
}

@inproceedings{schwerdfegerVerifiableCompositionDeterministic2009,
  title = {Verifiable {{Composition}} of {{Deterministic Grammars}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Schwerdfeger, August C. and Van Wyk, Eric R.},
  year = {2009},
  series = {{{PLDI}} '09},
  pages = {199--210},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1542476.1542499},
  urldate = {2019-07-04},
  abstract = {There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension.},
  isbn = {978-1-60558-392-1},
  keywords = {context-aware scanning,extensible languages,grammar composition,language composition,lr parsing},
  file = {/home/vipa/Zotero/storage/35TBU6E5/Schwerdfeger and Van Wyk - 2009 - Verifiable Composition of Deterministic Grammars.pdf}
}

@article{scottGLLParsing2010,
  title = {{{GLL Parsing}}},
  author = {Scott, Elizabeth and Johnstone, Adrian},
  year = {2010},
  month = sep,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)},
  volume = {253},
  number = {7},
  pages = {177--189},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2010.08.041},
  urldate = {2019-06-20},
  abstract = {Recursive Descent (RD) parsers are popular because their control flow follows the structure of the grammar and hence they are easy to write and to debug. However, the class of grammars which admit RD parsers is very limited. Backtracking techniques may be used to extend this class, but can have explosive runtimes and cannot deal with grammars with left recursion. Tomita-style RNGLR parsers are fully general but are based on LR techniques and do not have the direct relationship with the grammar that an RD parser has. We develop the fully general GLL parsing technique which is recursive descent-like, and has the property that the parse follows closely the structure of the grammar rules, but uses RNGLR-like machinery to handle non-determinism. The resulting recognisers run in worst-case cubic time and can be built even for left recursive grammars.},
  keywords = {context free languages,generalised parsing,recursive descent,RNGLR and RIGLR parsing},
  file = {/home/vipa/Zotero/storage/AV8LK8XI/Scott and Johnstone - 2010 - GLL Parsing.pdf;/home/vipa/Zotero/storage/Z9RNKQSY/S1571066110001209.html}
}

@article{scottSPPFStyleParsingEarley2008,
  title = {{{SPPF-Style Parsing From Earley Recognisers}}},
  author = {Scott, Elizabeth},
  year = {2008},
  month = apr,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the {{Seventh Workshop}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} 2007)},
  volume = {203},
  number = {2},
  pages = {53--67},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2008.03.044},
  urldate = {2019-11-01},
  abstract = {In its recogniser form, Earley's algorithm for testing whether a string can be derived from a grammar is worst case cubic on general context free grammars (CFG). Earley gave an outline of a method for turning his recognisers into parsers, but it turns out that this method is incorrect. Tomita's GLR parser returns a shared packed parse forest (SPPF) representation of all derivations of a given string from a given CFG but is worst case unbounded polynomial order. We have given a modified worst-case cubic version, the BRNGLR algorithm, that, for any string and any CFG, returns a binarised SPPF representation of all possible derivations of a given string. In this paper we apply similar techniques to develop two versions of an Earley parsing algorithm that, in worst-case cubic time, return an SPPF representation of all derivations of a given string from a given CFG.},
  langid = {english},
  keywords = {context free languages,cubic generalised parsing,Earley parsing},
  file = {/home/vipa/Zotero/storage/RQKLH2FZ/Scott - 2008 - SPPF-Style Parsing From Earley Recognisers.pdf;/home/vipa/Zotero/storage/DVJE2AF7/S1571066108001497.html}
}

@misc{senderovTreePPLUniversalProbabilistic2023,
  title = {{{TreePPL}}: {{A Universal Probabilistic Programming Language}} for {{Phylogenetics}}},
  shorttitle = {{{TreePPL}}},
  author = {Senderov, Viktor and Kudlicka, Jan and Lund{\'e}n, Daniel and Palmkvist, Viktor and Braga, Mariana P. and Granqvist, Emma and Broman, David and Ronquist, Fredrik},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.10.10.561673},
  publisher = {bioRxiv},
  doi = {10.1101/2023.10.10.561673},
  urldate = {2024-08-10},
  abstract = {We present TreePPL, a language for probabilistic modeling and inference in statistical phylogenetics. Specifically, TreePPL is a domain-specific universal probabilistic programming language (PPL), particularly designed for describing phylogenetic models. The core idea is to express the model as a computer program, which estimates the posterior probability distribution of interest when executed sufficiently many times. The program uses two special probabilistic constructs: assume statements, which describe latent random variables in the model, and observe statements, which condition random variables in the model on observed data. The assume and observe statements make it possible for generic inference algorithms, such as sequential Monte Carlo and Markov chain Monte Carlo algorithms, to identify checkpoints that enable them to generate and manipulate simulations from the posterior probability distribution. This means that a user can focus on describing the model, and leave the estimation of the posterior probability distribution to TreePPL's inference machinery. The TreePPL modeling language is inspired by R, Python, and the functional programming language OCaml. The model script can be conveniently run from a Python environment (an R environment is work in progress), which can be used for pre-processing, feeding the model with the observed data, controlling and running the inference, and receiving and post-processing the output data. The inference machinery is generated by a compiler framework developed specifically for supporting domain-specific modeling and inference, the Miking CorePPL framework. It currently supports a range of inference strategies, including several recent innovations that are important for efficient inference on phylogenetic models. It also supports the implementation of novel inference strategies for models described using TreePPL or other domain-specific modeling languages. We briefly describe the TreePPL modeling language and the Python environment, and give some examples of modeling and inference with TreePPL. The examples illustrate how TreePPL can be used to address a range of common problem types considered in statistical phylogenetics, from diversification and co-speciation analysis to tree inference. Although much progress has been made in recent years, developing efficient algorithms for automatic PPL-based inference is still a very active field. A few major challenges remain to be addressed before the entire phylogenetic model space is adequately covered by efficient automatic inference techniques, but several of them are being addressed in ongoing work on TreePPL. We end the paper by discussing how probabilistic programming can support the use of machine learning in designing and fine-tuning inference strategies and in extending incomplete model descriptions in phylogenetics.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@article{serranoQuickLookImpredicativity2020,
  title = {A Quick Look at Impredicativity},
  author = {Serrano, Alejandro and Hage, Jurriaan and Peyton Jones, Simon and Vytiniotis, Dimitrios},
  year = {2020},
  month = aug,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {ICFP},
  pages = {89:1--89:29},
  doi = {10.1145/3408971},
  urldate = {2021-09-03},
  abstract = {Type inference for parametric polymorphism is wildly successful, but has always suffered from an embarrassing flaw: polymorphic types are themselves not first class. We present Quick Look, a practical, implemented, and deployable design for impredicative type inference. To demonstrate our claims, we have modified GHC, a production-quality Haskell compiler, to support impredicativity. The changes required are modest, localised, and are fully compatible with GHC's myriad other type system extensions.},
  keywords = {constraint-based inference,impredicative polymorphism,Type systems},
  file = {/home/vipa/Zotero/storage/GGSK35EL/Serrano et al. - 2020 - A quick look at impredicativity.pdf}
}

@inproceedings{sestoftMLPatternMatch1996,
  title = {{{ML}} Pattern Match Compilation and Partial Evaluation},
  booktitle = {Partial {{Evaluation}}},
  author = {Sestoft, Peter},
  editor = {Danvy, Olivier and Gl{\"u}ck, Robert and Thiemann, Peter},
  year = {1996},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {446--464},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-61580-6_22},
  abstract = {We derive a compiler for ML-style pattern matches. It is conceptually simple and produces reasonably good compiled matches. The derivation is inspired by the instrumentation and partial evaluation of na{\"I}ve string matchers. Following that paradigm, we first present a general and na{\"I}ve ML pattern matcher, instrument it to collect and exploit extra information, and show that partial evaluation of the instrumented general matcher with respect to a given match produces an efficient specialized matcher.We then discard the partial evaluator and show that a match compiler can be obtained just by slightly modifying the instrumented general matcher. The resulting match compiler is interesting in its own right, and naturally detects inexhaustive matches and redundant match rules.},
  isbn = {978-3-540-70589-5},
  langid = {english},
  keywords = {Match Rule,Negative Information,Partial Evaluation,Pattern Match,Term Description},
  file = {/home/vipa/Zotero/storage/IHK64NW9/Sestoft - 1996 - ML pattern match compilation and partial evaluatio.pdf}
}

@inproceedings{shachamChameleonAdaptiveSelection2009,
  title = {Chameleon: Adaptive Selection of Collections},
  shorttitle = {Chameleon},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Shacham, Ohad and Vechev, Martin and Yahav, Eran},
  year = {2009},
  month = jun,
  series = {{{PLDI}} '09},
  pages = {408--418},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1542476.1542522},
  urldate = {2022-11-15},
  abstract = {Languages such as Java and C\#, as well as scripting languages like Python, and Ruby, make extensive use of Collection classes. A collection implementation represents a fixed choice in the dimensions of operation time, space utilization, and synchronization. Using the collection in a manner not consistent with this fixed choice can cause significant performance degradation. In this paper, we present CHAMELEON, a low-overhead automatic tool that assists the programmer in choosing the appropriate collection implementation for her application. During program execution, CHAMELEON computes elaborate trace and heap-based metrics on collection behavior. These metrics are consumed on-thefly by a rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply these corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON on top of a IBM's J9 production JVM, and evaluated it over a small set of benchmarks. We show that for some applications, using CHAMELEON leads to a significant improvement of the memory footprint of the application.},
  isbn = {978-1-60558-392-1},
  keywords = {bloat,collections,java,semantic profiler},
  file = {/home/vipa/Zotero/storage/LMKY653E/Shacham et al. - 2009 - Chameleon adaptive selection of collections.pdf}
}

@inproceedings{shaikhhaFineTuningDataStructures2023,
  title = {Fine-{{Tuning Data Structures}} for {{Query Processing}}},
  booktitle = {Proceedings of the 21st {{ACM}}/{{IEEE International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Shaikhha, Amir and Kelepeshis, Marios and Ghorbani, Mahdi},
  year = {2023},
  month = feb,
  series = {{{CGO}} 2023},
  pages = {149--161},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3579990.3580016},
  urldate = {2024-01-07},
  abstract = {We introduce a framework for automatically choosing data structures for efficient query processing. Our contributions are twofold. First, we introduce a novel low-level intermediate language that can express the algorithms behind various query processing paradigms such as classical joins, groupjoin, and in-database machine learning engines. This language is designed around the notion of dictionaries and allows for a more fine-grained choice of its low-level implementation. Second, the cost model for alternative implementations is automatically inferred by combining machine learning and program reasoning. The dictionary cost model is learned using a regression model trained over the profiling data of dictionary operations on a given architecture. Program reasoning helps to infer the expected cost of the whole query by combining the learned dictionary cost estimates. Our experimental results show the effectiveness of the trained cost model on microbenchmarks. Furthermore, we show that the code generated by our framework outperforms or is competitive with state-of-the-art analytical query and in-database machine learning engines.},
  isbn = {9798400701016},
  keywords = {Data-Structure Selection,Dictionary},
  file = {/home/vipa/Zotero/storage/2XVNUHNA/Shaikhha et al. - 2023 - Fine-Tuning Data Structures for Query Processing.pdf}
}

@article{shaikhhaFunctionalCollectionProgramming2022,
  title = {Functional Collection Programming with Semi-Ring Dictionaries},
  author = {Shaikhha, Amir and Huot, Mathieu and Smith, Jaclyn and Olteanu, Dan},
  year = {2022},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {OOPSLA1},
  pages = {89:1--89:33},
  doi = {10.1145/3527333},
  urldate = {2023-05-26},
  abstract = {This paper introduces semi-ring dictionaries, a powerful class of compositional and purely functional collections that subsume other collection types such as sets, multisets, arrays, vectors, and matrices. We developed SDQL, a statically typed language that can express relational algebra with aggregations, linear algebra, and functional collections over data such as relations and matrices using semi-ring dictionaries. Furthermore, thanks to the algebraic structure behind these dictionaries, SDQL unifies a wide range of optimizations commonly used in databases (DB) and linear algebra (LA). As a result, SDQL enables efficient processing of hybrid DB and LA workloads, by putting together optimizations that are otherwise confined to either DB systems or LA frameworks. We show experimentally that a handful of DB and LA workloads can take advantage of the SDQL language and optimizations. SDQL can be competitive with or outperforms a host of systems that are state of the art in their own domain: in-memory DB systems Typer and Tectorwise for (flat, not nested) relational data; SciPy for LA workloads; sparse tensor compiler taco; the Trance nested relational engine; and the in-database machine learning engines LMFAO and Morpheus for hybrid DB/LA workloads over relational data.},
  keywords = {Nested Relational Algebra,Semi-Ring Dictionary,Sparse Linear Algebra},
  file = {/home/vipa/Zotero/storage/3J96PHY5/Shaikhha et al. - 2022 - Functional collection programming with semi-ring d.pdf}
}

@inproceedings{sheardTemplateMetaprogrammingHaskell2002,
  title = {Template {{Meta-programming}} for {{Haskell}}},
  booktitle = {Proceedings of the 2002 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  author = {Sheard, Tim and Jones, Simon Peyton},
  year = {2002},
  series = {Haskell '02},
  pages = {1--16},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/581690.581691},
  urldate = {2018-10-18},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
  isbn = {978-1-58113-605-0},
  keywords = {meta programming,templates},
  file = {/home/vipa/Zotero/storage/LK92M7E7/Sheard and Jones - 2002 - Template Meta-programming for Haskell.pdf}
}

@article{sherwoodDiscoveringExploitingProgram2003,
  title = {Discovering and Exploiting Program Phases},
  author = {Sherwood, T. and Perelman, E. and Hamerly, G. and Sair, S. and Calder, B.},
  year = {2003},
  month = nov,
  journal = {IEEE Micro},
  volume = {23},
  number = {6},
  pages = {84--93},
  issn = {1937-4143},
  doi = {10.1109/MM.2003.1261391},
  abstract = {Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the largest of scales (that is, over the program's complete execution). During one part of the execution, a program can be completely memory bound; in another, it can repeatedly stall on branch mispredicts. Average statistics gathered about a program might not accurately picture where the real problems lie. This realization has ramifications for many architecture and compiler techniques, from how to best schedule threads on a multithreaded machine, to feedback-directed optimizations, power management, and the simulation and test of architectures. Taking advantage of time-varying behavior requires a set of automated analytic tools and hardware techniques that can discover similarities and changes in program behavior on the largest of time scales. The challenge in building such tools is that during a program's lifetime it can execute billions or trillions of instructions. How can high-level behavior be extracted from this sea of instructions? Some programs change behavior drastically, switching between periods of high and low performance, yet system design and optimization typically focus on average system behavior. It is argued that instead of assuming average behavior, it is now time to model and optimize phase-based program behavior.},
  keywords = {Buildings,Computer architecture,Design optimization,Energy management,Hardware,Optimizing compilers,Random access memory,Statistics,Testing},
  file = {/home/vipa/Zotero/storage/8JZR6GFX/Sherwood et al. - 2003 - Discovering and exploiting program phases.pdf;/home/vipa/Zotero/storage/VL4DTFJQ/1261391.html}
}

@inproceedings{silkensenWellTypedIslandsParse2013,
  title = {Well-{{Typed Islands Parse Faster}}},
  booktitle = {Trends in {{Functional Programming}}},
  author = {Silkensen, Erik and Siek, Jeremy},
  editor = {Loidl, Hans-Wolfgang and Pe{\~n}a, Ricardo},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {69--84},
  publisher = {Springer Berlin Heidelberg},
  abstract = {This paper addresses the problem of specifying and parsing the syntax of domain-specific languages (DSLs) in a modular, user-friendly way. We want to enable the design of composable DSLs that combine the natural syntax of external DSLs with the easy implementation of internal DSLs. The challenge in parsing these DSLs is that the composition of several languages is likely to contain ambiguities. We present the design of a system that uses a type-oriented variant of island parsing to efficiently parse the syntax of composable DSLs. In particular, we argue that the running time of type-oriented island parsing doesn't depend on the number of DSLs imported. We also show how to use our tool to implement DSLs on top of a host language such as Typed Racket.},
  isbn = {978-3-642-40447-4},
  langid = {english},
  keywords = {Concrete Syntax,Deductive System,Grammar Rule,Input String,Parse Tree},
  file = {/home/vipa/Zotero/storage/6NZRH6B6/Silkensen and Siek - 2013 - Well-Typed Islands Parse Faster.pdf}
}

@article{smallboneQuickSpecificationsBusy2017,
  title = {Quick Specifications for the Busy Programmer},
  author = {Smallbone, Nicholas and Johansson, Moa and Claessen, Koen and Algehed, Maximilian},
  year = {2017},
  month = jan,
  journal = {Journal of Functional Programming},
  volume = {27},
  pages = {e18},
  publisher = {Cambridge University Press},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796817000090},
  urldate = {2024-01-16},
  abstract = {QuickSpec is a theory exploration system which tests a Haskell program to find equational properties of it, automatically. The equations can be used to help understand the program, or as lemmas to help prove the program correct. QuickSpec is largely automatic: the user just supplies the functions to be tested and QuickCheck data generators. Previous theory exploration systems, including earlier versions of QuickSpec itself, scaled poorly. This paper describes a new architecture for theory exploration with which we can find vastly more complex laws than before, and much faster. We demonstrate theory exploration in QuickSpec on problems both from functional programming and mathematics.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/TP9KKRPM/Smallbone et al. - 2017 - Quick specifications for the busy programmer.pdf}
}

@inproceedings{stansiferRomeo2014,
  title = {Romeo},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming - {{ICFP}} '14},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2014},
  volume = {49},
  pages = {53--65},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/2628136.2628162},
  abstract = {Current languages for safely manipulating values with names only support term languages with simple binding syntax. As a result, no tools exist to safely manipulate code written in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m, but is a full-fledged binding-safe language like Pure FreshML.},
  isbn = {978-1-4503-2873-9}
}

@article{stansiferRomeoSystemMore2016,
  title = {Romeo: {{A}} System for More Flexible Binding-Safe Programming*},
  shorttitle = {Romeo},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2016/ed},
  journal = {Journal of Functional Programming},
  volume = {26},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796816000137},
  urldate = {2018-10-18},
  abstract = {Current systems for safely manipulating values containing names only support simple binding structures for those names. As a result, few tools exist to safely manipulate code in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m , but is a full-fledged binding-safe language like Pure FreshML.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/EFS37BVK/Stansifer and Wand - 2016 - Romeo A system for more flexible binding-safe pro.pdf;/home/vipa/Zotero/storage/QGXXGJKH/F9C128D1D92B6BCD489E0FE41D255AE0.html}
}

@inproceedings{steeleOverviewCOMMONLISP1982,
  title = {An {{Overview}} of {{COMMON LISP}}},
  booktitle = {Proceedings of the 1982 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  author = {Steele, Jr., Guy L.},
  year = {1982},
  series = {{{LFP}} '82},
  pages = {98--107},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/800068.802140},
  urldate = {2018-10-18},
  abstract = {A dialect of LISP called ``COMMON LISP'' is being cooperatively developed and implemented at several sites. It is a descendant of the MACLISP family of LISP dialects, and is intended to unify the several divergent efforts of the last five years. We first give an extensive history of LISP, particularly of the MACLISP branch, in order to explain in context the motivation for COMMON LISP. We enumerate the goals and non-goals of the language design, discuss the language features of primary interest, and then consider how these features help to meet the expressed goals. Finally, the status (as of May 1982) of six implementations of COMMON LISP is summarized.},
  isbn = {978-0-89791-082-8},
  file = {/home/vipa/Zotero/storage/3T8T9C8N/Steele - 1982 - An Overview of COMMON LISP.pdf}
}

@book{sudkampLanguagesMachinesIntroduction1997,
  title = {Languages and {{Machines}}: {{An Introduction}} to the {{Theory}} of {{Computer Science}}},
  shorttitle = {Languages and {{Machines}}},
  author = {Sudkamp, Thomas A.},
  year = {1997},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {Boston, MA, USA},
  isbn = {978-0-201-82136-9}
}

@article{sujeethDeliteCompilerArchitecture2014,
  title = {Delite: {{A Compiler Architecture}} for {{Performance-Oriented Embedded Domain-Specific Languages}}},
  shorttitle = {Delite},
  author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle},
  year = {2014},
  month = apr,
  journal = {ACM Trans. Embed. Comput. Syst.},
  volume = {13},
  number = {4s},
  pages = {134:1--134:25},
  issn = {1539-9087},
  doi = {10.1145/2584665},
  urldate = {2018-10-18},
  abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., OpenMP for CMPs, CUDA for GPUs, MPI for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages (DSLs) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because DSLs have higher-level semantics and restrictions than general-purpose languages, so DSL compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented DSLs is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the DSLs that have been developed with it. Delite simplifies the process of DSL development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in DSL implementations. Delite DSLs are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation (IR) of user programs and compile to multiple languages (including C++, CUDA, and OpenCL). DSL programs are automatically parallelized and different parts of the application can run simultaneously on CPUs and GPUs. We present Delite DSLs for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
  keywords = {code generation,Domain-specific languages,language virtualization,multistage programming},
  file = {/home/vipa/Zotero/storage/STUG7GZL/Sujeeth et al. - 2014 - Delite A Compiler Architecture for Performance-Or.pdf}
}

@inproceedings{tangGenericFlowsensitiveOptimizing2010,
  title = {Generic Flow-Sensitive Optimizing Transformations in {{C}}++ with Concepts},
  booktitle = {Proceedings of the 2010 {{ACM Symposium}} on {{Applied Computing}}},
  author = {Tang, Xiaolong and J{\"a}rvi, Jaakko},
  year = {2010},
  month = mar,
  series = {{{SAC}} '10},
  pages = {2111--2118},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1774088.1774532},
  urldate = {2020-11-20},
  abstract = {Compilers are typically hardwired to attempt optimizations only on expressions involving particular built-in types. Ideally, an optimizing compiler can recognize a rewrite opportunity whenever the operands in an expression satisfy the (algebraic) properties that justify the rewrite. This paper applies the principles and techniques of generic programming and the planned "concepts" language feature of C++ to approximate this ideal. Concretely, a concept defines the signature and algebraic laws of a class of types. We attach rewrite rules to a concept, so that the rules apply to all types in the class defined by a concept. The annotation burden to a programmer is thus small---the declaration that a type models a particular concept is simultaneously taken as an annotation that enables optimizations. To increase the applicability of generic rewrite rules, we instantiate them to type-specific rules, enabling the use of data-flow information from the compiler's existing analyses, and interleave the application of rewrite rules with function inlining. Our prototype is implemented as an extension of the ConceptGCC compiler; our experiments show the approach is effective in eliminating abstraction penalties.},
  isbn = {978-1-60558-639-7},
  keywords = {C++,concepts,high-level optimizations,rewriting},
  file = {/home/vipa/Zotero/storage/TRANL98R/Tang and Järvi - 2010 - Generic flow-sensitive optimizing transformations .pdf}
}

@article{tasosReshapeYourLayouts2020,
  title = {Reshape Your Layouts, Not Your Programs: {{A}} Safe Language Extension for Better Cache Locality},
  shorttitle = {Reshape Your Layouts, Not Your Programs},
  author = {Tasos, Alexandros and Franco, Juliana and Drossopoulou, Sophia and Wrigstad, Tobias and Eisenbach, Susan},
  year = {2020},
  month = oct,
  journal = {Science of Computer Programming},
  volume = {197},
  pages = {102481},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2020.102481},
  urldate = {2024-01-06},
  abstract = {The vast divide between the speed of CPU and RAM means that effective use of CPU caches is often a prerequisite for high performance on modern architectures. Hence, developers need to consider how to place data in memory so as to exploit spatial locality and achieve high memory bandwidth. Such manual memory optimisations are common in unmanaged languages (e.g. C, C++), but they sacrifice readability, maintainability, memory safety, and object abstraction. In managed languages, such as Java and C\#, where the runtime abstracts away the memory from the developer, such optimisations are almost impossible. We present a language extension called SHAPES, which aims to offer developers more fine-grained control over the placement of data, without sacrificing memory safety or object abstraction. In SHAPES, programmers group related objects into pools, and specify how objects are laid out in these pools. Classes and types are annotated by pool parameters, which allow placement aspects to be changed orthogonally to the code that operates on the objects in the pool. These design decisions disentangle business logic and memory concerns. We give a formal model of SHAPES, present its type and memory safety model, and present its translation to a low-level language. We argue why we expect this translation to be efficient in terms of runtime representation of objects and access to their fields. We argue that SHAPES can be incorporated into existing managed and unmanaged language runtimes and fit well with garbage collection.},
  keywords = {Cache utilisation,Data representation,Memory safety,Type systems},
  file = {/home/vipa/Zotero/storage/RIJDYIKG/Tasos et al. - 2020 - Reshape your layouts, not your programs A safe la.pdf;/home/vipa/Zotero/storage/H9Y9NAWA/S0167642320300915.html}
}

@inproceedings{tateEqualitySaturationNew2009,
  title = {Equality Saturation: A New Approach to Optimization},
  shorttitle = {Equality Saturation},
  booktitle = {Proceedings of the 36th Annual {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  year = {2009},
  month = jan,
  series = {{{POPL}} '09},
  pages = {264--276},
  publisher = {Association for Computing Machinery},
  address = {Savannah, GA, USA},
  doi = {10.1145/1480881.1480915},
  urldate = {2020-02-17},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  isbn = {978-1-60558-379-2},
  keywords = {compiler optimization,equality reasoning,intermediate representation},
  file = {/home/vipa/Zotero/storage/29PCGI6B/Tate et al. - 2009 - Equality saturation a new approach to optimizatio.pdf}
}

@misc{teamCoqProofAssistant2022,
  title = {The {{Coq Proof Assistant}}},
  author = {Team, The Coq Development},
  year = {2022},
  month = jan,
  doi = {10.5281/zenodo.5846982},
  howpublished = {Zenodo}
}

@article{thorupControlledGrammaticAmbiguity1994,
  title = {Controlled Grammatic Ambiguity},
  author = {Thorup, Mikkel},
  year = {1994},
  month = may,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {16},
  number = {3},
  pages = {1024--1050},
  issn = {0164-0925},
  doi = {10.1145/177492.177759},
  urldate = {2020-11-06},
  abstract = {A new approach to ambiguity of context-free grammars is presented, and within this approach the LL and LR techniques are generalized to solve the following problems for large classes of ambiguous grammars: The user may control the parser generation so as to get a parser which finds some specific parse trees for the sentences. The generalized LL and LR techniques will still guarantee that the resulting parser accepts all sentences and terminates in linear time on all input.},
  keywords = {grammatic ambiguity,semantic unambiguity},
  file = {/home/vipa/Zotero/storage/VJZJHC2G/Thorup - 1994 - Controlled grammatic ambiguity.pdf}
}

@misc{tlc,
  title = {The {{TLC}} Coq Library},
  author = {Chargu{\'e}raud, Arthur},
  year = {2022},
  publisher = {GitHub}
}

@article{tobin-hochstadtExtensiblePatternMatching2011,
  title = {Extensible {{Pattern Matching}} in an {{Extensible Language}}},
  author = {{Tobin-Hochstadt}, Sam},
  year = {2011},
  month = jun,
  urldate = {2018-10-18},
  langid = {english},
  keywords = {Computer Science - Programming Languages},
  file = {/home/vipa/Zotero/storage/3NCFYXR8/Tobin-Hochstadt - 2011 - Extensible Pattern Matching in an Extensible Langu.pdf;/home/vipa/Zotero/storage/MW6J4GDP/1106.html}
}

@inproceedings{tobin-hochstadtLanguagesLibraries2011,
  title = {Languages {{As Libraries}}},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {{Tobin-Hochstadt}, Sam and {St-Amour}, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
  year = {2011},
  series = {{{PLDI}} '11},
  pages = {132--141},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1993498.1993514},
  urldate = {2018-10-18},
  abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's string-based macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.  The design of Racket---a descendant of Scheme---goes even further with the introduction of a full-fledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from "native" notation, large and complex embedded domain-specific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sub-languages for programming with first-class classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language. This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a type-based optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation.},
  isbn = {978-1-4503-0663-8},
  keywords = {extensible languages,macros,modules,typed racket},
  file = {/home/vipa/Zotero/storage/6JIYM55T/Tobin-Hochstadt et al. - 2011 - Languages As Libraries.pdf}
}

@inproceedings{tomitaEfficientContextfreeParsing1985,
  title = {An Efficient Context-Free Parsing Algorithm for Natural Languages},
  booktitle = {Proceedings of the 9th International Joint Conference on {{Artificial}} Intelligence - {{Volume}} 2},
  author = {Tomita, Masaru},
  year = {1985},
  month = aug,
  series = {{{IJCAI}}'85},
  pages = {756--764},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2020-11-08},
  abstract = {This paper introduces an efficient context-free parsing algorithm and emphasizes its practical value in natural language processing. The algorithm can be viewed as an extended LR parsing algorithm which embodies the concept of a "graph-structured stack." Unlike the standard LR, the algorithm is capable of handling arbitrary non cyclic context-free grammars including ambiguous grammars, while most of the LR parsing efficiency is preserved. The algorithm seems more efficient than any existing algorithms including the Cocke Younger Kasami algorithm and Earley's algorithm, as far as practical natural language parsing is concerned, due to utilization of LR parsing tables. The algorithm is an all-path parsing algorithm; it produces all possible parse trees (a parse forest) in an efficient representation called a "shared-packed forest." This paper also shows that Earley's forest representation has a defect and his algorithm cannot be used in natural language processing as an all-path parsing algorithm.},
  isbn = {978-0-934613-02-6}
}

@book{tomitaEfficientParsingNatural2013,
  title = {Efficient {{Parsing}} for {{Natural Language}}: {{A Fast Algorithm}} for {{Practical Systems}}},
  shorttitle = {Efficient {{Parsing}} for {{Natural Language}}},
  author = {Tomita, Masaru},
  year = {2013},
  month = apr,
  publisher = {Springer Science \& Business Media},
  abstract = {Parsing Efficiency is crucial when building practical natural language systems. 'Ibis is especially the case for interactive systems such as natural language database access, interfaces to expert systems and interactive machine translation. Despite its importance, parsing efficiency has received little attention in the area of natural language processing. In the areas of compiler design and theoretical computer science, on the other hand, parsing algorithms 3 have been evaluated primarily in terms of the theoretical worst case analysis (e.g. lXn>>, and very few practical comparisons have been made. This book introduces a context-free parsing algorithm that parses natural language more efficiently than any other existing parsing algorithms in practice. Its feasibility for use in practical systems is being proven in its application to Japanese language interface at Carnegie Group Inc., and to the continuous speech recognition project at Carnegie-Mellon University. This work was done while I was pursuing a Ph.D degree at Carnegie-Mellon University. My advisers, Herb Simon and Jaime Carbonell, deserve many thanks for their unfailing support, advice and encouragement during my graduate studies. I would like to thank Phil Hayes and Ralph Grishman for their helpful comments and criticism that in many ways improved the quality of this book. I wish also to thank Steven Brooks for insightful comments on theoretical aspects of the book (chapter 4, appendices A, B and C), and Rich Thomason for improving the linguistic part of tile book (the very beginning of section 1.1).},
  googlebooks = {DAjkBwAAQBAJ},
  isbn = {978-1-4757-1885-0},
  langid = {english},
  keywords = {Computers / Information Technology,Computers / Intelligence (AI) & Semantics,Computers / Natural Language Processing,Language Arts & Disciplines / Linguistics / General}
}

@misc{TransformingOurWorld2015,
  title = {Transforming Our {{World}}: {{The}} 2030 {{Agenda}} for {{Sustainable Development}}},
  year = {2015},
  publisher = {United Nations}
}

@inproceedings{tristanEvaluatingValuegraphTranslation2011,
  title = {Evaluating Value-Graph Translation Validation for {{LLVM}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Tristan, Jean-Baptiste and Govereau, Paul and Morrisett, Greg},
  year = {2011},
  month = jun,
  series = {{{PLDI}} '11},
  pages = {295--305},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1993498.1993533},
  urldate = {2020-11-20},
  abstract = {Translation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the value-graphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intra-procedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs.},
  isbn = {978-1-4503-0663-8},
  keywords = {llvm,optimization,symbolic evaluation,translation validation},
  file = {/home/vipa/Zotero/storage/5GU53GHL/Tristan et al. - 2011 - Evaluating value-graph translation validation for .pdf}
}

@inproceedings{urecheAutomatingAdHoc2015,
  title = {Automating Ad Hoc Data Representation Transformations},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object-Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Ureche, Vlad and Biboudis, Aggelos and Smaragdakis, Yannis and Odersky, Martin},
  year = {2015},
  month = oct,
  series = {{{OOPSLA}} 2015},
  pages = {801--820},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2814270.2814271},
  urldate = {2024-01-06},
  abstract = {To maximize run-time performance, programmers often specialize their code by hand, replacing library collections and containers by custom objects in which data is restructured for efficient access. However, changing the data representation is a tedious and error-prone process that makes it hard to test, maintain and evolve the source code. We present an automated and composable mechanism that allows programmers to safely change the data representation in delimited scopes containing anything from expressions to entire class definitions. To achieve this, programmers define a transformation and our mechanism automatically and transparently applies it during compilation, eliminating the need to manually change the source code. Our technique leverages the type system in order to offer correctness guarantees on the transformation and its interaction with object-oriented language features, such as dynamic dispatch, inheritance and generics. We have embedded this technique in a Scala compiler plugin and used it in four very different transformations, ranging from improving the data layout and encoding, to retrofitting specialization and value class status, and all the way to collection deforestation. On our benchmarks, the technique obtained speedups between 1.8x and 24.5x.},
  isbn = {978-1-4503-3689-5},
  keywords = {bytecode,compatibility,data representation,jvm,optimization,safety,semantics,transformation},
  file = {/home/vipa/Zotero/storage/YNMNYINY/Ureche et al. - 2015 - Automating ad hoc data representation transformati.pdf}
}

@phdthesis{urecheCompileTimeTypeDrivenData2015,
  title = {Compile-{{Time Type-Driven Data Representation Transformations}} in {{Object-Oriented Languages}}},
  author = {Ureche, Vlad},
  year = {2015},
  address = {Lausanne},
  doi = {10.5075/epfl-thesis-6850},
  abstract = {High-level languages allow programmers to express data structures and algorithms that abstract over the type of data they handle. This improves code reuse and makes it possible to develop general-purpose libraries. Yet, data abstractions slow down program execution, as they require low-level indirection. In this thesis we explore three compile-time approaches that leverage type systems to reduce the cost of data abstractions, thus improving program performance. In the first part of the thesis we present miniboxing, a compile-time transformation that replaces generic classes by more efficient variants, optimized to handle primitive types. These variants use the miniboxed data encoding, producing speedups of up to 20x compared to generic classes. The miniboxing transformation is the main result of this thesis and motivates the other techniques. Generalizing miniboxing, we show the Late Data Layout (LDL) mechanism, which uses the type system to guide performance-oriented program rewritings. It can be instantiated to perform a host of transformations, such as miniboxing generics, inlining value classes and unboxing primitive types. The LDL mechanism has many desirable properties, such as provable correctness in handling different data representations, reduced number of conversions and built-in support for the object-oriented paradigm. Finally, we show Data-centric Metaprogramming, a technique that allows programmers to go beyond standard compiler optimizations by defining custom representations to be used for their data. These representations are then automatically introduced by the compiler when translating programs. This technique, similar in spirit to metaprogramming, opens new directions in programmer-driven optimizations and shows encouraging results, with speedups of up to 25x. Under the hood, Data-centric Metaprogramming uses the Late Data Layout mechanism},
  langid = {english},
  school = {EPFL},
  keywords = {Bytecode,Data Representation,Generics,Java,Java Virtual Machine,Object-Oriented,Performance,Specialization,Static Type System,Transformation},
  file = {/home/vipa/Zotero/storage/Y7L3YHEC/Ureche - 2015 - Compile-Time Type-Driven Data Representation Trans.pdf}
}

@inproceedings{vanderrestLanguageDefiningReusable2022,
  title = {Towards a~{{Language}} for {{Defining Reusable Programming Language Components}}},
  booktitle = {Trends in {{Functional Programming}}},
  author = {{van der Rest}, Cas and Poulsen, Casper Bach},
  editor = {Swierstra, Wouter and Wu, Nicolas},
  year = {2022},
  pages = {18--38},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-21314-4_2},
  abstract = {Developing programming languages is a difficult task that requires a lot of time, effort, and expertise. Reusable programming language components make this task easier, by allowing language designers to grab off-the-shelf components for common language features. Modern functional programming languages, however, lack support for reuse of definitions, and thus language components defined using algebraic data types and pattern matching functions cannot be reused without modifying or copying existing code. To improve the situation, we introduce CS, a functional meta-language for developing reusable programming language components, that features built-in support for extensible data types and functions, as well as effects and handlers. In CS, we can define language components using algebraic data types and pattern matching functions, such that we can compose language components into larger languages and define new interpretations for existing components without modifying existing definitions.},
  isbn = {978-3-031-21314-4},
  langid = {english},
  file = {/home/vipa/Zotero/storage/IC2UMDSL/van der Rest and Poulsen - 2022 - Towards a Language for Defining Reusable Programmi.pdf}
}

@article{vanputLANCETNiftyCode2005,
  title = {{{LANCET}}: A Nifty Code Editing Tool},
  shorttitle = {{{LANCET}}},
  author = {Van Put, Ludo and De Sutter, Bjorn and Madou, Matias and De Bus, Bruno and Chanet, Dominique and Smits, Kristof and De Bosschere, Koen},
  year = {2005},
  month = sep,
  journal = {ACM SIGSOFT Software Engineering Notes},
  volume = {31},
  number = {1},
  pages = {75--81},
  issn = {0163-5948},
  doi = {10.1145/1108768.1108812},
  urldate = {2020-11-27},
  abstract = {This paper presents LANCET, a multi-platform software visualization tool that enables the inspection of programs at the binary code level. Implemented on top of the link-time rewriting framework DIABLO, LANCET provides several views on the interprocedural control flow graph of a program. These views can be used to navigate through the program, to edit the program in a efficient manner, and to interact with the existing whole-program analyses and optimizations that are implemented in DIABLO or existing applications of DIABLO. As such, LANCET is an ideal tool to examine compiler-generated code, to assist the development of new compiler optimizations, or to optimize assembly code manually.},
  keywords = {assembler,binary code,instrumentation,optimization,visualization},
  file = {/home/vipa/Zotero/storage/A92L3Q2K/Van Put et al. - 2005 - LANCET a nifty code editing tool.pdf}
}

@article{vantangTighterBoundDeterminization2009,
  title = {A {{Tighter Bound}} for the {{Determinization}} of {{Visibly Pushdown Automata}}},
  author = {Van Tang, Nguyen},
  year = {2009},
  month = nov,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {10},
  eprint = {0911.3275},
  pages = {62--76},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.10.5},
  urldate = {2019-05-09},
  abstract = {Visibly pushdown automata (VPA), introduced by Alur and Madhusuan in 2004, is a subclass of pushdown automata whose stack behavior is completely determined by the input symbol according to a fixed partition of the input alphabet. Since its introduce, VPAs have been shown to be useful in various context, e.g., as specification formalism for verification and as automaton model for processing XML streams. Due to high complexity, however, implementation of formal verification based on VPA framework is a challenge. In this paper we consider the problem of implementing VPA-based model checking algorithms. For doing so, we first present an improvement on upper bound for determinization of VPA. Next, we propose simple on-the-fly algorithms to check universality and inclusion problems of this automata class. Then, we implement the proposed algorithms in a prototype tool. Finally, we conduct experiments on randomly generated VPAs. The experimental results show that the proposed algorithms are considerably faster than the standard ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Formal Languages and Automata Theory,Computer Science - Logic in Computer Science,F.4.1,F.4.3},
  file = {/home/vipa/Zotero/storage/PMPSTZL9/Van Tang - 2009 - A Tighter Bound for the Determinization of Visibly.pdf;/home/vipa/Zotero/storage/B7N4P3B8/0911.html}
}

@inproceedings{vanwykContextawareScanningParsing2007,
  title = {Context-Aware {{Scanning}} for {{Parsing Extensible Languages}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Van Wyk, Eric R. and Schwerdfeger, August C.},
  year = {2007},
  series = {{{GPCE}} '07},
  pages = {63--72},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1289971.1289983},
  urldate = {2018-10-18},
  abstract = {This paper introduces new parsing and context-aware scanning algorithms in which the scanner uses contextual information to disambiguate lexical syntax. The parser uses a slightly modified LR-style algorithm that passes to the scanner the set of valid symbols that the scanner may return at that point in parsing. This set is those terminals whose entries in the parse table for the current parse state are shift, reduce, or accept, but not error. The scanner then only returns tokens in this set. An analysis is given that can statically verify that the scanner will never return more than one token for a single input. Context-aware scanning is especially useful when parsing and scanning extensible languages in which domain specific languages can be embedded. It has been used in extensible versions of Java 1.4 and ANSI C. We illustrate this approach with a declarative specification of a subset of Java and extensions that embed SQL queries and Boolean expression tables into Java.},
  isbn = {978-1-59593-855-8},
  keywords = {context-aware scanning,extensible languages},
  file = {/home/vipa/Zotero/storage/BL4I4DNX/Van Wyk and Schwerdfeger - 2007 - Context-aware Scanning for Parsing Extensible Lang.pdf}
}

@article{vanwykSilverExtensibleAttribute2010,
  title = {Silver: {{An}} Extensible Attribute Grammar System},
  shorttitle = {Silver},
  author = {Van Wyk, Eric and Bodin, Derek and Gao, Jimin and Krishnan, Lijesh},
  year = {2010},
  month = jan,
  journal = {Science of Computer Programming},
  series = {Special {{Issue}} on {{ETAPS}} 2006 and 2007 {{Workshops}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} '06 and '07)},
  volume = {75},
  number = {1},
  pages = {39--54},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2009.07.004},
  urldate = {2018-10-18},
  abstract = {Attribute grammar specification languages, like many domain-specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars are often not adopted to the degree that their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible attribute grammar specification system, and show how it can be extended with general purpose features such as pattern matching and domain-specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an attribute grammar specification language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner.},
  keywords = {Attribute grammars,Extensible compilers,Extensible languages,Forwarding,Silver attribute grammar system},
  file = {/home/vipa/Zotero/storage/CFSWWB64/Van Wyk et al. - 2010 - Silver An extensible attribute grammar system.pdf;/home/vipa/Zotero/storage/58HY84IZ/S0167642309001099.html}
}

@article{veldhuizenUsingTemplateMetaprograms1995,
  title = {Using {{C}}++ Template Metaprograms},
  author = {Veldhuizen, Todd},
  year = {1995},
  journal = {C++ Report},
  volume = {7},
  number = {4},
  pages = {36--43}
}

@article{visserSurveyRewritingStrategies2001,
  title = {A {{Survey}} of {{Rewriting Strategies}} in {{Program Transformation Systems}}},
  author = {Visser, Eelco},
  year = {2001},
  month = dec,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{WRS}} 2001, 1st {{International Workshop}} on {{Reduction Strategies}} in {{Rewriting}} and {{Programming}}},
  volume = {57},
  pages = {109--143},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00270-1},
  urldate = {2020-11-27},
  abstract = {Program transformation is used in a wide range of applications including compiler construction, optimization, program synthesis, refactoring, software renovation, and reverse engineering. Complex program transformations are achieved through a number of consecutive modifications of a program. Transformation rules define basic modifications. A transformation strategy is an algorithm for choosing a path in the rewrite relation induced by a set of rules. This paper surveys the support for the definition of strategies in program transformation systems. After a discussion of kinds of program transformation and choices in program representation, the basic elements of a strategy system are discussed and the choices in the design of a strategy language are considered. Several styles of strategy systems as provided in existing languages are then analyzed.},
  langid = {english},
  file = {/home/vipa/Zotero/storage/5TPGM43J/Visser - 2001 - A Survey of Rewriting Strategies in Program Transf.pdf;/home/vipa/Zotero/storage/3FTKS6IS/S1571066104002701.html}
}

@phdthesis{visserSyntaxDefinitionLanguage1997,
  title = {Syntax Definition for Language Prototyping},
  author = {Visser, Eelco},
  year = {1997},
  month = sep,
  advisor = {Paul Klint},
  citedby = {6},
  cites = {0},
  researchr = {https://researchr.org/publication/Visser97},
  school = {University of Amsterdam},
  tags = {syntax definition, SDF, syntax definition formalism, disambiguation, testing, language design, graph-rewriting, algebraic specification, algebra, context-aware, rewriting, parsing, design, scannerless parsing, ASF+SDF, SGLR},
  file = {/home/vipa/Zotero/storage/T8L7APFF/Visser - 1997 - Syntax definition for language prototyping.pdf}
}

@inproceedings{vollmerLoCalLanguagePrograms2019,
  title = {{{LoCal}}: A Language for Programs Operating on Serialized Data},
  shorttitle = {{{LoCal}}},
  booktitle = {Proceedings of the 40th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Vollmer, Michael and Koparkar, Chaitanya and Rainey, Mike and Sakka, Laith and Kulkarni, Milind and Newton, Ryan R.},
  year = {2019},
  month = jun,
  series = {{{PLDI}} 2019},
  pages = {48--62},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3314221.3314631},
  urldate = {2024-06-04},
  abstract = {In a typical data-processing program, the representation of data in memory is distinct from its representation in a serialized form on disk. The former has pointers and arbitrary, sparse layout, facilitating easy manipulation by a program, while the latter is packed contiguously, facilitating easy I/O. We propose a language, LoCal, to unify in-memory and serialized formats. LoCal extends a region calculus into a location calculus, employing a type system that tracks the byte-addressed layout of all heap values. We formalize LoCal and prove type safety, and show how LoCal programs can be inferred from unannotated source terms. We transform the existing Gibbon compiler to use LoCal as an intermediate language, with the goal of achieving a balance between code speed and data compactness by introducing just enough indirection into heap layouts, preserving the asymptotic complexity of traditional representations, but working with mostly or completely serialized data. We show that our approach yields significant performance improvement over prior approaches to operating on packed data, without abandoning idiomatic programming with recursive functions.},
  isbn = {978-1-4503-6712-7},
  keywords = {Compiler Optimization,Data Encoding,Region Calculus,Tree Traversal},
  file = {/home/vipa/Zotero/storage/SF3QGFKX/Vollmer et al. - 2019 - LoCal a language for programs operating on serial.pdf}
}

@inproceedings{voronenkoSystemDemonstrationSpiral2008,
  title = {System {{Demonstration}} of {{Spiral}}: {{Generator}} for {{High-Performance Linear Transform Libraries}}},
  shorttitle = {System {{Demonstration}} of {{Spiral}}},
  booktitle = {Algebraic {{Methodology}} and {{Software Technology}}},
  author = {Voronenko, Yevgen and Franchetti, Franz and {de Mesmay}, Fr{\'e}d{\'e}ric and P{\"u}schel, Markus},
  editor = {Meseguer, Jos{\'e} and Ro{\c s}u, Grigore},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {407--412},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-79980-1_30},
  abstract = {We demonstrate Spiral, a domain-specific library generation system. Spiral generates high performance source code for linear transforms (such as the discrete Fourier transform and many others) directly from a problem specification. The key idea underlying Spiral is to perform automatic reasoning and optimizations at a high abstraction level using the mathematical, declarative domain-specific languages SPL and {$\Sigma$}-SPL and a rigorous rewriting framework. Optimization includes various forms of parallelization. Even though Spiral provides complete automation, its generated libraries often run faster than any existing hand-written code.},
  isbn = {978-3-540-79980-1},
  langid = {english},
  keywords = {automatic performance tuni,discrete Fourier transform,domain-specific language,FFT,Linear transform,matrix algebra,multithreading,program generation,rewriting,SIMD vector instructions},
  file = {/home/vipa/Zotero/storage/IHU7CVEW/Voronenko et al. - 2008 - System Demonstration of Spiral Generator for High.pdf}
}

@inproceedings{wadlerHowMakeAdhoc1989,
  title = {How to Make Ad-Hoc Polymorphism Less Ad Hoc},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Wadler, P. and Blott, S.},
  year = {1989},
  month = jan,
  series = {{{POPL}} '89},
  pages = {60--76},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/75277.75283},
  urldate = {2022-09-01},
  abstract = {This paper presents type classes, a new approach to ad-hoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the ``eqtype variables'' of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.},
  isbn = {978-0-89791-294-5},
  file = {/home/vipa/Zotero/storage/BTUPSH5M/Wadler and Blott - 1989 - How to make ad-hoc polymorphism less ad hoc.pdf}
}

@article{wandTypeInferenceRecord1991,
  title = {Type Inference for Record Concatenation and Multiple Inheritance},
  author = {Wand, Mitchell},
  year = {1991},
  month = jul,
  journal = {Information and Computation},
  series = {Selections from 1989 {{IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  volume = {93},
  number = {1},
  pages = {1--15},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90050-C},
  urldate = {2024-07-22},
  abstract = {We show that the type inference problem for a lambda calculus with records, including a record concatenation operator, is decidable. We show that this calculus does not have principal types, but does have finite complete sets of types: that is, for any term M in the calculus, there exists an effectively generable finite set of type schemes such that every typing for M is an instance of one of the schemes in the set. We show how a simple model of object-oriented programming, including hidden instance variables and multiple inheritance, may be coded in this calculus. We conclude that type inference is decidable for object-oriented programs, even with multiple inheritance and classes as first-class values.},
  file = {/home/vipa/Zotero/storage/5DETNP8R/Wand - 1991 - Type inference for record concatenation and multip.pdf;/home/vipa/Zotero/storage/CYPZD2NP/089054019190050C.html}
}

@inproceedings{wanFunctionalReactiveProgramming2000,
  title = {Functional {{Reactive Programming}} from {{First Principles}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2000 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Wan, Zhanyong and Hudak, Paul},
  year = {2000},
  series = {{{PLDI}} '00},
  pages = {242--252},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/349299.349331},
  urldate = {2018-10-18},
  abstract = {Functional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a high-level, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are time-ordered sequences of discrete-time event occurrences. FRP is the essence of Fran, a domain-specific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications.  In this paper we explore the formal semantics of FRP and how it relates to an implementation based on streams that represent (and therefore only approximate) continuous behaviors. We show that, in the limit as the sampling interval goes to zero, the implementation is faithful to the formal, continuous semantics, but only when certain constraints on behaviors are observed. We explore the nature of these constraints, which vary amongst the FRP primitives. Our results show both the power and limitations of this approach to language design and implementation. As an example of a limitation, we show that streams are incapable of representing instantaneous predicate events over behaviors.},
  isbn = {978-1-58113-199-4},
  file = {/home/vipa/Zotero/storage/J8CF6LMM/Wan and Hudak - 2000 - Functional Reactive Programming from First Princip.pdf}
}

@article{wangComplexityguidedContainerReplacement2022,
  title = {Complexity-Guided Container Replacement Synthesis},
  author = {Wang, Chengpeng and Yao, Peisen and Tang, Wensheng and Shi, Qingkai and Zhang, Charles},
  year = {2022},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {OOPSLA1},
  pages = {68:1--68:31},
  doi = {10.1145/3527312},
  urldate = {2024-01-06},
  abstract = {Containers, such as lists and maps, are fundamental data structures in modern programming languages. However, improper choice of container types may lead to significant performance issues. This paper presents Cres, an approach that automatically synthesizes container replacements to improve runtime performance. The synthesis algorithm works with static analysis techniques to identify how containers are utilized in the program, and attempts to select a method with lower time complexity for each container method call. Our approach can preserve program behavior and seize the opportunity of reducing execution time effectively for general inputs. We implement Cres and evaluate it on 12 real-world Java projects. It is shown that Cres synthesizes container replacements for the projects with 384.2 KLoC in 14 minutes and discovers six categories of container replacements, which can achieve an average performance improvement of 8.1\%.},
  keywords = {data structure specification,program optimization,program synthesis},
  file = {/home/vipa/Zotero/storage/HR7LSIHV/Wang et al. - 2022 - Complexity-guided container replacement synthesis.pdf}
}

@book{webberModernProgrammingLanguages2003,
  title = {Modern {{Programming Languages}}: {{A Practical Introduction}}},
  shorttitle = {Modern {{Programming Languages}}},
  author = {Webber, Adam Brooks},
  year = {2003},
  publisher = {Franklin, Beedle \& Associates},
  abstract = {Typical undergraduate CS/CE majors have a practical orientation: they study computing because they like programming and are good at it. This book has strong appeal to this core student group. There is more than enough material for a semester-long course. The challenge for a course in programming language concepts is to help practical ......},
  googlebooks = {RUd5QgAACAAJ},
  isbn = {978-1-887902-76-2},
  langid = {english},
  keywords = {Computers / General}
}

@inproceedings{winantCoherentExplicitDictionary2018,
  title = {Coherent Explicit Dictionary Application for {{Haskell}}},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN International Symposium}} on {{Haskell}}},
  author = {Winant, Thomas and Devriese, Dominique},
  year = {2018},
  month = sep,
  pages = {81--93},
  publisher = {ACM},
  address = {St. Louis MO USA},
  doi = {10.1145/3242744.3242752},
  urldate = {2022-09-01},
  abstract = {Type classes are one of Haskell's most popular features and extend its type system with ad-hoc polymorphism. Since their conception, there were useful features that could not be offered because of the desire to offer two correctness properties: coherence and global uniqueness of instances. Coherence essentially guarantees that program semantics are independent from type-checker internals. Global uniqueness of instances is relied upon by libraries for enforcing, for example, that a single order relation is used for all manipulations of an ordered binary tree. The features that could not be offered include explicit dictionary application and local instances, which would be highly useful in practice. In this paper, we propose a new design for offering explicit dictionary application, without compromising coherence and global uniqueness. We introduce a novel criterion based on GHC's type argument roles to decide when a dictionary application is safe with respect to global uniqueness of instances. We preserve coherence by detecting potential sources of incoherence, and prove it formally. Moreover, our solution makes it possible to use local dictionaries. In addition to developing our ideas formally, we have implemented a working prototype in GHC.},
  isbn = {978-1-4503-5835-4},
  langid = {english},
  file = {/home/vipa/Zotero/storage/UW262QXP/Winant and Devriese - 2018 - Coherent explicit dictionary application for Haske.pdf}
}

@inproceedings{xuCoCoSoundAdaptive2013,
  title = {{{CoCo}}: {{Sound}} and {{Adaptive Replacement}} of {{Java Collections}}},
  shorttitle = {{{CoCo}}},
  booktitle = {{{ECOOP}} 2013 -- {{Object-Oriented Programming}}},
  author = {Xu, Guoqing},
  editor = {Castagna, Giuseppe},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--26},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-39038-8_1},
  abstract = {Inefficient use of Java containers is an important source of run-time inefficiencies in large applications. This paper presents an application-level dynamic optimization technique called CoCo, that exploits algorithmic advantages of Java collections to improve performance. CoCo dynamically identifies optimal Java collection objects and safely performs run-time collection replacement, both using pure Java code. At the heart of this technique is a framework that abstracts container elements to achieve efficiency and that concretizes abstractions to achieve soundness. We have implemented part of the Java collection framework as instances of this framework, and developed a static CoCo compiler to generate Java code that performs optimizations. This work is the first step towards achieving the ultimate goal of automatically optimizing away semantic inefficiencies.},
  isbn = {978-3-642-39038-8},
  langid = {english},
  keywords = {Abstract Element,Adaptive Replacement,Allocation Site,Concrete Element,Execution Scenario},
  file = {/home/vipa/Zotero/storage/Z66SIVAF/Xu - 2013 - CoCo Sound and Adaptive Replacement of Java Colle.pdf}
}

@inproceedings{xuDetectingInefficientlyusedContainers2010,
  title = {Detecting Inefficiently-Used Containers to Avoid Bloat},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Xu, Guoqing and Rountev, Atanas},
  year = {2010},
  month = jun,
  series = {{{PLDI}} '10},
  pages = {160--173},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1806596.1806616},
  urldate = {2022-11-15},
  abstract = {Runtime bloat degrades significantly the performance and scalability of software systems. An important source of bloat is the inefficient use of containers. It is expensive to create inefficiently-used containers and to invoke their associated methods, as this may ultimately execute large volumes of code, with call stacks dozens deep, and allocate many temporary objects. This paper presents practical static and dynamic tools that can find inappropriate use of containers in Java programs. At the core of these tools is a base static analysis that identifies, for each container, the objects that are added to this container and the key statements (i.e., heap loads and stores) that achieve the semantics of common container operations such as ADD and GET. The static tool finds problematic uses of containers by considering the nesting relationships among the loops where these semantics-achieving statements are located, while the dynamic tool can instrument these statements and find inefficiencies by profiling their execution frequencies. The high precision of the base analysis is achieved by taking advantage of a context-free language (CFL)-reachability formulation of points-to analysis and by accounting for container-specific properties. It is demand-driven and client-driven, facilitating refinement specific to each queried container object and increasing scalability. The tools built with the help of this analysis can be used both to avoid the creation of container-related performance problems early during development, and to help with diagnosis when problems are observed during tuning. Our experimental results show that the static tool has a low false positive rate and produces more relevant information than its dynamic counterpart. Further case studies suggest that significant optimization opportunities can be found by focusing on statically-identified containers for which high allocation frequency is observed at run time.},
  isbn = {978-1-4503-0019-3},
  keywords = {cfl reachability,container bloat,points-to analysis},
  file = {/home/vipa/Zotero/storage/H8TIA9TJ/Xu and Rountev - 2010 - Detecting inefficiently-used containers to avoid b.pdf}
}

@inproceedings{xuFindingLowutilityData2010,
  title = {Finding Low-Utility Data Structures},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Xu, Guoqing and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Schonberg, Edith and Sevitsky, Gary},
  year = {2010},
  month = jun,
  series = {{{PLDI}} '10},
  pages = {174--186},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1806596.1806617},
  urldate = {2024-01-07},
  abstract = {Many opportunities for easy, big-win, program optimizations are missed by compilers. This is especially true in highly layered Java applications. Often at the heart of these missed optimization opportunities lie computations that, with great expense, produce data values that have little impact on the program's final output. Constructing a new date formatter to format every date, or populating a large set full of expensively constructed structures only to check its size: these involve costs that are out of line with the benefits gained. This disparity between the formation costs and accrued benefits of data structures is at the heart of much runtime bloat. We introduce a run-time analysis to discover these low-utility data structures. The analysis employs dynamic thin slicing, which naturally associates costs with value flows rather than raw data flows. It constructs a model of the incremental, hop-to-hop, costs and benefits of each data structure. The analysis then identifies suspicious structures based on imbalances of its incremental costs and benefits. To decrease the memory requirements of slicing, we introduce abstract dynamic thin slicing, which performs thin slicing over bounded abstract domains. We have modified the IBM J9 commercial JVM to implement this approach. We demonstrate two client analyses: one that finds objects that are expensive to construct but are not necessary for the forward execution, and second that pinpoints ultimately-dead values. We have successfully applied them to large-scale and long-running Java applications. We show that these analyses are effective at detecting operations that have unbalanced costs and benefits.},
  isbn = {978-1-4503-0019-3},
  keywords = {abstract dynamic thin slicing,cost benefit analysis,memory bloat},
  file = {/home/vipa/Zotero/storage/VXHZSAFQ/Xu et al. - 2010 - Finding low-utility data structures.pdf}
}

@article{youngerRecognitionParsingContextFree1967,
  title = {Recognition and Parsing of Context-Free Languages in Time $n^3$},
  author = {Younger, Daniel H.},
  year = {1967},
  month = feb,
  journal = {Information and Control},
  volume = {10},
  number = {2},
  pages = {189-208},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(67)80007-X},
  urldate = {2019-06-20},
  abstract = {A recognition algorithm is exhibited whereby an arbitrary string over a given vocabulary can be tested for containment in a given context-free language. A special merit of this algorithm is that it is completed in a number of steps proportional to the “cube” of the number of symbols in the tested string. As a byproduct of the grammatical analysis, required by the recognition algorithm, one can obtain, by some additional processing not exceeding the “cube” factor of computational complexity, a parsing matrix—a complete summary of the grammatical structure of the sentence. It is also shown how, by means of a minor modification of the recognition algorithm, one can obtain an integer representing the ambiguity of the sentence, i.e., the number of distinct ways in which that sentence can be generated by the grammar. The recognition algorithm is then simulated on a Turing Machine. It is shown that this simulation likewise requires a number of steps proportional to only the “cube” of the test string length.},
  file = {/home/vipa/Zotero/storage/LYAS92HB/Younger - 1967 - Recognition and parsing of context-free languages .pdf;/home/vipa/Zotero/storage/54GLX7B9/S001999586780007X.html}
}

@inproceedings{yuxinwangIntegratedAutomatedMemory2012,
  title = {An Integrated and Automated Memory Optimization Flow for {{FPGA}} Behavioral Synthesis},
  booktitle = {17th {{Asia}} and {{South Pacific Design Automation Conference}}},
  author = {{Yuxin Wang} and Zhang, P. and {Xu Cheng} and Cong, J.},
  year = {2012},
  month = jan,
  pages = {257--262},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2012.6164955},
  abstract = {Behavioral synthesis tools have made significant progress in compiling high-level programs into register-transfer level (RTL) specifications. But manually rewriting code is still necessary in order to obtain better quality of results in memory system optimization. In recent years different automated memory optimization techniques have been proposed and implemented, such as data reuse and memory partitioning, but the problem of integrating these techniques into an applicable flow to obtain a better performance has become a challenge. In this paper we integrate data reuse, loop pipelining, memory partitioning, and memory merging into an automated optimization flow (AMO) for FPGA behavioral synthesis. We develop memory padding to help in the memory partitioning of indices with modulo operations. Experimental results on Xilinx Virtex-6 FPGAs show that our integrated approach can gain an average 5.8{\texttimes} throughput and 4.55{\texttimes} latency improvement compared to the approach without memory partitioning. Moreover, memory merging saves up to 44.32\% of block RAM (BRAM).},
  keywords = {AMO,Arrays,automated memory optimization flow,Behavioral Synthesis,behavioral synthesis tools,data integrity,data reusability,field programmable gate arrays,formal specification,loop pipelining,Memory management,memory merging,Memory Merging,memory padding,memory partitioning,Memory Partitioning,merging,Merging,modulo operations,optimisation,Optimization,pattern classification,pipeline processing,Pipeline processing,program compiler,program compilers,Random access memory,register-transfer level,rewriting code,RTL specifications,software reusability,storage management,Throughput,Xilinx Virtex-6 FPGA},
  file = {/home/vipa/Zotero/storage/F9YUGQ6G/Yuxin Wang et al. - 2012 - An integrated and automated memory optimization fl.pdf;/home/vipa/Zotero/storage/XGGNKJXT/6164955.html}
}

@inproceedings{zhouCalculusRecursiveTypes2022,
  title = {A {{Calculus}} with~{{Recursive Types}}, {{Record Concatenation}} and~{{Subtyping}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Zhou, Yaoda and Oliveira, Bruno C. d. S. and Fan, Andong},
  editor = {Sergey, Ilya},
  year = {2022},
  pages = {175--195},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-21037-2_9},
  abstract = {Calculi with subtyping, a form of record concatenation and recursive types are useful to model objects with multiple inheritance. Surprisingly, almost no existing calculi supports the three features together, partly because the combination of subtyping and record concatenation is already known to be troublesome. Recently, a line of work on disjoint intersection types with a merge operator has emerged as a new approach to deal with the interaction between subtyping and record concatenation. However, the addition of recursive types has not been studied.},
  isbn = {978-3-031-21037-2},
  langid = {english},
  file = {/home/vipa/Zotero/storage/TJ4ERT6Q/Zhou et al. - 2022 - A Calculus with Recursive Types, Record Concatenat.pdf}
}
